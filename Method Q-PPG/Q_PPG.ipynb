{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNMoss0qA512"
      },
      "source": [
        "# folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JooVpn6SpSFS",
        "outputId": "a144f04d-4c63-42db-d989-d5d56c6d3d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-mW91r4q6AX"
      },
      "outputs": [],
      "source": [
        "!mkdir architecture_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT5Hu_Caq3SA",
        "outputId": "83c6cbfa-7d6f-4d64-b28c-8ccba04440d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/architecture_search\n"
          ]
        }
      ],
      "source": [
        "cd /content/architecture_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLlWlr1GBFfp"
      },
      "outputs": [],
      "source": [
        "!mkdir custom_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4G2g9JGBJ96"
      },
      "outputs": [],
      "source": [
        "!mkdir models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPClsjNEB6_x"
      },
      "outputs": [],
      "source": [
        "!mkdir preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxbOLRc7CI3J"
      },
      "outputs": [],
      "source": [
        "!mkdir trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ktu_0lV0x0v"
      },
      "source": [
        "# custom_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EynPNjjn-pP",
        "outputId": "9d6c170b-8312-40de-8c39-5ab03da239d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/architecture_search/custom_layers\n"
          ]
        }
      ],
      "source": [
        "cd /content/architecture_search/custom_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcC-TlyK0aC0",
        "outputId": "795babff-d3d8-43d2-9aef-c27183fd538b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing __init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile __init__.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0UeNCGc1D3o",
        "outputId": "acc9264e-c6c5-4753-ea0a-fc945d08d1c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing auto_layers.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile auto_layers.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tensorflow.python.framework import tensor_shape\n",
        "from tensorflow.python.eager import context\n",
        "from tensorflow.python.framework import common_shapes\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.keras import activations\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.python.keras import constraints\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.python.keras import regularizers\n",
        "# from tensorflow.python.keras.engine.base_layer import InputSpec\n",
        "from keras.engine.input_spec import InputSpec\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.python.keras.utils import generic_utils\n",
        "from tensorflow.python.keras.utils import tf_utils\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import gen_math_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import nn\n",
        "from tensorflow.python.ops import nn_ops\n",
        "from tensorflow.python.ops import sparse_ops\n",
        "from tensorflow.python.ops import standard_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.ops import variables as tf_variables\n",
        "from tensorflow.python.util.tf_export import tf_export\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Layer\n",
        "\n",
        "from utils import max_dil, prune_mul, binarize, g_weights\n",
        "\n",
        "import sys\n",
        "\n",
        "class Dilation_Reg(tf.keras.regularizers.Regularizer):\n",
        "\n",
        "    def __init__(self, reg_strength, c_in, c_out, r_f, l2=0.05):\n",
        "        self.reg_strength = reg_strength\n",
        "        self.c_in = c_in\n",
        "        self.c_out = c_out\n",
        "        self.r_f = r_f\n",
        "        self.l2 = l2\n",
        "\n",
        "    def __call__(self, w):\n",
        "        gamma_weights = g_weights(w, self.c_in, self.c_out, self.r_f)\n",
        "        return self.reg_strength * tf.reduce_sum(\n",
        "            tf.multiply(\n",
        "                    gamma_weights,\n",
        "                    tf.abs(w),\n",
        "                    )\n",
        "            ) + self.l2 * tf.reduce_sum(\n",
        "            tf.square(w)\n",
        "            )\n",
        "\n",
        "    # Necessary to support serialization\n",
        "    def get_config(self):\n",
        "        return {'regularization_strength': self.reg_strength,\n",
        "                'channel_in' : self.c_in,\n",
        "                'channel_out' : self.c_out,\n",
        "                'receptive field' : self.r_f,\n",
        "                'l2_strength' : self.l2\n",
        "                }\n",
        "\n",
        "class clip_0_1(tf.keras.constraints.Constraint):\n",
        "    \"\"\"Constrains weight tensors to be between 0 and 1.\"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, w):\n",
        "        return tf.clip_by_value(w, 0, 1)\n",
        "\n",
        "    def get_config(self):\n",
        "        pass\n",
        "\n",
        "class LearnedConv2D(Conv2D):\n",
        "\n",
        "    def __init__(self, cf=None, gamma_trainable=True, hyst=0, **kwargs):\n",
        "        self.cf = cf\n",
        "        self.gamma_trainable = gamma_trainable\n",
        "        self.hyst = hyst\n",
        "        super(LearnedConv2D, self).__init__(**kwargs)\n",
        "\n",
        "    def _assign_new_value(self, variable, value):\n",
        "        with K.name_scope('AssignNewValue') as scope:\n",
        "            with ops.colocate_with(variable):\n",
        "                return state_ops.assign(variable, value, name=scope)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        if self.data_format == 'channels_first':\n",
        "          channel_axis = 1\n",
        "        else:\n",
        "          channel_axis = -1\n",
        "        if input_shape.dims[channel_axis].value is None:\n",
        "          raise ValueError('The channel dimension of the inputs '\n",
        "                           'should be defined. Found `None`.')\n",
        "\n",
        "        input_dim = int(input_shape[channel_axis])\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        # Trainable parameters that identifies the learned amount of dilation\n",
        "        self.gamma = self.add_weight(\n",
        "            name='gamma',\n",
        "            shape=(1, max_dil(self.kernel_size[-1])),\n",
        "            #constraint=tf.keras.constraints.NonNeg(),\n",
        "            constraint=clip_0_1(),\n",
        "            regularizer=Dilation_Reg(self.cf.reg_strength, input_dim,\n",
        "                self.filters, self.kernel_size[-1], l2=self.cf.l2),\n",
        "            initializer=tf.keras.initializers.RandomUniform(1,1),\n",
        "            trainable=self.gamma_trainable,\n",
        "            dtype=self.dtype)\n",
        "\n",
        "        if self.hyst == 1:\n",
        "            self.alpha = self.add_weight(\n",
        "                name='alpha',\n",
        "                shape=(1, max_dil(self.kernel_size[-1])),\n",
        "                constraint=tf.keras.constraints.NonNeg(),\n",
        "                initializer=tf.keras.initializers.RandomUniform(1.,1.),\n",
        "                synchronization=tf_variables.VariableSynchronization.ON_READ,\n",
        "                trainable=False,\n",
        "                dtype=self.dtype)\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=kernel_shape,\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "\n",
        "        if self.use_bias:\n",
        "          self.bias = self.add_weight(\n",
        "              name='bias',\n",
        "              shape=(self.filters,),\n",
        "              initializer=self.bias_initializer,\n",
        "              regularizer=self.bias_regularizer,\n",
        "              constraint=self.bias_constraint,\n",
        "              trainable=True,\n",
        "              dtype=self.dtype)\n",
        "        else:\n",
        "          self.bias = None\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "\n",
        "        if self.padding == 'causal':\n",
        "          op_padding = 'valid'\n",
        "        else:\n",
        "          op_padding = self.padding\n",
        "        if not isinstance(op_padding, (list, tuple)):\n",
        "          op_padding = op_padding.upper()\n",
        "\n",
        "        self._convolution_op = nn_ops.Convolution(\n",
        "            input_shape,\n",
        "            filter_shape=self.kernel.shape,\n",
        "            dilation_rate=self.dilation_rate,\n",
        "            strides=self.strides,\n",
        "            padding=op_padding,\n",
        "            data_format=conv_utils.convert_data_format(self.data_format,\n",
        "                                                       self.rank + 2))\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        if self.hyst == 0:\n",
        "            bin_gamma = binarize(self.gamma, self.cf.threshold)\n",
        "            pruned_kernel = prune_mul(self.kernel, bin_gamma)\n",
        "        elif self.hyst == 1:\n",
        "            bin_alpha_a = binarize(self.gamma, self.cf.threshold)\n",
        "            bin_alpha_b = binarize(self.gamma, self.cf.threshold+self.cf.epsilon)\n",
        "            bin_alpha = tf.add(\n",
        "                tf.multiply(\n",
        "                    self.alpha,\n",
        "                    bin_alpha_a\n",
        "                    ),\n",
        "                tf.multiply(\n",
        "                    tf.constant(1.0, shape=[1, max_dil(self.kernel_size[-1])]) - self.alpha,\n",
        "                    bin_alpha_b\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            #self.add_update((self.alpha, bin_alpha), inputs)\n",
        "            self._assign_new_value(self.alpha, bin_alpha)\n",
        "\n",
        "            pruned_kernel = prune_mul(self.kernel, bin_alpha)\n",
        "\n",
        "        outputs = self._convolution_op(inputs, pruned_kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "          if self.data_format == 'channels_first':\n",
        "            if self.rank == 1:\n",
        "              # nn.bias_add does not accept a 1D input tensor.\n",
        "              bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "              outputs += bias\n",
        "            else:\n",
        "              outputs = nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
        "          else:\n",
        "            outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
        "\n",
        "        if self.activation is not None:\n",
        "          return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "class WeightNormConv2D(Conv2D):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(WeightNormConv2D, self).__init__(**kwargs)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        if self.data_format == 'channels_first':\n",
        "          channel_axis = 1\n",
        "        else:\n",
        "          channel_axis = -1\n",
        "        if input_shape.dims[channel_axis].value is None:\n",
        "          raise ValueError('The channel dimension of the inputs '\n",
        "                           'should be defined. Found `None`.')\n",
        "\n",
        "        input_dim = int(input_shape[channel_axis])\n",
        "        kernel_shape = self.kernel_size + (input_dim, self.filters)\n",
        "\n",
        "        self.wn_g = self.add_weight(\n",
        "            name='wn_g',\n",
        "            shape=(self.filters,),\n",
        "            initializer=tf.keras.initializers.RandomUniform(1,1),\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=kernel_shape,\n",
        "            initializer=self.kernel_initializer,\n",
        "            regularizer=self.kernel_regularizer,\n",
        "            constraint=self.kernel_constraint,\n",
        "            trainable=True,\n",
        "            dtype=self.dtype)\n",
        "\n",
        "        if self.use_bias:\n",
        "          self.bias = self.add_weight(\n",
        "              name='bias',\n",
        "              shape=(self.filters,),\n",
        "              initializer=self.bias_initializer,\n",
        "              regularizer=self.bias_regularizer,\n",
        "              constraint=self.bias_constraint,\n",
        "              trainable=True,\n",
        "              dtype=self.dtype)\n",
        "        else:\n",
        "          self.bias = None\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=self.rank + 2,\n",
        "                                    axes={channel_axis: input_dim})\n",
        "\n",
        "        if self.padding == 'causal':\n",
        "          op_padding = 'valid'\n",
        "        else:\n",
        "          op_padding = self.padding\n",
        "        if not isinstance(op_padding, (list, tuple)):\n",
        "          op_padding = op_padding.upper()\n",
        "\n",
        "        self._convolution_op = nn_ops.Convolution(\n",
        "            input_shape,\n",
        "            filter_shape=self.kernel.shape,\n",
        "            dilation_rate=self.dilation_rate,\n",
        "            strides=self.strides,\n",
        "            padding=op_padding,\n",
        "            data_format=conv_utils.convert_data_format(self.data_format,\n",
        "                                                       self.rank + 2))\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "\n",
        "        norm_w = tf.sqrt(tf.reduce_sum(\n",
        "        tf.square(self.kernel), [0, 1, 2], keepdims=False))\n",
        "        norm_v = tf.rsqrt(tf.reduce_sum(\n",
        "        tf.square(self.wn_g)))\n",
        "        norm_kernel = self.kernel * self.wn_g * (norm_v * norm_w)\n",
        "\n",
        "        outputs = self._convolution_op(inputs, norm_kernel)\n",
        "\n",
        "        if self.use_bias:\n",
        "          if self.data_format == 'channels_first':\n",
        "            if self.rank == 1:\n",
        "              # nn.bias_add does not accept a 1D input tensor.\n",
        "              bias = array_ops.reshape(self.bias, (1, self.filters, 1))\n",
        "              outputs += bias\n",
        "            else:\n",
        "              outputs = nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
        "          else:\n",
        "            outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
        "\n",
        "        if self.activation is not None:\n",
        "          return self.activation(outputs)\n",
        "        return outputs\n",
        "\n",
        "class DenseTied(Layer):\n",
        "\n",
        "    \"\"\"Just your regular densely-connected NN layer.\n",
        "    `Dense` implements the operation:\n",
        "    `output = activation(dot(input, kernel) + bias)`\n",
        "    where `activation` is the element-wise activation function\n",
        "    passed as the `activation` argument, `kernel` is a weights matrix\n",
        "    created by the layer, and `bias` is a bias vector created by the layer\n",
        "    (only applicable if `use_bias` is `True`).\n",
        "    Note: if the input to the layer has a rank greater than 2, then\n",
        "    it is flattened prior to the initial dot product with `kernel`.\n",
        "    Example:\n",
        "    ```python\n",
        "        # as first layer in a sequential model:\n",
        "        model = Sequential()\n",
        "        model.add(Dense(32, input_shape=(16,)))\n",
        "        # now the model will take as input arrays of shape (*, 16)\n",
        "        # and output arrays of shape (*, 32)\n",
        "        # after the first layer, you don't need to specify\n",
        "        # the size of the input anymore:\n",
        "        model.add(Dense(32))\n",
        "    ```\n",
        "    Arguments:\n",
        "        units: Positive integer, dimensionality of the output space.\n",
        "        activation: Activation function to use.\n",
        "            If you don't specify anything, no activation is applied\n",
        "            (ie. \"linear\" activation: `a(x) = x`).\n",
        "        use_bias: Boolean, whether the layer uses a bias vector.\n",
        "        kernel_initializer: Initializer for the `kernel` weights matrix.\n",
        "        bias_initializer: Initializer for the bias vector.\n",
        "        kernel_regularizer: Regularizer function applied to\n",
        "            the `kernel` weights matrix.\n",
        "        bias_regularizer: Regularizer function applied to the bias vector.\n",
        "        activity_regularizer: Regularizer function applied to\n",
        "            the output of the layer (its \"activation\")..\n",
        "        kernel_constraint: Constraint function applied to\n",
        "            the `kernel` weights matrix.\n",
        "        bias_constraint: Constraint function applied to the bias vector.\n",
        "        tied_to: tf layer name or layer variable to tie\n",
        "    Input shape:\n",
        "        nD tensor with shape: `(batch_size, ..., input_dim)`.\n",
        "        The most common situation would be\n",
        "        a 2D input with shape `(batch_size, input_dim)`.\n",
        "    Output shape:\n",
        "        nD tensor with shape: `(batch_size, ..., units)`.\n",
        "        For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
        "        the output would have shape `(batch_size, units)`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 # kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 # kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 # kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 tied_to=None,\n",
        "                 **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "\n",
        "        super(DenseTied, self).__init__(\n",
        "            activity_regularizer=regularizers.get(activity_regularizer), **kwargs)\n",
        "        self.tied_to = tied_to\n",
        "        self.units = int(units)\n",
        "        self.activation = activations.get(activation)\n",
        "\n",
        "        \"\"\"transposed weights are variables and don't use any regularizators or initizlizators\"\"\"\n",
        "        # self.kernel_initializer = None\n",
        "        # self.kernel_constraint = None\n",
        "        # self.kernel_regularizer = None\n",
        "\n",
        "        \"\"\"biases are still initialized and regularized\"\"\"\n",
        "        self.use_bias = use_bias\n",
        "        self.bias_initializer = initializers.get(bias_initializer)\n",
        "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
        "        self.bias_constraint = constraints.get(bias_constraint)\n",
        "        self.supports_masking = True\n",
        "        self.input_spec = InputSpec(min_ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        if input_shape[-1] is None:\n",
        "            raise ValueError('The last dimension of the inputs to `Dense` '\n",
        "                             'should be defined. Found `None`.')\n",
        "        self.input_spec = InputSpec(min_ndim=2,\n",
        "                                    axes={-1: input_shape[-1]})\n",
        "\n",
        "        \"\"\"Get and transpose tied weights\n",
        "        Caution: <weights> method returns array of arrays with kernels and biases and use only kernels here\"\"\"\n",
        "\n",
        "        if isinstance(self.tied_to, str):\n",
        "            # if <tied_to> is str i.e. tf layer name\n",
        "            try:\n",
        "                weights = model.get_layer(\"{}\".format(self.tied_to)).weights[0]\n",
        "            except:\n",
        "                weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, \"{}\".format(self.tied_to))[0]\n",
        "            self.transposed_weights = tf.transpose(weights, name='{}_kernel_transpose'.format(self.tied_to))\n",
        "\n",
        "        else:\n",
        "            # if <tied_to> is layer variable\n",
        "            weights = self.tied_to.weights[0]\n",
        "            #weights = self.tied_to.kernel\n",
        "            self.transposed_weights = tf.transpose(weights, name='{}_kernel_transpose'.format(self.tied_to.name))\n",
        "\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                'bias',\n",
        "                shape=[self.units, ],\n",
        "                initializer=self.bias_initializer,\n",
        "                regularizer=self.bias_regularizer,\n",
        "                constraint=self.bias_constraint,\n",
        "                dtype=self.dtype,\n",
        "                trainable=True)\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        rank = len(inputs.shape)\n",
        "        if rank > 2:\n",
        "          # Broadcasting is required for the inputs.\n",
        "          outputs = standard_ops.tensordot(inputs, self.transposed_weights, [[rank - 1], [0]])\n",
        "          # Reshape the output back to the original ndim of the input.\n",
        "          if not context.executing_eagerly():\n",
        "            shape = inputs.shape.as_list()\n",
        "            output_shape = shape[:-1] + [self.units]\n",
        "            outputs.set_shape(output_shape)\n",
        "        else:\n",
        "          inputs = math_ops.cast(inputs, self._compute_dtype)\n",
        "          if K.is_sparse(inputs):\n",
        "            outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, self.transposed_weights)\n",
        "          else:\n",
        "            outputs = gen_math_ops.mat_mul(inputs, self.transposed_weights)\n",
        "        if self.use_bias:\n",
        "          outputs = nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "          return self.activation(outputs)  # pylint: disable=not-callable\n",
        "        return outputs\n",
        "        '''\n",
        "        inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\n",
        "        rank = common_shapes.rank(inputs)\n",
        "        if rank > 2:\n",
        "            # Broadcasting is required for the inputs.\n",
        "            outputs = standard_ops.tensordot(inputs, self.transposed_weights, [[rank - 1], [0]])\n",
        "            # Reshape the output back to the original ndim of the input.\n",
        "            if not context.executing_eagerly():\n",
        "                shape = inputs.get_shape().as_list()\n",
        "                output_shape = shape[:-1] + [self.units]\n",
        "                outputs.set_shape(output_shape)\n",
        "        else:\n",
        "            outputs = gen_math_ops.mat_mul(inputs, self.transposed_weights)\n",
        "        if self.use_bias:\n",
        "            outputs = nn.bias_add(outputs, self.bias)\n",
        "        if self.activation is not None:\n",
        "            return self.activation(outputs)  # pylint: disable=not-callable\n",
        "        return outputs\n",
        "        '''\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        input_shape = tensor_shape.TensorShape(input_shape)\n",
        "        input_shape = input_shape.with_rank_at_least(2)\n",
        "        if input_shape[-1].value is None:\n",
        "            raise ValueError(\n",
        "                'The innermost dimension of input_shape must be defined, but saw: %s'\n",
        "                % input_shape)\n",
        "        return input_shape[:-1].concatenate(self.units)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'units': self.units,\n",
        "            'activation': activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias,\n",
        "            # 'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
        "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
        "            # 'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
        "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
        "            'activity_regularizer':\n",
        "                regularizers.serialize(self.activity_regularizer),\n",
        "            # 'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
        "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
        "        }\n",
        "        base_config = super(DenseTied, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzyXY0Mj1WnR"
      },
      "source": [
        "# models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFaU9dvBrwXP",
        "outputId": "4cc4f9e2-e2c5-4940-f779-43b4a6362407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/architecture_search/models\n"
          ]
        }
      ],
      "source": [
        "cd /content/architecture_search/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0sMjkiX1X4D",
        "outputId": "78682be2-94ae-4477-f9ad-0fc0209895ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing __init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile __init__.py\n",
        "\n",
        "from .build_TEMPONet import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsorEu0J1nGt",
        "outputId": "a4fcb74e-c572-4264-dd9d-72edb750f6df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing build_TEMPONet.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile build_TEMPONet.py\n",
        "\n",
        "from tensorflow.keras import Sequential, layers\n",
        "from custom_layers.auto_layers import LearnedConv2D\n",
        "import math\n",
        "\n",
        "def TEMPONet_pit(width_mult, in_shape, cf, trainable=True, ofmap=[]):\n",
        "\n",
        "    input_channel = width_mult * 32\n",
        "    output_channel = input_channel * 2\n",
        "\n",
        "    if not ofmap:\n",
        "        ofmap = [\n",
        "                32, 32, 64,\n",
        "                64, 64, 128,\n",
        "                128, 128, 128,\n",
        "                256, 128, 1\n",
        "                ]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LearnedConv2D(\n",
        "        cf=cf,\n",
        "        gamma_trainable=trainable,\n",
        "        filters=ofmap[0], kernel_size=(1,5), padding='same',\n",
        "        dilation_rate=(1,1), input_shape = (1, in_shape, 4)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(LearnedConv2D(\n",
        "        cf=cf,\n",
        "        gamma_trainable=trainable,\n",
        "        filters=ofmap[1], kernel_size=(1,5), padding='same',\n",
        "        dilation_rate=(1,1), input_shape = (1, in_shape, 4)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(LearnedConv2D(\n",
        "        cf=cf,\n",
        "        gamma_trainable=trainable,\n",
        "        filters=ofmap[2], kernel_size=(1,5), padding='same',\n",
        "        dilation_rate=(1,1), input_shape = (1, in_shape, 4)))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    input_channel = width_mult * 64\n",
        "    output_channel = input_channel*2\n",
        "\n",
        "    model.add(LearnedConv2D(\n",
        "        cf=cf,\n",
        "        gamma_trainable=trainable,\n",
        "        filters=ofmap[3], kernel_size=(1,9), padding='same',\n",
        "        dilation_rate=(1,1), input_shape = (1, in_shape//2, 4)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(LearnedConv2D(\n",
        "        cf=cf,\n",
        "        gamma_trainable=trainable,\n",
        "        filters=ofmap[4], kernel_size=(1,9), padding='same',\n",
        "        dilation_rate=(1,1), input_shape = (1, in_shape//2, 4)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[5],\n",
        "        kernel_size=(1,5),\n",
        "        padding='same',\n",
        "        strides=2))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    input_channel = width_mult * 128\n",
        "    output_channel = input_channel*2\n",
        "\n",
        "    model.add(LearnedConv2D(\n",
        "        cf=cf,\n",
        "        gamma_trainable=trainable,\n",
        "        filters=ofmap[6], kernel_size=(1,17), padding='same',\n",
        "        dilation_rate=(1,1), input_shape = (1, in_shape//4, 4)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(LearnedConv2D(\n",
        "        cf=cf,\n",
        "        gamma_trainable=trainable,\n",
        "        filters=ofmap[7], kernel_size=(1,17), padding='same',\n",
        "        dilation_rate=(1,1), input_shape = (1, in_shape//4, 4)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(filters=ofmap[8],\n",
        "        kernel_size=(1,5),\n",
        "        padding='valid',\n",
        "        strides=4))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(ofmap[9]))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(ofmap[10]))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(ofmap[11]))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "def TEMPONet_mn(width_mult, in_shape, dil_ht, dil_list=[], ofmap=[]):\n",
        "\n",
        "    rf = [5, 9, 17]\n",
        "\n",
        "    if dil_ht:\n",
        "        dil_list = [\n",
        "                    2, 2, 1,\n",
        "                    4, 4,\n",
        "                    8, 8\n",
        "                    ]\n",
        "    else:\n",
        "        if not dil_list:\n",
        "            dil_list = [\n",
        "                    1, 1, 1,\n",
        "                    1, 1,\n",
        "                    1, 1\n",
        "                    ]\n",
        "\n",
        "    if not ofmap:\n",
        "        ofmap = [\n",
        "                32, 32, 64,\n",
        "                64, 64, 128,\n",
        "                128, 128, 128,\n",
        "                256, 128, 1\n",
        "                ]\n",
        "\n",
        "    input_channel = width_mult * 32\n",
        "    output_channel = input_channel * 2\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[0],\n",
        "        kernel_size=(1,math.ceil(rf[0]/dil_list[0])),\n",
        "        padding='same', dilation_rate=(1,dil_list[0]),\n",
        "        input_shape = (1, in_shape, 4)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[1],\n",
        "        kernel_size=(1,math.ceil(rf[0]/dil_list[1])),\n",
        "        padding='same', dilation_rate=(1,dil_list[1]),\n",
        "        input_shape = (1, in_shape, 32)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.ZeroPadding2D(padding=((0, 0), (4, 0))))\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[2],\n",
        "        kernel_size=(1,math.ceil(rf[0]/dil_list[2])),\n",
        "        padding='valid', dilation_rate=(1,dil_list[2]),\n",
        "        input_shape = (1, in_shape+4, 32)))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    input_channel = width_mult * 64\n",
        "    output_channel = input_channel*2\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[3],\n",
        "        kernel_size=(1,math.ceil(rf[1]/dil_list[3])),\n",
        "        padding='same', dilation_rate=(1,dil_list[3]),\n",
        "        input_shape = (1, in_shape/2 + 8, 64)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[4],\n",
        "        kernel_size=(1,math.ceil(rf[1]/dil_list[4])),\n",
        "        padding='same', dilation_rate=(1,dil_list[4]),\n",
        "        input_shape = (1, in_shape/2 + 8, 64)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.ZeroPadding2D(padding=((0, 0), (4, 0))))\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[5],\n",
        "        kernel_size=(1,5), padding='valid',\n",
        "        strides=2, input_shape = (1, in_shape/2 + 4, 64)))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    input_channel = width_mult * 128\n",
        "    output_channel = input_channel*2\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[6],\n",
        "        kernel_size=(1,math.ceil(rf[2]/dil_list[5])),\n",
        "        padding='same', dilation_rate=(1,dil_list[5]),\n",
        "        input_shape = (1, in_shape/8 + 16, 128)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[7],\n",
        "        kernel_size=(1,math.ceil(rf[2]/dil_list[6])),\n",
        "        padding='same', dilation_rate=(1,dil_list[6]),\n",
        "        input_shape = (1, in_shape/8 + 16, 128)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.ZeroPadding2D(padding=((0, 0), (5, 0))))\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[8],\n",
        "        kernel_size=(1,5), padding='valid',\n",
        "        strides=4, input_shape = (1, in_shape/8 + 5, 128)))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    #model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Conv2D <==> Dense(256)\n",
        "    model.add(layers.Conv2D(filters=ofmap[9], kernel_size=(1,4), padding='valid', strides=1, input_shape = (1, in_shape/64, 128)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "\n",
        "    # Conv2D <==> Dense(128)\n",
        "    model.add(layers.Conv2D(filters=ofmap[10], kernel_size=(1,1), padding='valid', strides=1, input_shape = (1, in_shape/256, 256)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "\n",
        "    # Conv2D <==> Dense(1)\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Conv2D(filters=1, kernel_size=(1,1), padding='valid', strides=1, input_shape = (1, in_shape/256, 128)))\n",
        "\n",
        "    model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "def TEMPONet_learned(width_mult, in_shape, dil_ht, dil_list=[], ofmap=[], n_ch=4):\n",
        "\n",
        "    rf = [5, 9, 17]\n",
        "\n",
        "    if not dil_list and dil_ht:\n",
        "        dil_list = [\n",
        "                    2, 2, 1,\n",
        "                    4, 4,\n",
        "                    8, 8\n",
        "                    ]\n",
        "    elif not dil_list:\n",
        "        dil_list = [\n",
        "                    1, 1, 1,\n",
        "                    1, 1,\n",
        "                    1, 1\n",
        "                    ]\n",
        "\n",
        "\n",
        "    if not ofmap:\n",
        "        ofmap = [\n",
        "                32, 32, 64,\n",
        "                64, 64, 128,\n",
        "                128, 128, 128,\n",
        "                256, 128, 1\n",
        "                ]\n",
        "    else:\n",
        "        for idx, i in enumerate(ofmap):\n",
        "            if i == 0:\n",
        "                ofmap[idx] = 1\n",
        "\n",
        "\n",
        "    input_channel = width_mult * 32\n",
        "    output_channel = input_channel * 2\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[0],\n",
        "        kernel_size=(1,math.ceil(rf[0]/dil_list[0])),\n",
        "        padding='same', dilation_rate=(1,dil_list[0]),\n",
        "        input_shape = (1, in_shape, n_ch)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[1],\n",
        "        kernel_size=(1,math.ceil(rf[0]/dil_list[1])),\n",
        "        padding='same', dilation_rate=(1,dil_list[1]),\n",
        "        input_shape = (1, in_shape, 32)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.ZeroPadding2D(padding=((0, 0), (4, 0))))\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[2],\n",
        "        kernel_size=(1,math.ceil(rf[0]/dil_list[2])),\n",
        "        padding='valid', dilation_rate=(1,dil_list[2]),\n",
        "        input_shape = (1, in_shape+4, 32)))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    input_channel = width_mult * 64\n",
        "    output_channel = input_channel*2\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[3],\n",
        "        kernel_size=(1,math.ceil(rf[1]/dil_list[3])),\n",
        "        padding='same', dilation_rate=(1,dil_list[3]),\n",
        "        input_shape = (1, in_shape/2 + 8, 64)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[4],\n",
        "        kernel_size=(1,math.ceil(rf[1]/dil_list[4])),\n",
        "        padding='same', dilation_rate=(1,dil_list[4]),\n",
        "        input_shape = (1, in_shape/2 + 8, 64)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.ZeroPadding2D(padding=((0, 0), (4, 0))))\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[5],\n",
        "        kernel_size=(1,5), padding='valid',\n",
        "        strides=2, input_shape = (1, in_shape/2 + 4, 64)))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    input_channel = width_mult * 128\n",
        "    output_channel = input_channel*2\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[6],\n",
        "        kernel_size=(1,math.ceil(rf[2]/dil_list[5])),\n",
        "        padding='same', dilation_rate=(1,dil_list[5]),\n",
        "        input_shape = (1, in_shape/8 + 16, 128)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[7],\n",
        "        kernel_size=(1,math.ceil(rf[2]/dil_list[6])),\n",
        "        padding='same', dilation_rate=(1,dil_list[6]),\n",
        "        input_shape = (1, in_shape/8 + 16, 128)))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.ZeroPadding2D(padding=((0, 0), (5, 0))))\n",
        "    model.add(layers.Conv2D(\n",
        "        filters=ofmap[8],\n",
        "        kernel_size=(1,5), padding='valid',\n",
        "        strides=4, input_shape = (1, in_shape/8 + 5, 128)))\n",
        "    model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(ofmap[9]))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(ofmap[10]))\n",
        "    model.add(layers.Activation('relu'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(ofmap[11]))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nKWGtet1zHV"
      },
      "source": [
        "# preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ygtSAvsAJP",
        "outputId": "2fd07b49-1d4d-4833-b4a0-004f6d2fd977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/architecture_search/preprocessing\n"
          ]
        }
      ],
      "source": [
        "cd /content/architecture_search/preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXzd8eJW14_2",
        "outputId": "45730f65-4542-4ee7-cfd1-368657d7db61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting __init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile __init__.py\n",
        "\n",
        "from .preprocessing_Dalia import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9-T0IzN2MRO",
        "outputId": "f8faf100-8b54-415b-e730-322cd30fb3d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting preprocessing_Dalia.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile preprocessing_Dalia.py\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "from skimage.util.shape import view_as_windows\n",
        "from scipy.io import loadmat\n",
        "import random\n",
        "import os\n",
        "\n",
        "def preprocessing(dataset, cf):\n",
        "    # Sampling frequency of both ppg and acceleration data in IEEE_Training dataset\n",
        "    fs_IEEE_Training = 125\n",
        "    # Sampling frequency of acceleration data in PPG_Dalia dataset\n",
        "    # The sampling frequency of ppg data in PPG_Dalia dataset is fs_PPG_Dalia*2\n",
        "    fs_PPG_Dalia = 32\n",
        "\n",
        "    fs_activity = 4\n",
        "\n",
        "    Sessioni = dict()\n",
        "    S = dict()\n",
        "    acc = dict()\n",
        "    ppg = dict()\n",
        "    activity = dict()\n",
        "\n",
        "    random.seed(20)\n",
        "\n",
        "    ground_truth = dict()\n",
        "\n",
        "    val = dataset\n",
        "\n",
        "    if not os.path.exists('/content/drive/MyDrive/slimmed/slimmed_dalia.pkl'):\n",
        "        numbers= list(range(1,16))\n",
        "        session_list=random.sample(numbers,len(numbers))\n",
        "        for j in session_list:\n",
        "            paz = j\n",
        "\n",
        "            with open(cf.path_PPG_Dalia + 'PPG_FieldStudy/S' + str(j) +'/S' + str(j) +'.pkl', 'rb') as f:\n",
        "                S[paz] = pickle.load(f, encoding='latin1')\n",
        "            ppg[paz] = S[paz]['signal']['wrist']['BVP'][::2]\n",
        "            acc[paz] = S[paz]['signal']['wrist']['ACC']\n",
        "            activity[paz] = S[paz]['activity']\n",
        "            ground_truth[paz] = S[paz]['label']\n",
        "\n",
        "        sig = dict()\n",
        "        act_list = []\n",
        "        groups= []\n",
        "        sig_list = []\n",
        "        ground_truth_list = []\n",
        "\n",
        "        # Loop on keys of dictionary ground_truth\n",
        "        for k in ground_truth:\n",
        "            # Remeber to set the desired time window\n",
        "            activity[k] = np.moveaxis(view_as_windows(activity[k], (4*8,1),4*2)[:,0,:,:],1,2)\n",
        "            activity[k] = activity[k][:,:,0]\n",
        "            sig[k] = np.concatenate((ppg[k],acc[k]),axis=1)\n",
        "            sig[k]= np.moveaxis(view_as_windows(sig[k], (fs_PPG_Dalia*8,4),fs_PPG_Dalia*2)[:,0,:,:],1,2)\n",
        "            groups.append(np.full(sig[k].shape[0],k))\n",
        "            sig_list.append(sig[k])\n",
        "            act_list.append(activity[k])\n",
        "            ground_truth[k] = np.reshape(ground_truth[k], (ground_truth[k].shape[0],1))\n",
        "            ground_truth_list.append(ground_truth[k])\n",
        "\n",
        "        #print(\"gruppo\",groups)\n",
        "        groups = np.hstack(groups)\n",
        "        X = np.vstack(sig_list)\n",
        "        y = np.reshape(np.vstack(ground_truth_list),(-1,1))\n",
        "\n",
        "        act = np.vstack(act_list)\n",
        "\n",
        "        data = dict()\n",
        "        data['X'] = X\n",
        "        data['y'] = y\n",
        "        data['groups'] = groups\n",
        "        data['act'] = act\n",
        "\n",
        "        with open('cf.path_PPG_Dalia+'slimmed_dalia.pkl'', 'wb') as f:\n",
        "            pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    else:\n",
        "        with open(cf.path_PPG_Dalia+'slimmed_dalia.pkl', 'rb') as f:\n",
        "            data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "            X = data['X']\n",
        "            y = data['y']\n",
        "            groups = data['groups']\n",
        "            act = data['act']\n",
        "\n",
        "    print(\"dimensione train\",X.shape, \"dimesione test\", y.shape,\"dimensione gruppi\",groups.shape)\n",
        "\n",
        "    return X[:y.shape[0]], y, groups[:y.shape[0]], act[:y.shape[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiEdfQTo2ZVg"
      },
      "source": [
        "# trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYrpqlspsFQY",
        "outputId": "e4365fce-def0-42d9-d775-77b04d94a9ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/architecture_search/trainer\n"
          ]
        }
      ],
      "source": [
        "cd /content/architecture_search/trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_rKUnVg2hRG",
        "outputId": "3c2859be-0acb-4069-b10f-042e6d498145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing __init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile __init__.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHFJ9SlI2mJq",
        "outputId": "5bb0bacb-6bd1-45f4-b365-ca143091b3b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_TEMPONet.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_TEMPONet.py\n",
        "\n",
        "import numpy as np\n",
        "#import RandomGroupkfold as rgkf\n",
        "from RandomGroupkfold import RandomGroupKFold_split\n",
        "\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import json\n",
        "from models import build_TEMPONet\n",
        "\n",
        "def warmup(model, epochs_num, X_sh, y_sh, early_stop, checkpoint, cf):\n",
        "    hist = model.fit(x=np.transpose(X_sh.reshape(X_sh.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), \\\n",
        "                        y=y_sh, shuffle=True, \\\n",
        "                        validation_split=0.1, verbose=1, \\\n",
        "                        batch_size= cf.batch_size, epochs=epochs_num,\n",
        "                        callbacks = [early_stop, checkpoint])\n",
        "    return hist\n",
        "\n",
        "def train_gammas(model, X_sh, y_sh, early_stop, save_gamma, exp_str, cf):\n",
        "    hist = model.fit(x=np.transpose(X_sh.reshape(X_sh.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), \\\n",
        "                        y=y_sh, shuffle=True, \\\n",
        "                        validation_split=0.1, verbose=1, \\\n",
        "                        batch_size= cf.batch_size, epochs=cf.epochs,\n",
        "                        callbacks = [early_stop, save_gamma, exp_str])\n",
        "    return hist\n",
        "\n",
        "def morphnet_search(model, X_sh, y_sh, callback_list, cf):\n",
        "    hist = model.fit(\n",
        "        x=np.transpose(X_sh.reshape(X_sh.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), \\\n",
        "        y=y_sh, shuffle=True, \\\n",
        "        validation_split=0.1, verbose=1, \\\n",
        "        batch_size= cf.batch_size_MN, epochs=cf.epochs_MN,\n",
        "        callbacks = callback_list)\n",
        "\n",
        "    return hist\n",
        "\n",
        "def retrain_dil(groups, X, y, activity, checkpoint, early_stop, cf, ofmap):\n",
        "\n",
        "    predictions = dict()\n",
        "    MAE = dict()\n",
        "\n",
        "    dataset = dict()\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    # retrain and cross-validate\n",
        "    #result = rgkf.RandomGroupKFold_split(groups,4,cf.a)\n",
        "    result = RandomGroupKFold_split(groups,4,cf.a)\n",
        "    for train_index, test_val_index in result:\n",
        "        X_train, X_val_test = X[train_index], X[test_val_index]\n",
        "        y_train, y_val_test = y[train_index], y[test_val_index]\n",
        "        activity_train, activity_val_test = activity[train_index], activity[test_val_index]\n",
        "\n",
        "        logo = LeaveOneGroupOut()\n",
        "        logo.get_n_splits(groups=groups[test_val_index])  # 'groups' is always required\n",
        "        for validate_index, test_index in logo.split(X_val_test, y_val_test, groups[test_val_index]):\n",
        "            X_validate, X_test = X_val_test[validate_index], X_val_test[test_index]\n",
        "            y_validate, y_test = y_val_test[validate_index], y_val_test[test_index]\n",
        "            activity_validate, activity_test = activity_val_test[validate_index], activity_val_test[test_index]\n",
        "            groups_val=groups[test_val_index]\n",
        "            k=groups_val[test_index][0]\n",
        "\n",
        "            # init\n",
        "            try:\n",
        "               del model\n",
        "            except:\n",
        "               pass\n",
        "\n",
        "            # obtain conv #output filters from learned json structure\n",
        "            with open(cf.saving_path+'/learned_dil_'+\n",
        "                      '{:.1e}'.format(cf.reg_strength)+\n",
        "                      '_'+'{}'.format(cf.warmup)+'.json', 'r') as f:\n",
        "                dil_list = [val for _,val in json.loads(f.read()).items()]\n",
        "\n",
        "            model = build_TEMPONet.TEMPONet_learned(1, cf.input_shape,\n",
        "                                                    dil_ht=False,\n",
        "                                                    dil_list=dil_list, ofmap=ofmap)\n",
        "\n",
        "            # save model and weights\n",
        "            val_mae = 'val_mean_absolute_error'\n",
        "            mae = 'mean_absolute_error'\n",
        "            checkpoint = ModelCheckpoint(cf.saving_path+'test_reg'+str(k)+'.h5', monitor=val_mae, verbose=1,\\\n",
        "            save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
        "            #configure  model\n",
        "            adam = Adam(lr=cf.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "            model.compile(loss='logcosh', optimizer=adam, metrics=[mae])\n",
        "\n",
        "\n",
        "            X_train, y_train = shuffle(X_train, y_train)\n",
        "            print(X_train.shape)\n",
        "            print(X_validate.shape)\n",
        "            print(X_test.shape)\n",
        "\n",
        "            # Training\n",
        "            hist = model.fit(x=np.transpose(X_train.reshape(X_train.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), y=y_train, epochs=cf.epochs, batch_size=cf.batch_size, \\\n",
        "                             validation_data=(np.transpose(X_validate.reshape(X_validate.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), y_validate), verbose=1,\\\n",
        "                                 callbacks=[checkpoint, early_stop])\n",
        "\n",
        "            #evaluate\n",
        "            predictions[k] = model.predict(np.transpose(X_test.reshape(X_test.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)))\n",
        "            MAE[k] = np.linalg.norm(y_test-predictions[k], ord=1)/y_test.shape[0]\n",
        "\n",
        "            print(MAE)\n",
        "\n",
        "            dataset['P'+str(k)+'_label'] = y_test\n",
        "            dataset['P'+str(k)+'_pred'] = predictions[k]\n",
        "            dataset['P'+str(k)+'_activity'] = activity_test\n",
        "\n",
        "    # save predictions and real values\n",
        "    with open(cf.saving_path+\n",
        "              '{:.1e}'.format(cf.reg_strength)+\n",
        "              '_'+\n",
        "              '{}'.format(cf.warmup)+\n",
        "              '_data.pickle', 'wb') as handle:\n",
        "        pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return model, MAE\n",
        "\n",
        "def retrain_ch(groups, X, y, activity, early_stop, cf, ofmap):\n",
        "\n",
        "    predictions = dict()\n",
        "    MAE = dict()\n",
        "    dataset = dict()\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    # retrain and cross-validate\n",
        "    #result = rgkf.RandomGroupKFold_split(groups,4,cf.a)\n",
        "    result = RandomGroupKFold_split(groups,4,cf.a)\n",
        "    for train_index, test_val_index in result:\n",
        "        X_train, X_val_test = X[train_index], X[test_val_index]\n",
        "        y_train, y_val_test = y[train_index], y[test_val_index]\n",
        "        activity_train, activity_val_test = activity[train_index], activity[test_val_index]\n",
        "\n",
        "        logo = LeaveOneGroupOut()\n",
        "        logo.get_n_splits(groups=groups[test_val_index])  # 'groups' is always required\n",
        "        for validate_index, test_index in logo.split(X_val_test, y_val_test, groups[test_val_index]):\n",
        "            X_validate, X_test = X_val_test[validate_index], X_val_test[test_index]\n",
        "            y_validate, y_test = y_val_test[validate_index], y_val_test[test_index]\n",
        "            activity_validate, activity_test = activity_val_test[validate_index], activity_val_test[test_index]\n",
        "            groups_val=groups[test_val_index]\n",
        "            k=groups_val[test_index][0]\n",
        "\n",
        "            # init\n",
        "            try:\n",
        "               del model\n",
        "            except:\n",
        "               pass\n",
        "\n",
        "            # obtain conv #output filters from learned json structure\n",
        "            with open(cf.saving_path+\n",
        "                      'learned_structure/learned_channels_'+\n",
        "                      '{:.1e}'.format(cf.reg_strength)+\n",
        "                      '_'+\n",
        "                      '{:.1e}'.format(cf.threshold)+'.json',\n",
        "                      'r') as f:\n",
        "                conv_list = [val for _,val in json.loads(f.read()).items()]\n",
        "            #conv_list=conv_list[3:]+conv_list[:3]\n",
        "\n",
        "            model = build_TEMPONet.TEMPONet_learned(1,\n",
        "                                                    cf.input_shape,\n",
        "                                                    dil_ht=False,\n",
        "                                                    dil_list=[],\n",
        "                                                    ofmap=conv_list)\n",
        "\n",
        "            # save model and weights\n",
        "            val_mae = 'val_mean_absolute_error'\n",
        "            mae = 'mean_absolute_error'\n",
        "            #configure  model\n",
        "            adam = Adam(lr=cf.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "            model.compile(loss='logcosh', optimizer=adam, metrics=[mae])\n",
        "\n",
        "\n",
        "            X_train, y_train = shuffle(X_train, y_train)\n",
        "            print(X_train.shape)\n",
        "            print(X_validate.shape)\n",
        "            print(X_test.shape)\n",
        "\n",
        "            # Training\n",
        "            hist = model.fit(\n",
        "                x=np.transpose(X_train.reshape(X_train.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)),\n",
        "                y=y_train, epochs=cf.epochs, batch_size=cf.batch_size, \\\n",
        "                validation_data=(np.transpose(X_validate.reshape(X_validate.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), y_validate), verbose=1,\\\n",
        "                callbacks=[early_stop])\n",
        "\n",
        "            #evaluate\n",
        "            predictions[k] = model.predict(np.transpose(X_test.reshape(X_test.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)))\n",
        "            MAE[k] = np.linalg.norm(y_test-predictions[k], ord=1)/y_test.shape[0]\n",
        "\n",
        "            print(MAE)\n",
        "\n",
        "            dataset['P'+str(k)+'_label'] = y_test\n",
        "            dataset['P'+str(k)+'_pred'] = predictions[k]\n",
        "            dataset['P'+str(k)+'_activity'] = activity_test\n",
        "\n",
        "    # save predictions and real values\n",
        "    with open(\n",
        "            cf.saving_path+\n",
        "            '{:.1e}'.format(cf.reg_strength)+\n",
        "            '_'+\n",
        "            '{:.1e}'.format(cf.threshold)+\n",
        "            '_data.pickle',\n",
        "            'wb') as handle:\n",
        "        pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return model, MAE\n",
        "\n",
        "def retrain(groups, X, y, activity, checkpoint, early_stop, cf, ofmap, dil, input_setup='normal', test_all_subj=False):\n",
        "\n",
        "    predictions = dict()\n",
        "    MAE = dict()\n",
        "\n",
        "    dataset = dict()\n",
        "\n",
        "    all_subjects_perf = dict()\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    # retrain and cross-validate\n",
        "    #result = rgkf.RandomGroupKFold_split(groups,4,cf.a)\n",
        "    result = RandomGroupKFold_split(groups,4,cf.a)\n",
        "    for train_index, test_val_index in result:\n",
        "        X_train, X_val_test = X[train_index], X[test_val_index]\n",
        "        y_train, y_val_test = y[train_index], y[test_val_index]\n",
        "        activity_train, activity_val_test = activity[train_index], activity[test_val_index]\n",
        "\n",
        "        logo = LeaveOneGroupOut()\n",
        "        logo.get_n_splits(groups=groups[test_val_index])  # 'groups' is always required\n",
        "        for validate_index, test_index in logo.split(X_val_test, y_val_test, groups[test_val_index]):\n",
        "            X_validate, X_test = X_val_test[validate_index], X_val_test[test_index]\n",
        "            y_validate, y_test = y_val_test[validate_index], y_val_test[test_index]\n",
        "            activity_validate, activity_test = activity_val_test[validate_index], activity_val_test[test_index]\n",
        "            groups_val=groups[test_val_index]\n",
        "            k=groups_val[test_index][0]\n",
        "\n",
        "\n",
        "            # init\n",
        "            try:\n",
        "               del model\n",
        "            except:\n",
        "               pass\n",
        "\n",
        "            dil_list = dil\n",
        "\n",
        "            # [PPG_1, PPG_2, ACC_x, ACC_y, ACC_z]\n",
        "            if input_setup == 'normal':\n",
        "                n_ch = 4\n",
        "                #X_train = X_train[:, [0, 2, 3, 4], :]\n",
        "                #X_validate = X_validate[:, [0, 2, 3, 4], :]\n",
        "                #X_test = X_test[:, [0, 2, 3, 4], :]\n",
        "            elif input_setup == 'ppg_only_1':\n",
        "                n_ch = 1\n",
        "                X_train = X_train[:, [0], :]\n",
        "                X_validate = X_validate[:, [0], :]\n",
        "                X_test = X_test[:, [0], :]\n",
        "            elif input_setup == 'ppg_only_2':\n",
        "                n_ch = 2\n",
        "                X_train = X_train[:, [0, 1], :]\n",
        "                X_validate = X_validate[:, [0, 1], :]\n",
        "                X_test = X_test[:, [0, 1], :]\n",
        "            elif input_setup == 'all':\n",
        "                n_ch = 5\n",
        "            else:\n",
        "                raise ValueError()\n",
        "\n",
        "            model = build_TEMPONet.TEMPONet_learned(1, cf.input_shape,\n",
        "                                                    dil_ht=False,\n",
        "                                                    dil_list=dil_list, ofmap=ofmap,\n",
        "                                                    n_ch = n_ch)\n",
        "\n",
        "            # save model and weights\n",
        "            val_mae = 'val_mean_absolute_error'\n",
        "            mae = 'mean_absolute_error'\n",
        "            checkpoint = ModelCheckpoint(cf.saving_path+'test_reg'+str(k)+'.h5', monitor=val_mae, verbose=1,\\\n",
        "            save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
        "            #configure  model\n",
        "            adam = Adam(lr=cf.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "            model.compile(loss='logcosh', optimizer=adam, metrics=[mae])\n",
        "\n",
        "\n",
        "            X_train, y_train = shuffle(X_train, y_train)\n",
        "            print(X_train.shape)\n",
        "            print(X_validate.shape)\n",
        "            print(X_test.shape)\n",
        "\n",
        "            #import pdb; pdb.set_trace()\n",
        "\n",
        "            # Training\n",
        "            hist = model.fit(\n",
        "                x = np.transpose(X_train.reshape(X_train.shape[0], n_ch, cf.input_shape, 1), (0, 3, 2, 1)),\n",
        "                y = y_train, epochs=cf.epochs, batch_size=cf.batch_size,\n",
        "                validation_data=(np.transpose(X_validate.reshape(X_validate.shape[0], n_ch, cf.input_shape, 1), (0, 3, 2, 1)), y_validate),\n",
        "                verbose=1, callbacks=[checkpoint, early_stop])\n",
        "\n",
        "            with open('retrain_hist.pickle', 'wb') as f:\n",
        "                pickle.dump(hist.history, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            #evaluate\n",
        "            predictions[k] = model.predict(\n",
        "                np.transpose(X_test.reshape(X_test.shape[0], n_ch, cf.input_shape, 1), (0, 3, 2, 1))\n",
        "                )\n",
        "            MAE[k] = np.linalg.norm(y_test-predictions[k], ord=1)/y_test.shape[0]\n",
        "\n",
        "            print(MAE)\n",
        "\n",
        "            if test_all_subj:\n",
        "                train_subj = np.unique(groups[train_index])\n",
        "                val_subj = np.unique(groups[test_val_index][validate_index])\n",
        "                test_subj = np.unique(groups[test_val_index][test_index])\n",
        "\n",
        "                all_subjects_perf['Subj_'+str(test_subj)] = dict()\n",
        "\n",
        "                for i in train_subj:\n",
        "                    train_data_X = X_train[groups[train_index] == i]\n",
        "                    train_data_y = y_train[groups[train_index] == i]\n",
        "                    predictions_curr = model.predict(\n",
        "                        np.transpose(\n",
        "                            train_data_X.reshape(train_data_X.shape[0], n_ch, cf.input_shape, 1),\n",
        "                            (0, 3, 2, 1)\n",
        "                            )\n",
        "                        )\n",
        "                    MAE_curr = np.linalg.norm(train_data_y-predictions_curr, ord=1)/train_data_y.shape[0]\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Train_subj_'+str(i)] = dict()\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Train_subj_'+str(i)]['P'+str(i)+'_label'] = train_data_y\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Train_subj_'+str(i)]['P'+str(i)+'_pred'] = predictions_curr\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Train_subj_'+str(i)]['P'+str(i)+'_MAE'] = MAE_curr\n",
        "\n",
        "                for i in val_subj:\n",
        "                    val_data_X = X_validate[groups[test_val_index][validate_index] == i]\n",
        "                    val_data_y = y_validate[groups[test_val_index][validate_index] == i]\n",
        "                    predictions_curr = model.predict(\n",
        "                        np.transpose(\n",
        "                            val_data_X.reshape(val_data_X.shape[0], n_ch, cf.input_shape, 1),\n",
        "                            (0, 3, 2, 1)\n",
        "                            )\n",
        "                        )\n",
        "                    MAE_curr = np.linalg.norm(val_data_y-predictions_curr, ord=1)/val_data_y.shape[0]\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Val_subj_'+str(i)] = dict()\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Val_subj_'+str(i)]['P'+str(i)+'_label'] = val_data_y\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Val_subj_'+str(i)]['P'+str(i)+'_pred'] = predictions_curr\n",
        "                    all_subjects_perf['Subj_'+str(test_subj)]['Val_subj_'+str(i)]['P'+str(i)+'_MAE'] = MAE_curr\n",
        "\n",
        "                all_subjects_perf['Subj_'+str(test_subj)]['Test_subj_'+str(test_subj)] = dict()\n",
        "                predictions_curr = model.predict(\n",
        "                        np.transpose(\n",
        "                            X_test.reshape(X_test.shape[0], n_ch, cf.input_shape, 1),\n",
        "                            (0, 3, 2, 1)\n",
        "                            )\n",
        "                        )\n",
        "                MAE_curr = np.linalg.norm(y_test-predictions_curr, ord=1)/y_test.shape[0]\n",
        "                all_subjects_perf['Subj_'+str(test_subj)]['Test_subj_'+str(test_subj)]['P'+str(k)+'_label'] = y_test\n",
        "                all_subjects_perf['Subj_'+str(test_subj)]['Test_subj_'+str(test_subj)]['P'+str(k)+'_pred'] = predictions_curr\n",
        "                all_subjects_perf['Subj_'+str(test_subj)]['Test_subj_'+str(test_subj)]['P'+str(k)+'_MAE'] = MAE_curr\n",
        "\n",
        "                print(all_subjects_perf)\n",
        "\n",
        "            dataset['P'+str(k)+'_label'] = y_test\n",
        "            dataset['P'+str(k)+'_pred'] = predictions[k]\n",
        "            dataset['P'+str(k)+'_activity'] = activity_test\n",
        "\n",
        "    if test_all_subj:\n",
        "        with open('all_subj_data.pickle', 'wb') as handle:\n",
        "            pickle.dump(all_subjects_perf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    # save predictions and real values\n",
        "    with open(cf.saving_path+\n",
        "              '{:.1e}'.format(cf.reg_strength)+\n",
        "              '_'+\n",
        "              '{:.1e}'.format(cf.threshold)+\n",
        "              '{}'.format(cf.warmup)+\n",
        "              '_data.pickle', 'wb') as handle:\n",
        "        pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return model, MAE\n",
        "\n",
        "def fine_tune(groups, X, y, activity, checkpoint, early_stop, cf, ofmap, dil):\n",
        "\n",
        "    predictions = dict()\n",
        "    MAE = dict()\n",
        "    predictions_fine = dict()\n",
        "    MAE_fine = dict()\n",
        "\n",
        "    dataset = dict()\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    # retrain and cross-validate\n",
        "    #result = rgkf.RandomGroupKFold_split(groups,4,cf.a)\n",
        "    result = RandomGroupKFold_split(groups,4,cf.a)\n",
        "    for train_index, test_val_index in result:\n",
        "        X_train, X_val_test = X[train_index], X[test_val_index]\n",
        "        y_train, y_val_test = y[train_index], y[test_val_index]\n",
        "        activity_train, activity_val_test = activity[train_index], activity[test_val_index]\n",
        "\n",
        "        logo = LeaveOneGroupOut()\n",
        "        logo.get_n_splits(groups=groups[test_val_index])  # 'groups' is always required\n",
        "        for validate_index, test_index in logo.split(X_val_test, y_val_test, groups[test_val_index]):\n",
        "            X_validate, X_test = X_val_test[validate_index], X_val_test[test_index]\n",
        "            y_validate, y_test = y_val_test[validate_index], y_val_test[test_index]\n",
        "            activity_validate, activity_test = activity_val_test[validate_index], activity_val_test[test_index]\n",
        "            groups_val=groups[test_val_index]\n",
        "            k=groups_val[test_index][0]\n",
        "\n",
        "            # init\n",
        "            try:\n",
        "               del model\n",
        "            except:\n",
        "               pass\n",
        "\n",
        "            dil_list = dil\n",
        "            model = build_TEMPONet.TEMPONet_learned(1, cf.input_shape,\n",
        "                                                    dil_ht=False,\n",
        "                                                    dil_list=dil_list, ofmap=ofmap)\n",
        "\n",
        "            # save model and weights\n",
        "            val_mae = 'val_mean_absolute_error'\n",
        "            mae = 'mean_absolute_error'\n",
        "            checkpoint = ModelCheckpoint(cf.saving_path+'model'+str(k)+'.h5', monitor=val_mae, verbose=1,\\\n",
        "            save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
        "            #configure  model\n",
        "            adam = Adam(lr=cf.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "            model.compile(loss='logcosh', optimizer=adam, metrics=[mae])\n",
        "\n",
        "\n",
        "            X_train, y_train = shuffle(X_train, y_train)\n",
        "            print(X_train.shape)\n",
        "            print(X_validate.shape)\n",
        "            print(X_test.shape)\n",
        "\n",
        "            # Training\n",
        "            hist = model.fit(x=np.transpose(X_train.reshape(X_train.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), y=y_train, epochs=cf.epochs, batch_size=cf.batch_size, \\\n",
        "                             validation_data=(np.transpose(X_validate.reshape(X_validate.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), y_validate), verbose=1,\\\n",
        "                                 callbacks=[checkpoint, early_stop])\n",
        "\n",
        "            #evaluate\n",
        "            predictions[k] = model.predict(np.transpose(X_test.reshape(X_test.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)))\n",
        "            MAE[k] = np.linalg.norm(y_test-predictions[k], ord=1)/y_test.shape[0]\n",
        "\n",
        "            print('MAE Pre Fine Tuning: {}'.format(MAE))\n",
        "\n",
        "            dataset['P'+str(k)+'_label'] = y_test\n",
        "            dataset['P'+str(k)+'_pred'] = predictions[k]\n",
        "\n",
        "            ### fine tuning ###\n",
        "            frac = X_test.shape[0]*25//100\n",
        "\n",
        "            X_fine_train = X_test[:frac]\n",
        "            y_fine_train = y_test[:frac]\n",
        "            activity_fine_train = activity_test[frac:]\n",
        "\n",
        "            X_fine_test = X_test[frac:]\n",
        "            y_fine_test = y_test[frac:]\n",
        "            activity_fine_test = activity_test[frac:]\n",
        "\n",
        "            try:\n",
        "                del model\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "            model = load_model(cf.saving_path+'model'+str(k)+'.h5')\n",
        "            adam = Adam(lr=cf.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "\n",
        "            j = 0\n",
        "            for i in range(len(model.layers)):\n",
        "                #if re.search('batch_normalization.+', layer.get_config()['name']):\n",
        "                    #layer.trainable = False\n",
        "                if j < 11:\n",
        "                    #print(model.layers[i].trainable)\n",
        "                    model.layers[i].trainable = False\n",
        "                    #print(model.layers[i].trainable)\n",
        "\n",
        "                j += 1\n",
        "\n",
        "            model.compile(loss='logcosh', optimizer=adam, metrics=['mean_absolute_error'])\n",
        "\n",
        "            model.summary()\n",
        "\n",
        "            hist = model.fit(x=np.transpose(X_fine_train.reshape(X_fine_train.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)), y=y_fine_train, epochs=100, batch_size=256, \\\n",
        "                         verbose=1)\n",
        "\n",
        "            #evaluate\n",
        "            predictions_fine[k] = model.predict(np.transpose(X_fine_test.reshape(X_fine_test.shape[0], 4, cf.input_shape, 1), (0, 3, 2, 1)))\n",
        "            MAE_fine[k] = np.linalg.norm(y_fine_test-predictions_fine[k], ord=1)/y_fine_test.shape[0]\n",
        "            print('MAE post fine tuning: {}'.format(MAE_fine))\n",
        "\n",
        "            dataset['P'+str(k)+'_label_fine'] = y_fine_test\n",
        "            dataset['P'+str(k)+'_pred_fine'] = predictions_fine[k]\n",
        "\n",
        "    # save predictions and real values\n",
        "    with open(cf.saving_path+\n",
        "              '{:.1e}'.format(cf.reg_strength)+\n",
        "              '_'+\n",
        "              '{}'.format(cf.warmup)+\n",
        "              '_data_finetune.pickle', 'wb') as handle:\n",
        "        pickle.dump(dataset, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return model, MAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVHUchkiLp4s"
      },
      "source": [
        "# Rest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEsNlkCmsYPr",
        "outputId": "7f5e509d-f617-4a9d-b142-11eae6dd6da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/architecture_search\n"
          ]
        }
      ],
      "source": [
        "cd /content/architecture_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxvYnCBJLrI-",
        "outputId": "bb5d3468-9b9a-43e4-c545-907b79bcbef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing RandomGroupkfold.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile RandomGroupkfold.py\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def RandomGroupKFold_split(groups, n, seed=None):  # noqa: N802\n",
        "    \"\"\"\n",
        "    Random analogous of sklearn.model_selection.GroupKFold.split.\n",
        "    :return: list of (train, test) indices\n",
        "    \"\"\"\n",
        "    groups = pd.Series(groups)\n",
        "    ix = np.arange(len(groups))\n",
        "    unique = np.unique(groups)\n",
        "    np.random.RandomState(seed).shuffle(unique)\n",
        "    result = []\n",
        "    for split in np.array_split(unique, n):\n",
        "        print(split)\n",
        "        mask = groups.isin(split)\n",
        "        train, test = ix[~mask], ix[mask]\n",
        "        result.append((train, test))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HelZZTVHMDbi",
        "outputId": "abbde98a-b879-447f-ff94-5a5e560cfbdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing __init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile __init__.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDXxF1SbMMRC",
        "outputId": "7cb1f975-efab-44c0-c761-b33221968f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "\n",
        "class Config:\n",
        "    def __init__(self, search_type, root='./'):\n",
        "        self.dataset = 'PPG_Dalia'\n",
        "        self.root = root\n",
        "\n",
        "        self.search_type = search_type\n",
        "\n",
        "        # Data preprocessing parameters. Needs to be left unchanged\n",
        "        self.time_window = 8\n",
        "        self.input_shape = 32 * self.time_window\n",
        "\n",
        "        # Training Parameters\n",
        "        self.batch_size = 128\n",
        "        self.lr = 0.001\n",
        "        self.epochs = 500\n",
        "        self.a = 35\n",
        "\n",
        "\n",
        "        self.path_PPG_Dalia = self.root\n",
        "\n",
        "        # warmup_epochs determines the number of training epochs without regularization\n",
        "        # it could be an integer number or the string 'max' to indicate that we fully train the\n",
        "        # network\n",
        "        self.warmup = 20\n",
        "        # reg_strength determines how agressive lasso-reg is\n",
        "        self.reg_strength = 1e-6\n",
        "        # Amount of l2 regularization to be applied. Usually 0.\n",
        "        self.l2 = 0.\n",
        "        # threshold value is the value at which a weight is treated as 0.\n",
        "        self.threshold = 0.5\n",
        "\n",
        "        self.hyst = 0\n",
        "\n",
        "        # Where data are saved\n",
        "        self.saving_path = self.root+'saved_models_'+self.search_type+'/'\n",
        "\n",
        "        # parameters MorphNet training\n",
        "        self.epochs_MN = 350\n",
        "        self.batch_size_MN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x0zhVsXMTlo",
        "outputId": "7e6dca85-9dfb-490a-f3ea-5239f33b621d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing custom_callbacks.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile custom_callbacks.py\n",
        "\n",
        "#import config as cf\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import utils\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from morph_net.tools import structure_exporter\n",
        "\n",
        "# aliases\n",
        "val_mae = 'val_mean_absolute_error'\n",
        "mae = 'mean_absolute_error'\n",
        "\n",
        "class export_structure(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, cf):\n",
        "        self.cf = cf\n",
        "        super(export_structure, self).__init__()\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # Initialize the best as infinity.\n",
        "        if self.cf.dataset == 'PPG_Dalia':\n",
        "           self.best = np.Inf\n",
        "        elif self.cf.dataset == 'Nottingham' or self.cf.dataset == 'JSB_Chorales':\n",
        "           self.best = np.Inf\n",
        "        elif self.cf.dataset == 'SeqMNIST' or self.cf.dataset == 'PerMNIST':\n",
        "           self.best = 0\n",
        "        else:\n",
        "           print(\"{} is not supported\".format(self.cf.dataset))\n",
        "           sys.exit()\n",
        "        self.gamma = dict()\n",
        "        self.i = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        #get current validation mae\n",
        "        if self.cf.dataset == 'PPG_Dalia':\n",
        "            current = logs.get(val_mae)\n",
        "            l = 1\n",
        "            h = 0\n",
        "            wait = 0\n",
        "        else:\n",
        "            print(\"{} is not supported\".format(self.cf.dataset))\n",
        "            sys.exit()\n",
        "\n",
        "        if self.i > wait:\n",
        "            # compare with previous best one\n",
        "            if bool(np.less(current, self.best) * l) ^ \\\n",
        "                bool((current > self.best) * h):\n",
        "                self.best = current\n",
        "\n",
        "                # Record the best model if current results is better.\n",
        "                names = [weight.name for layer in self.model.layers for weight in layer.weights]\n",
        "                weights = self.model.get_weights()\n",
        "                for name, weight in zip(names, weights):\n",
        "                    if re.search('learned_conv2d.+_?[0-9]/gamma', name):\n",
        "                        self.gamma[name] = weight\n",
        "                        self.gamma[name] = np.array(self.gamma[name] > self.cf.threshold, dtype=bool)\n",
        "                        self.gamma[name] = utils.dil_fact(self.gamma[name], op='mul')\n",
        "                    elif re.search('weight_norm.+_?[0-9]/gamma', name):\n",
        "                        self.gamma[name] = weight\n",
        "                        self.gamma[name] = np.array(self.gamma[name] > self.cf.threshold, dtype=bool)\n",
        "                        self.gamma[name] = utils.dil_fact(self.gamma[name], op='mul')\n",
        "                print(\"New best model, update file. \\n\")\n",
        "                print(self.gamma)\n",
        "                if self.cf.dataset == 'PPG_Dalia':\n",
        "                    utils.save_dil_fact(self.cf.saving_path, self.gamma, self.cf)\n",
        "        else:\n",
        "            self.i += 1\n",
        "\n",
        "class SaveGamma(tf.keras.callbacks.Callback):\n",
        "\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "\n",
        "        names = [weight.name for layer in self.model.layers for weight in layer.weights]\n",
        "        weights = self.model.get_weights()\n",
        "\n",
        "        gamma = dict()\n",
        "        i = 0\n",
        "        for name, weight in zip(names, weights):\n",
        "            if re.search('learned_conv2d.+_?[0-9]/gamma', name):\n",
        "                print('gamma: ', weight.tolist()[0])\n",
        "                gamma[i] = weight.tolist()[0]\n",
        "                i += 1\n",
        "            elif re.search('weight_norm.+_?[0-9]/gamma', name):\n",
        "                print('gamma: ', weight.tolist()[0])\n",
        "                gamma[i] = weight.tolist()[0]\n",
        "                i += 1\n",
        "        #gamma_history.append(gamma)\n",
        "\n",
        "class export_structure_MN(tf.keras.callbacks.Callback):\n",
        "    '''\n",
        "    Custom callback that saves the best structure of the network by following the next steps:\n",
        "    It creates a StructureExporter object from the network_regularizer we defined before.\n",
        "    It then creates a dictionary containing all of the tensors in the regularizer and evaluates them.\n",
        "    It populates the tensors with the evaluated values and saves the current status in a file.\n",
        "    Here it saves the structure at ./saved_models/MN/learned_structure/\n",
        "    with regularization_strength_gamma_threshold.json as the file name.\n",
        "\n",
        "    Moreover an earlystopping mechanism has been implemented .\n",
        "\n",
        "    Arguments:\n",
        "      patience: Number of epochs to wait after min has been hit. After this\n",
        "      number of no improvement, training stops.\n",
        "      '''\n",
        "\n",
        "    def __init__(self, cf, network_regularizer, patience=0):\n",
        "        super(export_structure_MN, self).__init__()\n",
        "        self.cf = cf\n",
        "        self.network_regularizer = network_regularizer\n",
        "        self.patience = patience\n",
        "        # exporter to store best architecture.\n",
        "        self.exporter = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # The number of epoch it has waited when loss is no longer minimum.\n",
        "        self.wait = 0\n",
        "        # The epoch the training stops at.\n",
        "        self.stopped_epoch = 0\n",
        "        # Initialize the best as infinity.\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        #get current validation mae\n",
        "        current = logs.get(\"val_mean_absolute_error\")\n",
        "\n",
        "        #compare with previous best one\n",
        "        if np.less(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "            # Record the best model if current results is better (less).\n",
        "            self.exporter = structure_exporter.StructureExporter(self.network_regularizer.op_regularizer_manager)\n",
        "\n",
        "            values = {}\n",
        "            for key, item in self.exporter.tensors.items():\n",
        "              values[key] = tf.keras.backend.eval(item)\n",
        "\n",
        "            self.exporter.populate_tensor_values(values)\n",
        "\n",
        "            self.exporter.create_file_and_save_alive_counts(\n",
        "                self.cf.saving_path,\n",
        "                'learned_channels_{:.1e}'.format(self.cf.reg_strength)+\n",
        "                '_'+\n",
        "                '{:.1e}'.format(self.cf.threshold)+'.json')\n",
        "            # rename file because the exporter.create_file_and_save_alive_counts() methods automatically\n",
        "            # add an unwanted prefix\n",
        "            os.replace(\n",
        "                self.cf.saving_path+'learned_structure/alive_learned_channels_'+\n",
        "                '{:.1e}'.format(self.cf.reg_strength)+\n",
        "                '_'+'{:.1e}'.format(self.cf.threshold)+'.json',\n",
        "                self.cf.saving_path+'learned_structure/learned_channels_'+\n",
        "                '{:.1e}'.format(self.cf.reg_strength)+\n",
        "                '_'+'{:.1e}'.format(self.cf.threshold)+'.json')\n",
        "            path = self.cf.saving_path+'learned_channels_'+'{:.1e}'.format(self.cf.reg_strength)+'_'+'{:.1e}'.format(self.cf.threshold)+'.json'\n",
        "            print('\\nSaving model at:', path)\n",
        "        else:\n",
        "            self.wait += 1\n",
        "            print('\\nval_mae did not improve from {}'.format(self.best))\n",
        "            print('Keep going on for at least {} epochs'.format(self.patience-self.wait))\n",
        "            if self.wait >= self.patience:\n",
        "                self.stopped_epoch = epoch\n",
        "                self.model.stop_training = True\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.stopped_epoch > 0:\n",
        "            print(\"\\n Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4poO7rbMigx",
        "outputId": "ad1b85bb-266a-4eb6-cd37-f7cbded7cd0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing custom_losses.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile custom_losses.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "def NLL(y_true, y_pred):\n",
        "\n",
        "    return -tf.linalg.trace(\n",
        "        tf.matmul(\n",
        "            tf.cast(y_true, dtype='float32'),\n",
        "            tf.transpose(tf.cast(tf.math.log(tf.clip_by_value(y_pred, 1e-8, 1.0)), dtype='float32'), [0, 1, 3, 2])\n",
        "            ) +\n",
        "        tf.matmul(\n",
        "            tf.cast((1 - y_true), dtype='float32'),\n",
        "            tf.transpose(tf.cast(tf.math.log(tf.clip_by_value(1 - y_pred, 1e-8, 1.0)), dtype='float32'), [0, 1, 3, 2])\n",
        "            )\n",
        "        )\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "            if K.ndim(y_true) == K.ndim(y_pred):\n",
        "                y_true = K.squeeze(y_true, -1)\n",
        "            # convert dense predictions to labels\n",
        "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfXKS2rUMw8L",
        "outputId": "e6472664-0d1e-473e-8da2-ea1e75a79b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing eval_flops.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile eval_flops.py\n",
        "\n",
        "import re\n",
        "import sys\n",
        "\n",
        "def get_flops(model):\n",
        "    flops = 0\n",
        "    for i in range(len(model.layers)):\n",
        "        layer = model.get_layer(index=i)\n",
        "\n",
        "        if re.search('conv.+', layer.get_config()['name']) :\n",
        "            #print (layer)\n",
        "\n",
        "            in_shape = layer.input.get_shape().as_list()[2]\n",
        "            out_shape = layer.output.get_shape().as_list()[2]\n",
        "            c_in = layer.input.get_shape().as_list()[3]\n",
        "            c_out = layer.output.get_shape().as_list()[3]\n",
        "            k = layer.get_config()['kernel_size'][1]\n",
        "\n",
        "            flops += 2 * out_shape * k * c_in * c_out\n",
        "\n",
        "        elif re.search('dense.+', layer.get_config()['name']) :\n",
        "            #print (layer)\n",
        "\n",
        "            out_shape = layer.output.get_shape().as_list()[1]\n",
        "            in_shape = layer.input.get_shape().as_list()[1]\n",
        "            # import pdb\n",
        "            # pdb.set_trace()\n",
        "            flops += 2 * out_shape * in_shape\n",
        "\n",
        "        elif re.search('batch_normalization.+', layer.get_config()['name']) or \\\n",
        "            re.search('bn.+', layer.get_config()['name']):\n",
        "            #print (layer)\n",
        "\n",
        "            # bn after conv\n",
        "            if len(layer.output.get_shape().as_list()) == 4:\n",
        "                out_shape = layer.output.get_shape().as_list()[2]\n",
        "                c_out = layer.output.get_shape().as_list()[3]\n",
        "\n",
        "                flops += 2 * 4 * c_out * out_shape\n",
        "            # bn after fc\n",
        "            else:\n",
        "                out_shape = layer.output.get_shape().as_list()[1]\n",
        "\n",
        "                flops += 2 * 4 * out_shape\n",
        "\n",
        "        elif re.search('average_pooling2d.+', layer.get_config()['name']) or \\\n",
        "            re.search('^pool.+', layer.get_config()['name']):\n",
        "\n",
        "            #print(layer)\n",
        "            out_shape = layer.output.get_shape().as_list()[2]\n",
        "            c_out = layer.output.get_shape().as_list()[3]\n",
        "\n",
        "            flops += 2 * out_shape * c_out\n",
        "\n",
        "        elif re.search('act.+', layer.get_config()['name']) :\n",
        "\n",
        "            # act after conv\n",
        "            if len(layer.output.get_shape().as_list()) == 4:\n",
        "                out_shape = layer.output.get_shape().as_list()[2]\n",
        "                c_out = layer.output.get_shape().as_list()[3]\n",
        "\n",
        "                flops += 2 * out_shape * c_out\n",
        "            # act after fc\n",
        "            else:\n",
        "                out_shape = layer.output.get_shape().as_list()[1]\n",
        "\n",
        "                flops += 2 * 4 * out_shape\n",
        "\n",
        "        elif re.search('flatten.+', layer.get_config()['name']) :\n",
        "            pass\n",
        "\n",
        "        elif re.search('drop.+', layer.get_config()['name']) :\n",
        "            pass\n",
        "\n",
        "        elif re.search('zero_padding2d.+', layer.get_config()['name']) or \\\n",
        "            re.search('pad.+', layer.get_config()['name']):\n",
        "            pass\n",
        "\n",
        "        elif re.search('global_average_pooling2d.+', layer.get_config()['name']) or \\\n",
        "            re.search('gpool.+', layer.get_config()['name']):\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            print(\"Unknown layer: {}\".format(layer.get_config()['name']))\n",
        "            sys.exit()\n",
        "\n",
        "    return flops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlWhTk8vM9Wz",
        "outputId": "f032ae9a-a561-4dcc-de4c-e99a1a7f5b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing pit_mn.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile pit_mn.py\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import json\n",
        "from config import Config\n",
        "import sys\n",
        "import pdb\n",
        "\n",
        "import math\n",
        "\n",
        "# aliases\n",
        "val_mae = 'val_mean_absolute_error'\n",
        "mae = 'mean_absolute_error'\n",
        "\n",
        "from tensorflow.keras.optimizers.legacy import Adam, SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from morph_net.network_regularizers import flop_regularizer, model_size_regularizer\n",
        "from morph_net.tools import structure_exporter\n",
        "\n",
        "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "from custom_callbacks import SaveGamma, export_structure, export_structure_MN\n",
        "\n",
        "from preprocessing import preprocessing_Dalia as pp\n",
        "\n",
        "from trainer import train_TEMPONet\n",
        "\n",
        "from models import build_TEMPONet\n",
        "\n",
        "import utils\n",
        "import eval_flops\n",
        "\n",
        "import pickle\n",
        "\n",
        "# MorphNet is compatible only with tf1.x\n",
        "if tf.__version__ != '1.14.0':\n",
        "    import tensorflow.compat.v1 as tf\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "limit = 1024 * 2\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=limit)])\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# PARSER\n",
        "parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n",
        "parser.add_argument('--root', help='Insert the root path where dataset is stored \\\n",
        "                    and where data will be saved')\n",
        "parser.add_argument('--NAS', help='PIT | PIT-Retrain | MN-Size | MN-Flops | Retrain | Fine-Tune')\n",
        "parser.add_argument(\n",
        "    '--learned_ch', nargs='*', type=int, default=None\n",
        ")\n",
        "parser.add_argument('--strength', help='Regularization Strength')\n",
        "parser.add_argument('--threshold', help='Pruning Threshold', default=0.5)\n",
        "parser.add_argument('--warmup', help='Number of warmup epochs', default=0)\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Setup config\n",
        "cf = Config(args.NAS, args.root)\n",
        "cf.search_type = args.NAS\n",
        "cf.reg_strength = float(args.strength)\n",
        "cf.threshold = float(args.threshold)\n",
        "try:\n",
        "    cf.warmup = int(args.warmup)\n",
        "except:\n",
        "    if args.warmup == 'max':\n",
        "        cf.warmup = args.warmup\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "#######\n",
        "# PIT #\n",
        "#######\n",
        "if args.NAS == 'PIT':\n",
        "    # callbacks\n",
        "    save_gamma = SaveGamma()\n",
        "    exp_str = export_structure(cf)\n",
        "    early_stop = EarlyStopping(monitor=val_mae, min_delta=0.01, patience=35, mode='min', verbose=1)\n",
        "\n",
        "    # Load data\n",
        "    X, y, groups, activity = pp.preprocessing(cf.dataset, cf)\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    if args.learned_ch is not None:\n",
        "        ofmap = args.learned_ch\n",
        "\n",
        "    model = build_TEMPONet.TEMPONet_pit(1, cf.input_shape, cf, ofmap=ofmap)\n",
        "    del model\n",
        "    model = build_TEMPONet.TEMPONet_pit(1, cf.input_shape, cf, trainable=False,\n",
        "            ofmap=ofmap)\n",
        "\n",
        "    # save model and weights\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        cf.saving_path+\n",
        "        'weights_strength{}_warmup{}'.format(cf.reg_strength, cf.warmup)+'.h5',\n",
        "        monitor=val_mae, verbose=1,\n",
        "        save_best_only=True, save_weights_only=True, mode='min', period=1)\n",
        "    #configure  model\n",
        "    adam = Adam(lr=cf.lr, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    model.compile(loss='logcosh', optimizer=adam, metrics=[mae])\n",
        "\n",
        "    X_sh, y_sh = shuffle(X, y)\n",
        "\n",
        "    ##########\n",
        "    # Warmup #\n",
        "    ##########\n",
        "    if cf.warmup != 0:\n",
        "        print('Train model for {} epochs'.format(cf.warmup))\n",
        "        strg = cf.reg_strength\n",
        "        cf.reg_strength = 0\n",
        "\n",
        "        if cf.warmup == 'max':\n",
        "            epochs_num = cf.epochs\n",
        "        else:\n",
        "            epochs_num = cf.warmup\n",
        "\n",
        "        warmup_hist = train_TEMPONet.warmup(model, epochs_num, X_sh, y_sh,\n",
        "                              early_stop, checkpoint, cf)\n",
        "        cf.reg_strength = strg\n",
        "\n",
        "    del model\n",
        "    model = build_TEMPONet.TEMPONet_pit(1, cf.input_shape,\n",
        "                                       cf, trainable=True, ofmap=ofmap)\n",
        "    model.compile(loss='logcosh', optimizer=adam, metrics=[mae])\n",
        "\n",
        "    if cf.warmup != 0:\n",
        "        tmp_model = build_TEMPONet.TEMPONet_pit(1, cf.input_shape, cf, trainable=False, ofmap=ofmap)\n",
        "        # load weights in temp model\n",
        "        tmp_model.load_weights(cf.saving_path+\n",
        "                               'weights_strength{}_warmup{}'.format(cf.reg_strength, cf.warmup)+\n",
        "                               '.h5')\n",
        "        utils.copy_weights(model, tmp_model, cf)\n",
        "\n",
        "    ################\n",
        "    # Train gammas #\n",
        "    ################\n",
        "    print('Train on Gammas')\n",
        "    print('Reg strength : {}'.format(cf.reg_strength))\n",
        "    pit_hist = train_TEMPONet.train_gammas(model, X_sh, y_sh, early_stop, save_gamma, exp_str, cf)\n",
        "\n",
        "    # Save hist\n",
        "    try:\n",
        "        with open('warmup_hist.pickle', 'wb') as f:\n",
        "            pickle.dump(warmup_hist.history, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    except:\n",
        "        print('Something goes wrong')\n",
        "\n",
        "    with open('pit_hist.pickle', 'wb') as f:\n",
        "        pickle.dump(pit_hist.history, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    ##############################\n",
        "    # Retrain and cross-validate #\n",
        "    ##############################\n",
        "    tr_model, MAE = train_TEMPONet.retrain_dil(groups, X, y, activity, checkpoint, early_stop, cf, ofmap=ofmap)\n",
        "    print(MAE)\n",
        "    # Evaluate average MAE\n",
        "    avg = 0\n",
        "    for _, val in MAE.items():\n",
        "        avg += val\n",
        "        print(\"Average MAE : %f\", avg/len(MAE))\n",
        "\n",
        "    #######################\n",
        "    # Create summary file #\n",
        "    #######################\n",
        "    f=open(\n",
        "        cf.saving_path+\n",
        "        \"summary_strength{}_warmup{}.txt\".format(cf.reg_strength, cf.warmup),\n",
        "        \"a+\")\n",
        "    f.write(\"regularization strength : {reg_str} \\t warmup : {wu} \\t MAE : {mae} \\t Model size : {size} \\t FLOPS : {flops} \\n\".format(\n",
        "           reg_str = cf.reg_strength,\n",
        "           wu = cf.warmup,\n",
        "           mae = avg/len(MAE),\n",
        "           size = tr_model.count_params(),\n",
        "           flops = eval_flops.get_flops(tr_model)\n",
        "           ))\n",
        "    f.close()\n",
        "\n",
        "elif args.NAS == 'PIT-Retrain':\n",
        "    cf.saving_path = cf.root+'saved_models_PIT/'\n",
        "    # callbacks\n",
        "    save_gamma = SaveGamma()\n",
        "    exp_str = export_structure(cf)\n",
        "    early_stop = EarlyStopping(monitor=val_mae, min_delta=0.01, patience=35, mode='min', verbose=1)\n",
        "    # save model and weights\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        cf.saving_path+\n",
        "        'weights_strength{}_warmup{}'.format(cf.reg_strength, cf.warmup)+'.h5',\n",
        "        monitor=val_mae, verbose=1,\n",
        "        save_best_only=True, save_weights_only=True, mode='min', period=1)\n",
        "\n",
        "    # Load data\n",
        "    X, y, groups, activity = pp.preprocessing(cf.dataset, cf)\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    if args.learned_ch is not None:\n",
        "        ofmap = args.learned_ch\n",
        "\n",
        "    ##############################\n",
        "    # Retrain and cross-validate #\n",
        "    ##############################\n",
        "    tr_model, MAE = train_TEMPONet.retrain_dil(groups, X, y, activity, checkpoint, early_stop, cf, ofmap=ofmap)\n",
        "    print(MAE)\n",
        "    # Evaluate average MAE\n",
        "    avg = 0\n",
        "    for _, val in MAE.items():\n",
        "        avg += val\n",
        "        print(\"Average MAE : %f\", avg/len(MAE))\n",
        "\n",
        "    #######################\n",
        "    # Create summary file #\n",
        "    #######################\n",
        "    f=open(\n",
        "        cf.saving_path+\n",
        "        \"summary_strength{}_warmup{}.txt\".format(cf.reg_strength, cf.warmup),\n",
        "        \"a+\")\n",
        "    f.write(\"regularization strength : {reg_str} \\t warmup : {wu} \\t MAE : {mae} \\t Model size : {size} \\t FLOPS : {flops} \\n\".format(\n",
        "           reg_str = cf.reg_strength,\n",
        "           wu = cf.warmup,\n",
        "           mae = avg/len(MAE),\n",
        "           size = tr_model.count_params(),\n",
        "           flops = eval_flops.get_flops(tr_model)\n",
        "           ))\n",
        "    f.close()\n",
        "\n",
        "######\n",
        "# MN #\n",
        "######\n",
        "elif args.NAS == 'MN-Size' or args.NAS == 'MN-Flops':\n",
        "\n",
        "    # Load data\n",
        "    X, y, groups, activity = pp.preprocessing(cf.dataset, cf)\n",
        "\n",
        "    # Learn channels\n",
        "    model = build_TEMPONet.TEMPONet_mn(1, cf.input_shape,\n",
        "                                       dil_ht=False,\n",
        "                                       dil_list=[], ofmap=[])\n",
        "    del model\n",
        "    model = build_TEMPONet.TEMPONet_mn(1, cf.input_shape,\n",
        "                                       dil_ht=False,\n",
        "                                       dil_list=[], ofmap=[])\n",
        "\n",
        "    # MorphNet definition\n",
        "    if args.NAS == 'MN-Size':\n",
        "        regularizer_fn = model_size_regularizer.GroupLassoModelSizeRegularizer\n",
        "    elif args.NAS == 'MN-Flops':\n",
        "        regularizer_fn = flop_regularizer.GroupLassoFlopsRegularizer\n",
        "    network_regularizer = regularizer_fn(\n",
        "                        output_boundary=[model.output.op],\n",
        "                        input_boundary=[model.input.op],\n",
        "                        threshold=cf.threshold)\n",
        "                        #gamma_threshold=cf.gamma_threshold)\n",
        "\n",
        "    morph_net_loss = network_regularizer.get_regularization_term()*cf.reg_strength\n",
        "\n",
        "    cost = network_regularizer.get_cost()\n",
        "\n",
        "    # add the new loss to the model\n",
        "    model.add_loss(lambda: morph_net_loss)\n",
        "\n",
        "    # add the cost and the new loss as metrics so we can keep track of them\n",
        "    model.add_metric(cost, name='cost', aggregation='mean')\n",
        "    model.add_metric(morph_net_loss, name='morphnet_loss', aggregation='mean')\n",
        "\n",
        "    #configure  model\n",
        "    adam = Adam(lr=5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "    model.compile(loss='logcosh', optimizer=adam, metrics=[mae])\n",
        "\n",
        "    X_sh, y_sh = shuffle(X, y)\n",
        "\n",
        "    # Callbacks\n",
        "    callback_list = [export_structure_MN(cf, network_regularizer, patience=20)]\n",
        "\n",
        "\n",
        "    ###################\n",
        "    # Search Channels #\n",
        "    ###################\n",
        "    print('Search Channels')\n",
        "    print('Reg strength : {}'.format(cf.reg_strength))\n",
        "    mn_hist = train_TEMPONet.morphnet_search(model, X_sh, y_sh, callback_list, cf)\n",
        "\n",
        "    with open('mn_hist.pickle', 'wb') as f:\n",
        "        pickle.dump(mn_hist.history, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "    ##############################\n",
        "    # Retrain and cross-validate #\n",
        "    ##############################\n",
        "    # save model and weights\n",
        "    early_stop = EarlyStopping(monitor=val_mae, min_delta=0.01,\n",
        "                 patience=20, mode='min', verbose=1)\n",
        "\n",
        "    tr_model, MAE = train_TEMPONet.retrain_ch(\n",
        "        groups, X, y, activity, early_stop, cf, ofmap=[])\n",
        "    print(MAE)\n",
        "    # Evaluate average MAE\n",
        "    avg = 0\n",
        "    for _, val in MAE.items():\n",
        "        avg += val\n",
        "        print(\"Average MAE : %f\", avg/len(MAE))\n",
        "\n",
        "    #######################\n",
        "    # Create summary file #\n",
        "    #######################\n",
        "    f=open(\n",
        "        cf.saving_path+\n",
        "        \"summary_strength{}_threshold{}.txt\".format(cf.reg_strength, cf.threshold),\n",
        "        \"a+\")\n",
        "    f.write(\"regularization strength : {reg_str} \\t threshold : {th} \\t MAE : {mae} \\t Model size : {size} \\t FLOPS : {flops} \\n\".format(\n",
        "           reg_str = cf.reg_strength,\n",
        "           th = cf.threshold,\n",
        "           mae = avg/len(MAE),\n",
        "           size = tr_model.count_params(),\n",
        "           flops = eval_flops.get_flops(tr_model)\n",
        "           ))\n",
        "    f.close()\n",
        "\n",
        "elif args.NAS == 'Retrain':\n",
        "    cf.saving_path = cf.root+'saved_models/'\n",
        "    # callbacks\n",
        "    save_gamma = SaveGamma()\n",
        "    exp_str = export_structure(cf)\n",
        "    early_stop = EarlyStopping(monitor=val_mae, min_delta=0.01, patience=35, mode='min', verbose=1)\n",
        "    # save model and weights\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        cf.saving_path+\n",
        "        'weights_strength{}_warmup{}'.format(cf.reg_strength, cf.warmup)+'.h5',\n",
        "        monitor=val_mae, verbose=1,\n",
        "        save_best_only=True, save_weights_only=True, mode='min', period=1)\n",
        "\n",
        "    # Load data\n",
        "    X, y, groups, activity = pp.preprocessing(cf.dataset, cf)\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    # OFMAP\n",
        "    # Could be 'small' or 'medium' or 'large' or 'largest' or 'other'\n",
        "    ofmap_type = 'other'\n",
        "    if ofmap_type == 'small':\n",
        "        ofmap = [\n",
        "            1, 1, 16,\n",
        "            1, 1, 128,\n",
        "            1, 4, 2,\n",
        "            14, 74, 1\n",
        "        ]\n",
        "    elif ofmap_type == 'medium':\n",
        "        ofmap = [\n",
        "            3, 9, 1,\n",
        "            36, 8, 20,\n",
        "            2, 5, 25,\n",
        "            49, 85, 1\n",
        "        ]\n",
        "    elif ofmap_type == 'large':\n",
        "        ofmap = [\n",
        "            27, 26, 60,\n",
        "            58, 64, 80,\n",
        "            27, 29, 38,\n",
        "            44, 57, 1\n",
        "        ]\n",
        "    elif ofmap_type == 'largest':\n",
        "        ofmap = [\n",
        "            32, 32, 63,\n",
        "            62, 64, 128,\n",
        "            89, 45, 38,\n",
        "            50, 61, 1\n",
        "        ]\n",
        "    else:\n",
        "        # BestMAE\n",
        "        ofmap = [\n",
        "            32, 32, 63,\n",
        "            62, 64, 128,\n",
        "            89, 45, 38,\n",
        "            50, 61, 1\n",
        "        ]\n",
        "        dil = [\n",
        "            1, 1, 2,\n",
        "            2, 1,\n",
        "            2, 2\n",
        "        ]\n",
        "\n",
        "        # BestSize\n",
        "        #ofmap = [\n",
        "        #    1, 1, 16,\n",
        "        #    1, 1, 128,\n",
        "        #    1, 4, 2,\n",
        "        #    14, 74, 1\n",
        "        #]\n",
        "        #dil = [\n",
        "        #    2, 2, 4,\n",
        "        #    1, 1,\n",
        "        #    16, 1\n",
        "        #]\n",
        "\n",
        "    ##############################\n",
        "    # Retrain and cross-validate #\n",
        "    ##############################\n",
        "    # input_setup:\n",
        "    # 'normal': 4 channels, 1 PPG + 3 ACC\n",
        "    # 'ppg_only_1': 1 channel, 1 PPG\n",
        "    # 'ppg_only_2': 2 channels, 2 PPG\n",
        "    # 'all': 5 channels, 2 PPG + 3 ACC\n",
        "    input_setup = 'normal'\n",
        "    tr_model, MAE = train_TEMPONet.retrain(groups, X, y, activity, checkpoint, early_stop,\n",
        "        cf, ofmap=ofmap, dil=dil, input_setup = input_setup, test_all_subj = True)\n",
        "    print(MAE)\n",
        "    # Evaluate average MAE\n",
        "    avg = 0\n",
        "    for _, val in MAE.items():\n",
        "        avg += val\n",
        "        print(\"Average MAE : %f\", avg/len(MAE))\n",
        "\n",
        "    #######################\n",
        "    # Create summary file #\n",
        "    #######################\n",
        "    f=open(\n",
        "        cf.saving_path+\n",
        "        \"summary_strength{}_warmup{}.txt\".format(cf.reg_strength, cf.warmup),\n",
        "        \"a+\")\n",
        "    f.write(\"regularization strength : {reg_str} \\t warmup : {wu} \\t MAE : {mae} \\t Model size : {size} \\t FLOPS : {flops} \\n\".format(\n",
        "           reg_str = cf.reg_strength,\n",
        "           wu = cf.warmup,\n",
        "           mae = avg/len(MAE),\n",
        "           size = tr_model.count_params(),\n",
        "           flops = eval_flops.get_flops(tr_model)\n",
        "           ))\n",
        "    f.close()\n",
        "\n",
        "elif args.NAS == 'Fine-Tune':\n",
        "    cf.saving_path = cf.root+'saved_models/'\n",
        "    early_stop = EarlyStopping(monitor=val_mae, min_delta=0.01, patience=35, mode='min', verbose=1)\n",
        "    # save model and weights\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        cf.saving_path+\n",
        "        'weights_strength{}_warmup{}'.format(cf.reg_strength, cf.warmup)+'.h5',\n",
        "        monitor=val_mae, verbose=1,\n",
        "        save_best_only=True, save_weights_only=True, mode='min', period=1)\n",
        "\n",
        "    # Load data\n",
        "    X, y, groups, activity = pp.preprocessing(cf.dataset, cf)\n",
        "\n",
        "    # organize data\n",
        "    group_kfold = GroupKFold(n_splits=4)\n",
        "    group_kfold.get_n_splits(X, y, groups)\n",
        "\n",
        "    # OFMAP\n",
        "    # Could be 'small' or 'medium' or 'large' or 'largest' or 'other'\n",
        "    ofmap_type = 'other'\n",
        "    if ofmap_type == 'small':\n",
        "        ofmap = [\n",
        "            1, 1, 16,\n",
        "            1, 1, 128,\n",
        "            1, 4, 2,\n",
        "            14, 74, 1\n",
        "        ]\n",
        "    elif ofmap_type == 'medium':\n",
        "        ofmap = [\n",
        "            3, 9, 1,\n",
        "            36, 8, 20,\n",
        "            2, 5, 25,\n",
        "            49, 85, 1\n",
        "        ]\n",
        "    elif ofmap_type == 'large':\n",
        "        ofmap = [\n",
        "            27, 26, 60,\n",
        "            58, 64, 80,\n",
        "            27, 29, 38,\n",
        "            44, 57, 1\n",
        "        ]\n",
        "    elif ofmap_type == 'largest':\n",
        "        ofmap = [\n",
        "            32, 32, 63,\n",
        "            62, 64, 128,\n",
        "            89, 45, 38,\n",
        "            50, 61, 1\n",
        "        ]\n",
        "    else:\n",
        "        ofmap = [\n",
        "            32, 32, 63,\n",
        "            62, 64, 128,\n",
        "            89, 45, 38,\n",
        "            50, 61, 1\n",
        "        ]\n",
        "        dil = [\n",
        "            1, 1, 2,\n",
        "            2, 1,\n",
        "            2, 2\n",
        "        ]\n",
        "\n",
        "    ##############################\n",
        "    # Retrain and cross-validate #\n",
        "    ##############################\n",
        "    tr_model, MAE = train_TEMPONet.fine_tune(groups, X, y, activity, checkpoint, early_stop, cf, ofmap=ofmap, dil=dil)\n",
        "    print(MAE)\n",
        "    # Evaluate average MAE\n",
        "    avg = 0\n",
        "    for _, val in MAE.items():\n",
        "        avg += val\n",
        "        print(\"Average MAE : %f\", avg/len(MAE))\n",
        "\n",
        "    #######################\n",
        "    # Create summary file #\n",
        "    #######################\n",
        "    f=open(\n",
        "        cf.saving_path+\n",
        "        \"summary_strength{}_warmup{}_threshold{}.txt\".format(cf.reg_strength, cf.warmup, cf.threshold),\n",
        "        \"a+\")\n",
        "    f.write(\"regularization strength : {reg_str} \\t warmup : {wu} \\t MAE : {mae} \\t Model size : {size} \\t FLOPS : {flops} \\n\".format(\n",
        "           reg_str = cf.reg_strength,\n",
        "           wu = cf.warmup,\n",
        "           mae = avg/len(MAE),\n",
        "           size = tr_model.count_params(),\n",
        "           flops = eval_flops.get_flops(tr_model)\n",
        "           ))\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGHBRA03N3bI",
        "outputId": "5a7ef6b9-0298-4522-bd49-2dce0073628a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing post_proc.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile post_proc.py\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy import signal\n",
        "\n",
        "def obtain_MAE(dataset, fine=False):\n",
        "    pred = '_pred' if not fine else '_pred_fine'\n",
        "    label = '_label' if not fine else '_label_fine'\n",
        "    MAE = []\n",
        "    for pat in np.arange(1,16):\n",
        "        dataset['P' +str(pat) +pred] = dataset['P' +str(pat) +pred]\n",
        "        dataset['P' +str(pat) +label] = dataset['P' +str(pat) +label]\n",
        "        MAE_pat = np.mean(np.abs(dataset['P' +str(pat) +label]-dataset['P' +str(pat) +pred]))\n",
        "        MAE.append(MAE_pat)\n",
        "    MAE = np.asarray(MAE)\n",
        "    print('Mean-Pre: {}'.format(np.mean(MAE)))\n",
        "    print('Median-Pre: {}'.format(np.median(MAE)))\n",
        "    return MAE\n",
        "\n",
        "def post_processing(dataset, fine=False):\n",
        "    n = 10\n",
        "    f_h = 10\n",
        "    f_l = 10\n",
        "    pred = '_pred' if not fine else '_pred_fine'\n",
        "    label = '_label' if not fine else '_label_fine'\n",
        "    MAE_postprocessing = []\n",
        "    for pat in np.arange(1,16):\n",
        "    \told_value = dataset['P' +str(pat) +pred][0]\n",
        "    \tfor i in np.arange(n,len(dataset['P' +str(pat) +label])):\n",
        "    \t\tif np.mean(dataset['P' +str(pat) +pred][i]) > np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100+f_h)/100.0:\n",
        "    \t\t    dataset['P' +str(pat) +pred][i] = np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100+f_h)/100\n",
        "    \t\tif np.mean(dataset['P' +str(pat) +pred][i]) < np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100-f_l)/100.0:\n",
        "    \t\t    dataset['P' +str(pat) +pred][i] = np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100-f_l)/100\n",
        "    \tMAE_pat = np.mean(np.abs(dataset['P' +str(pat) +label]-dataset['P' +str(pat) +pred]))\n",
        "    \tMAE_postprocessing.append(MAE_pat)\n",
        "    MAE_postprocessing = np.asarray(MAE_postprocessing)\n",
        "    print('Mean-Post: {}'.format(np.mean(MAE_postprocessing)))\n",
        "    print('Median-Post: {}'.format(np.median(MAE_postprocessing)))\n",
        "    return MAE_postprocessing\n",
        "\n",
        "path = '5.0e-05_0_data.pickle'\n",
        "with open(path, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "MAE = obtain_MAE(dataset, fine=False)\n",
        "MAE_post = post_processing(dataset, fine=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mf4CPEAOSP4",
        "outputId": "923f0025-008f-4424-846d-0f9e41457094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing search.sh\n"
          ]
        }
      ],
      "source": [
        "%%writefile search.sh\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS MN-Flops --strength 1e-5 --threshold 1e-2\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS MN-Flops --strength 1e-5 --threshold 2.5e-2\n",
        "#\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS MN-Flops --strength 1e-7 --threshold 1e-2\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS MN-Flops --strength 1e-7 --threshold 2.5e-2\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS MN-Flops --strength 1e-7 --threshold 5e-2\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS MN-Flops --strength 1e-7 --threshold 7.5e-2\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS MN-Flops --strength 1e-7 --threshold 1e-1\n",
        "#\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-6 --warmup 0\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 5e-6 --warmup 0\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-5 --warmup 0\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 5e-5 --warmup 0\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 5e-8 --warmup 0\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 1e-7 --warmup 0\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 5e-7 --warmup 0\n",
        "#\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-6 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 5e-6 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/medium/ --NAS PIT --strength 5e-5 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/medium/ --NAS PIT --strength 1e-4 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 7.5e-5 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-4 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-3 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 5e-3 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 5e-7 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 1e-6 --warmup 20\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 5e-6 --warmup 20\n",
        "#\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-3 --warmup 40\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 5e-5 --warmup 40\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 5e-7 --warmup 40\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 1e-6 --warmup 40\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 5e-6 --warmup 40\n",
        "#\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-5 --warmup max\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 5e-5 --warmup max\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 1e-3 --warmup max\n",
        "#python pit_mn.py --root /space/risso/ppg_mixed/ --NAS PIT --strength 5e-3 --warmup max\n",
        "python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 1e-5 --warmup max\n",
        "python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 5e-5 --warmup max\n",
        "python pit_mn.py --root /space/risso/ppg_mixed/large/ --NAS PIT --strength 1e-4 --warmup max\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv6Wb_ZAOfYk",
        "outputId": "27e40e70-7087-4646-c91e-ec807601c668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile utils.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "import math\n",
        "import copy\n",
        "import re\n",
        "import json\n",
        "#import config as cf\n",
        "\n",
        "if tf.__version__ == '1.14.0':\n",
        "    def binarize(x, th):\n",
        "\n",
        "        g = tf.get_default_graph()\n",
        "\n",
        "        with ops.name_scope(\"Binarized\") as name:\n",
        "            with g.gradient_override_map({\"Sign\" : \"Identity\", \"Round\" : \"Identity\"}):\n",
        "                return tf.round((tf.sign(x-th) + 1) / 2)\n",
        "else:\n",
        "    @tf.custom_gradient\n",
        "    def binarize(x, th):\n",
        "        bin_x = tf.round((tf.sign(x-th) + 1) / 2)\n",
        "        def grad(dy):\n",
        "            return tf.identity(x)*dy, None\n",
        "\n",
        "        return bin_x, grad\n",
        "\n",
        "def gamma_mul(dil_fact, it=0, line=[]):\n",
        "\n",
        "    # entry point\n",
        "    if it == 0:\n",
        "        line = list()\n",
        "        line.extend([[1]])\n",
        "        it += 1\n",
        "\n",
        "    # exit point\n",
        "    elif it == int(math.log(dil_fact, 2)):\n",
        "        return line\n",
        "\n",
        "    else:\n",
        "        #it += 1\n",
        "        for pos in range(len(line)):\n",
        "            line[pos].append(0)\n",
        "\n",
        "        line.extend([[0]*(it) + [1]])\n",
        "\n",
        "        line.extend(copy.deepcopy(line[:(2**it-1)]))\n",
        "\n",
        "        it += 1\n",
        "\n",
        "    return gamma_mul(dil_fact, it, line)\n",
        "\n",
        "def prune_mul(kernel, gamma):\n",
        "    eps = 1e-6\n",
        "    kernel_size = kernel.get_shape().as_list()[1]\n",
        "    n_max = math.floor(math.log(kernel_size - eps,2))\n",
        "    dil_fact_max = 2 ** n_max\n",
        "\n",
        "    # gamma_mul matrix gen\n",
        "    matrix_list = list()\n",
        "    sum_list = list()\n",
        "\n",
        "    i = 0\n",
        "    while i < kernel_size:\n",
        "        vector_list = list()\n",
        "\n",
        "        # first element and multiples of dil_fact_max are always not pruned\n",
        "        if i % dil_fact_max == 0:\n",
        "            vector_list.extend([0] * n_max)\n",
        "            matrix_list.append(vector_list)\n",
        "            sum_list.append(1)\n",
        "            i += 1\n",
        "        else:\n",
        "            for line in gamma_mul(dil_fact_max):\n",
        "                matrix_list.append(line)\n",
        "                sum_list.append(0)\n",
        "                i += 1\n",
        "\n",
        "    # Truncate not necessary rows in matrix_list.\n",
        "    # i.e., from kernel_size to end\n",
        "    # if len(matrix_list) == kernel_size, matrix_list[:-0] = [] !!!\n",
        "    if len(matrix_list) != kernel_size:\n",
        "        matrix_list = matrix_list[:-(len(matrix_list)-kernel_size)]\n",
        "        # Same for sum_list\n",
        "        sum_list = sum_list[:-(len(sum_list)-kernel_size)]\n",
        "\n",
        "    mask_mul = tf.transpose(tf.constant(matrix_list,shape=[kernel_size,n_max], dtype='float32'))\n",
        "    mask_sum = tf.constant(sum_list,shape=[kernel_size,1], dtype='float32')\n",
        "\n",
        "    m_1 = tf.constant(\n",
        "        np.flip(\n",
        "            np.triu(\n",
        "                np.ones((n_max, n_max))),\n",
        "            1\n",
        "            ),\n",
        "        dtype='float32'\n",
        "        )\n",
        "\n",
        "    m_2 = tf.constant(\n",
        "        np.flip(\n",
        "            np.tril(\n",
        "                np.ones((n_max, n_max)),\n",
        "                -1\n",
        "                ),\n",
        "            1\n",
        "            ),\n",
        "        dtype='float32'\n",
        "        )\n",
        "\n",
        "    Gamma = tf.add(\n",
        "        tf.math.reduce_prod(\n",
        "            tf.matmul(\n",
        "                tf.add(\n",
        "                    tf.multiply(\n",
        "                        tf.matmul(\n",
        "                            tf.cast(tf.reshape(gamma, [tf.shape(gamma)[1], 1]), dtype='float32'),\n",
        "                            tf.constant(1, shape=(1,gamma.get_shape().as_list()[1]), dtype='float32')\n",
        "                            ),\n",
        "                            m_1\n",
        "                        ),\n",
        "                    m_2\n",
        "                    ),\n",
        "                mask_mul\n",
        "                ),\n",
        "            axis=0\n",
        "            ),\n",
        "        tf.reshape(mask_sum, [mask_sum.get_shape().as_list()[0], ])\n",
        "        )\n",
        "\n",
        "    Gamma = tf.reshape(Gamma, [Gamma.get_shape().as_list()[0], 1])\n",
        "\n",
        "    return tf.transpose(tf.multiply(tf.transpose(Gamma, [0, 1]),\n",
        "                       tf.transpose(kernel, [2, 3, 1, 0])\n",
        "                       ), [3, 2, 0, 1])\n",
        "\n",
        "def dil_fact(arr, op='sum'):\n",
        "    if op == 'sum':\n",
        "        dil = 0\n",
        "        for i in arr.flatten():\n",
        "            if i == 0:\n",
        "                dil +=1\n",
        "            else:\n",
        "                break\n",
        "        return 2 ** dil\n",
        "    else:\n",
        "        dil = 0\n",
        "        for i in reversed(arr.flatten()):\n",
        "            if i == 0:\n",
        "                dil +=1\n",
        "            else:\n",
        "                break\n",
        "        return 2 ** dil\n",
        "\n",
        "def save_dil_fact(saving_path, dil, cf):\n",
        "    f = open(saving_path+'learned_dil_'+\n",
        "             '{:.1e}'.format(cf.reg_strength)+\n",
        "             '_'+'{}'.format(cf.warmup)+'.json','w')\n",
        "    f.write(format_structure(dil))\n",
        "    f.close()\n",
        "\n",
        "def format_structure(dil):\n",
        "  return json.dumps(dil, indent=2, sort_keys=False, default=str)\n",
        "\n",
        "def g_weights(w, c_in, c_out, r_f):\n",
        "    kernel_size = w.get_shape().as_list()[1]\n",
        "\n",
        "    g_w_list = []\n",
        "    for i in range(kernel_size):\n",
        "        g_w_list.append(\n",
        "                c_in * c_out * math.ceil(((r_f - 1) / 2**(max_dil(r_f) - i)) - 0.5)\n",
        "                )\n",
        "    return tf.constant(g_w_list, shape=[1,kernel_size], dtype='float32')\n",
        "\n",
        "def effective_size(model, cf):\n",
        "    actual_gamma = dict()\n",
        "\n",
        "    delta_params = 0\n",
        "    i = 0\n",
        "    names = [weight.name for layer in model.layers for weight in layer.weights]\n",
        "    weights = model.get_weights()\n",
        "    for name, weight in zip(names, weights):\n",
        "        if re.search('learned_conv2d.+_?[0-9]/gamma', name):\n",
        "            actual_gamma[i] = weight\n",
        "            actual_gamma[i] = np.array(actual_gamma[i] > cf.threshold, dtype=bool)\n",
        "            actual_gamma[i] = dil_fact(actual_gamma[i], op='mul')\n",
        "            i += 1\n",
        "\n",
        "    i = 0\n",
        "    for layer in model.layers:\n",
        "        if re.search('learned_conv2d.+_?[0-9]', layer.name):\n",
        "            # weights organized in the returned list as a gamma | kernel | bias\n",
        "            kernels = layer.get_weights()[1]\n",
        "            delta_params += kernels.shape[3] * kernels.shape[2] * (\n",
        "                    kernels.shape[1] - math.ceil(kernels.shape[1] / actual_gamma[i]))\n",
        "            i += 1\n",
        "\n",
        "    return model.count_params() - delta_params\n",
        "\n",
        "def copy_weights(model, tmp_model, cf):\n",
        "    # copy weights from tmp_model to model\n",
        "    # this tedious step is necessary because keras save in last positions non-trainable\n",
        "    # weights, thus passing from non-trainable to trainable generates a mismatch error\n",
        "    # between shapes of array of weights\n",
        "    weight_list = tmp_model.get_weights()\n",
        "    for i, layer in enumerate(tmp_model.layers):\n",
        "        if re.search('learned_conv2d.+_?[0-9]', layer.name):\n",
        "            if cf.hyst == 0:\n",
        "                order = [2, 0, 1]\n",
        "            elif cf.hyst == 1:\n",
        "                order = [2, 0, 1, 3]\n",
        "            ordered_w = [layer.get_weights()[i] for i in order]\n",
        "            model.layers[i].set_weights(ordered_w)\n",
        "        elif re.search('weight_norm.+_?[0-9]', layer.name):\n",
        "            if cf.hyst == 0:\n",
        "                order = [0, 1, 4, 2, 3]\n",
        "            elif cf.hyst == 1:\n",
        "                order = [0, 1, 4, 2, 3, 5]\n",
        "            ordered_w = [layer.get_weights()[i] for i in order]\n",
        "            model.layers[i].set_weights(ordered_w)\n",
        "        else:\n",
        "            model.layers[i].set_weights(layer.get_weights())\n",
        "    return\n",
        "\n",
        "def max_dil(kernel_dim):\n",
        "    eps = 1e-6\n",
        "    return math.floor(math.log(kernel_dim - eps,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr0109rzz8wB"
      },
      "source": [
        "# install MorphNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aG2lMzY0CKI",
        "outputId": "54f58840-015d-41d1-b19e-d0ff10b916ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'morph-net'...\n",
            "remote: Enumerating objects: 1052, done.\u001b[K\n",
            "remote: Counting objects: 100% (143/143), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 1052 (delta 89), reused 114 (delta 78), pack-reused 909\u001b[K\n",
            "Receiving objects: 100% (1052/1052), 6.30 MiB | 18.42 MiB/s, done.\n",
            "Resolving deltas: 100% (720/720), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/architecture_search/morph-net\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: morph-net\n",
            "  Running setup.py develop for morph-net\n",
            "Successfully installed morph-net-0.2.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/google-research/morph-net.git\n",
        "pip install --editable morph-net/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKaQC9lr03jZ"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEU8ZbWKXzYO",
        "outputId": "bf4996e9-fd65-4afb-9dd9-6c1c91ca389c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTYMkanOX1iP"
      },
      "outputs": [],
      "source": [
        "root = \"/content/drive/MyDrive/Thiziri\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IVjVw0GdeEZ"
      },
      "source": [
        "# command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8FqUo2lDiae",
        "outputId": "2bbac013-62ed-4ef5-dd47-5710b61e5718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/architecture_search\n"
          ]
        }
      ],
      "source": [
        "cd /content/architecture_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS8wU2Cu9wxw",
        "outputId": "753cca9a-c78d-4b9f-d066-d548a7ab7cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-24 22:07:20.999239: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-24 22:07:21.943739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-24 22:07:24.596077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-24 22:07:25.123701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-24 22:07:25.124459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "usage: pit_mn.py\n",
            "       [-h]\n",
            "       [--root ROOT]\n",
            "       [--NAS NAS]\n",
            "       [--learned_ch [LEARNED_CH ...]]\n",
            "       [--strength STRENGTH]\n",
            "       [--threshold THRESHOLD]\n",
            "       [--warmup WARMUP]\n",
            "\n",
            "options:\n",
            "  -h, --help\n",
            "    show this help message and exit\n",
            "  --root ROOT\n",
            "    Insert the root path where dataset is stored                     and where data will be saved\n",
            "  --NAS NAS\n",
            "    PIT | PIT-Retrain | MN-Size | MN-Flops | Retrain | Fine-Tune\n",
            "  --learned_ch [LEARNED_CH ...]\n",
            "  --strength STRENGTH\n",
            "    Regularization Strength\n",
            "  --threshold THRESHOLD\n",
            "    Pruning Threshold\n",
            "  --warmup WARMUP\n",
            "    Number of warmup epochs\n"
          ]
        }
      ],
      "source": [
        "!python architecture_search/pit_mn.py --root /content/drive/MyDrive/Thiziri/ --h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYMkP6j3EIv0",
        "outputId": "dd6e836d-5512-4daa-aa79-884ed2e8f4c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-24 22:07:27.534727: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-24 22:07:28.442446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-05-24 22:07:30.282653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-24 22:07:30.314099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-05-24 22:07:30.314418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/architecture_search/pit_mn.py\", line 245, in <module>\n",
            "    X, y, groups, activity = pp.preprocessing(cf.dataset, cf)\n",
            "  File \"/content/architecture_search/preprocessing/preprocessing_Dalia.py\", line 36, in preprocessing\n",
            "    with open(cf.path_PPG_Dalia + 'PPG_FieldStudy/S' + str(j) +'/S' + str(j) +'.pkl', 'rb') as f:\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/Thiziri/PPG_FieldStudy/S15/S15.pkl'\n"
          ]
        }
      ],
      "source": [
        "!python architecture_search/pit_mn.py --root /content/drive/MyDrive/Thiziri/ --NAS MN-Size --strength 1e-6 --threshold 1e-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3asBAblUeqP",
        "outputId": "c6e0698b-b5ff-4d4f-b050-f815f628288a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 5.8388 - mean_absolute_error: 6.4571 - val_loss: 7.1530 - val_mean_absolute_error: 7.7562\n",
            "Epoch 4/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 3.9133 - mean_absolute_error: 4.5089\n",
            "Epoch 4: val_mean_absolute_error improved from 7.75623 to 7.54526, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg3.h5\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 3.9133 - mean_absolute_error: 4.5089 - val_loss: 6.9404 - val_mean_absolute_error: 7.5453\n",
            "Epoch 5/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.4838 - mean_absolute_error: 4.0709\n",
            "Epoch 5: val_mean_absolute_error did not improve from 7.54526\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 3.4824 - mean_absolute_error: 4.0695 - val_loss: 7.5772 - val_mean_absolute_error: 8.1619\n",
            "Epoch 6/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.2486 - mean_absolute_error: 3.8316\n",
            "Epoch 6: val_mean_absolute_error did not improve from 7.54526\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 3.2493 - mean_absolute_error: 3.8324 - val_loss: 8.1303 - val_mean_absolute_error: 8.7189\n",
            "Epoch 7/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.9267 - mean_absolute_error: 3.5049\n",
            "Epoch 7: val_mean_absolute_error did not improve from 7.54526\n",
            "47059/47059 [==============================] - 11s 224us/sample - loss: 2.9273 - mean_absolute_error: 3.5054 - val_loss: 8.0412 - val_mean_absolute_error: 8.6238\n",
            "Epoch 8/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.7422 - mean_absolute_error: 3.3203\n",
            "Epoch 8: val_mean_absolute_error did not improve from 7.54526\n",
            "47059/47059 [==============================] - 10s 215us/sample - loss: 2.7422 - mean_absolute_error: 3.3203 - val_loss: 7.5534 - val_mean_absolute_error: 8.1291\n",
            "Epoch 9/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.5735 - mean_absolute_error: 3.1471\n",
            "Epoch 9: val_mean_absolute_error improved from 7.54526 to 7.48135, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg3.h5\n",
            "47059/47059 [==============================] - 11s 244us/sample - loss: 2.5735 - mean_absolute_error: 3.1471 - val_loss: 6.9101 - val_mean_absolute_error: 7.4814\n",
            "Epoch 10/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.4164 - mean_absolute_error: 2.9905\n",
            "Epoch 10: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 2.4170 - mean_absolute_error: 2.9910 - val_loss: 7.7719 - val_mean_absolute_error: 8.3475\n",
            "Epoch 11/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 2.2942 - mean_absolute_error: 2.8689\n",
            "Epoch 11: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 2.2937 - mean_absolute_error: 2.8685 - val_loss: 7.0109 - val_mean_absolute_error: 7.5726\n",
            "Epoch 12/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.1797 - mean_absolute_error: 2.7505\n",
            "Epoch 12: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 2.1797 - mean_absolute_error: 2.7505 - val_loss: 7.5243 - val_mean_absolute_error: 8.0950\n",
            "Epoch 13/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.0830 - mean_absolute_error: 2.6537\n",
            "Epoch 13: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 10s 215us/sample - loss: 2.0823 - mean_absolute_error: 2.6531 - val_loss: 8.0012 - val_mean_absolute_error: 8.5900\n",
            "Epoch 14/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.9823 - mean_absolute_error: 2.5498\n",
            "Epoch 14: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 227us/sample - loss: 1.9815 - mean_absolute_error: 2.5488 - val_loss: 7.4513 - val_mean_absolute_error: 8.0269\n",
            "Epoch 15/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.9087 - mean_absolute_error: 2.4749\n",
            "Epoch 15: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.9187 - mean_absolute_error: 2.4853 - val_loss: 7.3678 - val_mean_absolute_error: 7.9377\n",
            "Epoch 16/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.9527 - mean_absolute_error: 2.5238\n",
            "Epoch 16: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.9538 - mean_absolute_error: 2.5251 - val_loss: 7.5072 - val_mean_absolute_error: 8.0758\n",
            "Epoch 17/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.8304 - mean_absolute_error: 2.3958\n",
            "Epoch 17: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 1.8301 - mean_absolute_error: 2.3955 - val_loss: 7.3789 - val_mean_absolute_error: 7.9545\n",
            "Epoch 18/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.7535 - mean_absolute_error: 2.3175\n",
            "Epoch 18: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 1.7559 - mean_absolute_error: 2.3201 - val_loss: 7.2034 - val_mean_absolute_error: 7.7707\n",
            "Epoch 19/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.7496 - mean_absolute_error: 2.3154\n",
            "Epoch 19: val_mean_absolute_error did not improve from 7.48135\n",
            "47059/47059 [==============================] - 12s 260us/sample - loss: 1.7496 - mean_absolute_error: 2.3154 - val_loss: 7.5404 - val_mean_absolute_error: 8.1052\n",
            "Epoch 20/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.6637 - mean_absolute_error: 2.2263\n",
            "Epoch 20: val_mean_absolute_error improved from 7.48135 to 7.23483, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg3.h5\n",
            "47059/47059 [==============================] - 10s 220us/sample - loss: 1.6647 - mean_absolute_error: 2.2274 - val_loss: 6.6555 - val_mean_absolute_error: 7.2348\n",
            "Epoch 21/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6665 - mean_absolute_error: 2.2282\n",
            "Epoch 21: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.6665 - mean_absolute_error: 2.2282 - val_loss: 7.8918 - val_mean_absolute_error: 8.4624\n",
            "Epoch 22/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.6258 - mean_absolute_error: 2.1853\n",
            "Epoch 22: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.6275 - mean_absolute_error: 2.1872 - val_loss: 7.4956 - val_mean_absolute_error: 8.0688\n",
            "Epoch 23/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6245 - mean_absolute_error: 2.1879\n",
            "Epoch 23: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 232us/sample - loss: 1.6245 - mean_absolute_error: 2.1879 - val_loss: 7.6303 - val_mean_absolute_error: 8.2052\n",
            "Epoch 24/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5582 - mean_absolute_error: 2.1162\n",
            "Epoch 24: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 234us/sample - loss: 1.5578 - mean_absolute_error: 2.1157 - val_loss: 7.1259 - val_mean_absolute_error: 7.6903\n",
            "Epoch 25/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5491 - mean_absolute_error: 2.1072\n",
            "Epoch 25: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 228us/sample - loss: 1.5491 - mean_absolute_error: 2.1072 - val_loss: 7.3877 - val_mean_absolute_error: 7.9515\n",
            "Epoch 26/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5682 - mean_absolute_error: 2.1301\n",
            "Epoch 26: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 10s 213us/sample - loss: 1.5671 - mean_absolute_error: 2.1289 - val_loss: 7.9090 - val_mean_absolute_error: 8.4985\n",
            "Epoch 27/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.4955 - mean_absolute_error: 2.0498\n",
            "Epoch 27: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.4960 - mean_absolute_error: 2.0505 - val_loss: 7.1011 - val_mean_absolute_error: 7.6788\n",
            "Epoch 28/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.4928 - mean_absolute_error: 2.0500\n",
            "Epoch 28: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 1.4920 - mean_absolute_error: 2.0490 - val_loss: 7.5215 - val_mean_absolute_error: 8.0939\n",
            "Epoch 29/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4991 - mean_absolute_error: 2.0581\n",
            "Epoch 29: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.4987 - mean_absolute_error: 2.0577 - val_loss: 7.1045 - val_mean_absolute_error: 7.6789\n",
            "Epoch 30/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5303 - mean_absolute_error: 2.0910\n",
            "Epoch 30: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.5303 - mean_absolute_error: 2.0910 - val_loss: 7.1702 - val_mean_absolute_error: 7.7445\n",
            "Epoch 31/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5069 - mean_absolute_error: 2.0651\n",
            "Epoch 31: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 10s 219us/sample - loss: 1.5083 - mean_absolute_error: 2.0666 - val_loss: 6.8984 - val_mean_absolute_error: 7.4626\n",
            "Epoch 32/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5688 - mean_absolute_error: 2.1333\n",
            "Epoch 32: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 10s 218us/sample - loss: 1.5725 - mean_absolute_error: 2.1371 - val_loss: 6.8681 - val_mean_absolute_error: 7.4328\n",
            "Epoch 33/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4337 - mean_absolute_error: 1.9902\n",
            "Epoch 33: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 234us/sample - loss: 1.4334 - mean_absolute_error: 1.9900 - val_loss: 7.3524 - val_mean_absolute_error: 7.9270\n",
            "Epoch 34/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4554 - mean_absolute_error: 2.0132\n",
            "Epoch 34: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 241us/sample - loss: 1.4554 - mean_absolute_error: 2.0132 - val_loss: 6.9723 - val_mean_absolute_error: 7.5444\n",
            "Epoch 35/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4472 - mean_absolute_error: 2.0043\n",
            "Epoch 35: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.4472 - mean_absolute_error: 2.0043 - val_loss: 7.2308 - val_mean_absolute_error: 7.7913\n",
            "Epoch 36/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4324 - mean_absolute_error: 1.9883\n",
            "Epoch 36: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.4316 - mean_absolute_error: 1.9875 - val_loss: 7.5927 - val_mean_absolute_error: 8.1558\n",
            "Epoch 37/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3738 - mean_absolute_error: 1.9247\n",
            "Epoch 37: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 10s 208us/sample - loss: 1.3738 - mean_absolute_error: 1.9247 - val_loss: 7.0280 - val_mean_absolute_error: 7.5886\n",
            "Epoch 38/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3964 - mean_absolute_error: 1.9515\n",
            "Epoch 38: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 231us/sample - loss: 1.4021 - mean_absolute_error: 1.9574 - val_loss: 6.7983 - val_mean_absolute_error: 7.3547\n",
            "Epoch 39/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.4320 - mean_absolute_error: 1.9899\n",
            "Epoch 39: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.4307 - mean_absolute_error: 1.9885 - val_loss: 7.2061 - val_mean_absolute_error: 7.7802\n",
            "Epoch 40/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.4539 - mean_absolute_error: 2.0133\n",
            "Epoch 40: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 234us/sample - loss: 1.4510 - mean_absolute_error: 2.0100 - val_loss: 7.0374 - val_mean_absolute_error: 7.5970\n",
            "Epoch 41/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3691 - mean_absolute_error: 1.9209\n",
            "Epoch 41: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 13s 266us/sample - loss: 1.3704 - mean_absolute_error: 1.9224 - val_loss: 6.9883 - val_mean_absolute_error: 7.5641\n",
            "Epoch 42/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3749 - mean_absolute_error: 1.9279\n",
            "Epoch 42: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.3735 - mean_absolute_error: 1.9263 - val_loss: 7.2331 - val_mean_absolute_error: 7.7970\n",
            "Epoch 43/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3852 - mean_absolute_error: 1.9402\n",
            "Epoch 43: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 10s 219us/sample - loss: 1.3852 - mean_absolute_error: 1.9402 - val_loss: 7.3538 - val_mean_absolute_error: 7.9416\n",
            "Epoch 44/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3819 - mean_absolute_error: 1.9368\n",
            "Epoch 44: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 10s 220us/sample - loss: 1.3819 - mean_absolute_error: 1.9368 - val_loss: 6.7594 - val_mean_absolute_error: 7.3219\n",
            "Epoch 45/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4022 - mean_absolute_error: 1.9619\n",
            "Epoch 45: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 232us/sample - loss: 1.4016 - mean_absolute_error: 1.9613 - val_loss: 7.0147 - val_mean_absolute_error: 7.5916\n",
            "Epoch 46/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3169 - mean_absolute_error: 1.8670\n",
            "Epoch 46: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 1.3174 - mean_absolute_error: 1.8676 - val_loss: 7.7711 - val_mean_absolute_error: 8.3371\n",
            "Epoch 47/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4380 - mean_absolute_error: 1.9959\n",
            "Epoch 47: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.4380 - mean_absolute_error: 1.9959 - val_loss: 7.2598 - val_mean_absolute_error: 7.8289\n",
            "Epoch 48/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3786 - mean_absolute_error: 1.9329\n",
            "Epoch 48: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 231us/sample - loss: 1.3896 - mean_absolute_error: 1.9441 - val_loss: 7.3773 - val_mean_absolute_error: 7.9519\n",
            "Epoch 49/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3263 - mean_absolute_error: 1.8797\n",
            "Epoch 49: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 10s 209us/sample - loss: 1.3234 - mean_absolute_error: 1.8764 - val_loss: 6.9085 - val_mean_absolute_error: 7.4767\n",
            "Epoch 50/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3239 - mean_absolute_error: 1.8791\n",
            "Epoch 50: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.3239 - mean_absolute_error: 1.8791 - val_loss: 7.1907 - val_mean_absolute_error: 7.7613\n",
            "Epoch 51/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3432 - mean_absolute_error: 1.8957\n",
            "Epoch 51: val_mean_absolute_error did not improve from 7.23483\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 1.3432 - mean_absolute_error: 1.8957 - val_loss: 7.4173 - val_mean_absolute_error: 7.9877\n",
            "Epoch 52/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3243 - mean_absolute_error: 1.8768\n",
            "Epoch 52: val_mean_absolute_error improved from 7.23483 to 7.21129, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg3.h5\n",
            "47059/47059 [==============================] - 12s 245us/sample - loss: 1.3254 - mean_absolute_error: 1.8780 - val_loss: 6.6436 - val_mean_absolute_error: 7.2113\n",
            "Epoch 53/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2917 - mean_absolute_error: 1.8438\n",
            "Epoch 53: val_mean_absolute_error did not improve from 7.21129\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.2908 - mean_absolute_error: 1.8428 - val_loss: 7.3951 - val_mean_absolute_error: 7.9556\n",
            "Epoch 54/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3429 - mean_absolute_error: 1.8970\n",
            "Epoch 54: val_mean_absolute_error did not improve from 7.21129\n",
            "47059/47059 [==============================] - 11s 228us/sample - loss: 1.3429 - mean_absolute_error: 1.8970 - val_loss: 7.3444 - val_mean_absolute_error: 7.9048\n",
            "Epoch 55/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2227 - mean_absolute_error: 1.7687\n",
            "Epoch 55: val_mean_absolute_error did not improve from 7.21129\n",
            "47059/47059 [==============================] - 10s 210us/sample - loss: 1.2225 - mean_absolute_error: 1.7684 - val_loss: 7.3531 - val_mean_absolute_error: 7.9239\n",
            "Epoch 56/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3326 - mean_absolute_error: 1.8882\n",
            "Epoch 56: val_mean_absolute_error did not improve from 7.21129\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.3345 - mean_absolute_error: 1.8901 - val_loss: 6.6714 - val_mean_absolute_error: 7.2332\n",
            "Epoch 57/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2356 - mean_absolute_error: 1.7818\n",
            "Epoch 57: val_mean_absolute_error did not improve from 7.21129\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 1.2351 - mean_absolute_error: 1.7813 - val_loss: 6.8196 - val_mean_absolute_error: 7.3901\n",
            "Epoch 58/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3055 - mean_absolute_error: 1.8563\n",
            "Epoch 58: val_mean_absolute_error improved from 7.21129 to 6.98769, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg3.h5\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 1.3044 - mean_absolute_error: 1.8553 - val_loss: 6.4312 - val_mean_absolute_error: 6.9877\n",
            "Epoch 59/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2674 - mean_absolute_error: 1.8144\n",
            "Epoch 59: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.2649 - mean_absolute_error: 1.8116 - val_loss: 7.1657 - val_mean_absolute_error: 7.7276\n",
            "Epoch 60/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3424 - mean_absolute_error: 1.8991\n",
            "Epoch 60: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 227us/sample - loss: 1.3424 - mean_absolute_error: 1.8991 - val_loss: 6.8444 - val_mean_absolute_error: 7.4110\n",
            "Epoch 61/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2805 - mean_absolute_error: 1.8286\n",
            "Epoch 61: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 10s 214us/sample - loss: 1.2795 - mean_absolute_error: 1.8275 - val_loss: 7.0983 - val_mean_absolute_error: 7.6612\n",
            "Epoch 62/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2398 - mean_absolute_error: 1.7903\n",
            "Epoch 62: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.2411 - mean_absolute_error: 1.7919 - val_loss: 6.7167 - val_mean_absolute_error: 7.2925\n",
            "Epoch 63/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2371 - mean_absolute_error: 1.7800\n",
            "Epoch 63: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 12s 265us/sample - loss: 1.2371 - mean_absolute_error: 1.7800 - val_loss: 6.9149 - val_mean_absolute_error: 7.4803\n",
            "Epoch 64/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2885 - mean_absolute_error: 1.8405\n",
            "Epoch 64: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.2881 - mean_absolute_error: 1.8401 - val_loss: 6.9884 - val_mean_absolute_error: 7.5571\n",
            "Epoch 65/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3437 - mean_absolute_error: 1.8982\n",
            "Epoch 65: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.3456 - mean_absolute_error: 1.9003 - val_loss: 7.0476 - val_mean_absolute_error: 7.6182\n",
            "Epoch 66/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2769 - mean_absolute_error: 1.8272\n",
            "Epoch 66: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 231us/sample - loss: 1.2769 - mean_absolute_error: 1.8272 - val_loss: 7.2806 - val_mean_absolute_error: 7.8466\n",
            "Epoch 67/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3042 - mean_absolute_error: 1.8556\n",
            "Epoch 67: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 10s 211us/sample - loss: 1.3044 - mean_absolute_error: 1.8560 - val_loss: 7.1126 - val_mean_absolute_error: 7.6744\n",
            "Epoch 68/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2331 - mean_absolute_error: 1.7785\n",
            "Epoch 68: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.2322 - mean_absolute_error: 1.7776 - val_loss: 7.0783 - val_mean_absolute_error: 7.6508\n",
            "Epoch 69/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2751 - mean_absolute_error: 1.8287\n",
            "Epoch 69: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.2751 - mean_absolute_error: 1.8287 - val_loss: 7.3087 - val_mean_absolute_error: 7.8828\n",
            "Epoch 70/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2884 - mean_absolute_error: 1.8408\n",
            "Epoch 70: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.2912 - mean_absolute_error: 1.8440 - val_loss: 7.1021 - val_mean_absolute_error: 7.6672\n",
            "Epoch 71/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2928 - mean_absolute_error: 1.8455\n",
            "Epoch 71: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.2917 - mean_absolute_error: 1.8442 - val_loss: 6.7079 - val_mean_absolute_error: 7.2649\n",
            "Epoch 72/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.1878 - mean_absolute_error: 1.7325\n",
            "Epoch 72: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 10s 215us/sample - loss: 1.1871 - mean_absolute_error: 1.7318 - val_loss: 6.8927 - val_mean_absolute_error: 7.4544\n",
            "Epoch 73/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3042 - mean_absolute_error: 1.8571\n",
            "Epoch 73: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 229us/sample - loss: 1.3079 - mean_absolute_error: 1.8610 - val_loss: 7.3572 - val_mean_absolute_error: 7.9264\n",
            "Epoch 74/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2524 - mean_absolute_error: 1.8046\n",
            "Epoch 74: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 1.2524 - mean_absolute_error: 1.8046 - val_loss: 7.3629 - val_mean_absolute_error: 7.9297\n",
            "Epoch 75/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.1685 - mean_absolute_error: 1.7116\n",
            "Epoch 75: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 241us/sample - loss: 1.1685 - mean_absolute_error: 1.7116 - val_loss: 6.8560 - val_mean_absolute_error: 7.4161\n",
            "Epoch 76/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.1210 - mean_absolute_error: 1.6574\n",
            "Epoch 76: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.1200 - mean_absolute_error: 1.6563 - val_loss: 7.0131 - val_mean_absolute_error: 7.5759\n",
            "Epoch 77/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2087 - mean_absolute_error: 1.7516\n",
            "Epoch 77: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 1.2139 - mean_absolute_error: 1.7573 - val_loss: 6.9421 - val_mean_absolute_error: 7.5106\n",
            "Epoch 78/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.1874 - mean_absolute_error: 1.7320\n",
            "Epoch 78: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 10s 210us/sample - loss: 1.1889 - mean_absolute_error: 1.7337 - val_loss: 7.0997 - val_mean_absolute_error: 7.6602\n",
            "Epoch 79/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2542 - mean_absolute_error: 1.8050\n",
            "Epoch 79: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 229us/sample - loss: 1.2523 - mean_absolute_error: 1.8029 - val_loss: 7.1615 - val_mean_absolute_error: 7.7235\n",
            "Epoch 80/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.1329 - mean_absolute_error: 1.6703\n",
            "Epoch 80: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.1329 - mean_absolute_error: 1.6703 - val_loss: 7.3891 - val_mean_absolute_error: 7.9647\n",
            "Epoch 81/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2404 - mean_absolute_error: 1.7887\n",
            "Epoch 81: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.2404 - mean_absolute_error: 1.7887 - val_loss: 6.9493 - val_mean_absolute_error: 7.5152\n",
            "Epoch 82/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.1109 - mean_absolute_error: 1.6471\n",
            "Epoch 82: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.1109 - mean_absolute_error: 1.6471 - val_loss: 7.2493 - val_mean_absolute_error: 7.8073\n",
            "Epoch 83/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2228 - mean_absolute_error: 1.7736\n",
            "Epoch 83: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.2243 - mean_absolute_error: 1.7753 - val_loss: 7.0603 - val_mean_absolute_error: 7.6232\n",
            "Epoch 84/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2268 - mean_absolute_error: 1.7746\n",
            "Epoch 84: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 244us/sample - loss: 1.2266 - mean_absolute_error: 1.7745 - val_loss: 6.9243 - val_mean_absolute_error: 7.4859\n",
            "Epoch 85/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.1180 - mean_absolute_error: 1.6588\n",
            "Epoch 85: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 10s 222us/sample - loss: 1.1180 - mean_absolute_error: 1.6588 - val_loss: 7.0953 - val_mean_absolute_error: 7.6584\n",
            "Epoch 86/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2382 - mean_absolute_error: 1.7923\n",
            "Epoch 86: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 1.2395 - mean_absolute_error: 1.7939 - val_loss: 6.9954 - val_mean_absolute_error: 7.5695\n",
            "Epoch 87/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2485 - mean_absolute_error: 1.7998\n",
            "Epoch 87: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 237us/sample - loss: 1.2485 - mean_absolute_error: 1.7998 - val_loss: 7.0646 - val_mean_absolute_error: 7.6274\n",
            "Epoch 88/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.1630 - mean_absolute_error: 1.7037\n",
            "Epoch 88: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.1625 - mean_absolute_error: 1.7033 - val_loss: 7.4915 - val_mean_absolute_error: 8.0524\n",
            "Epoch 89/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2385 - mean_absolute_error: 1.7830\n",
            "Epoch 89: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.2374 - mean_absolute_error: 1.7817 - val_loss: 7.0704 - val_mean_absolute_error: 7.6360\n",
            "Epoch 90/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2016 - mean_absolute_error: 1.7463\n",
            "Epoch 90: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 10s 208us/sample - loss: 1.2016 - mean_absolute_error: 1.7463 - val_loss: 7.1834 - val_mean_absolute_error: 7.7549\n",
            "Epoch 91/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.1883 - mean_absolute_error: 1.7334\n",
            "Epoch 91: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 227us/sample - loss: 1.1980 - mean_absolute_error: 1.7434 - val_loss: 6.9544 - val_mean_absolute_error: 7.5208\n",
            "Epoch 92/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.1643 - mean_absolute_error: 1.7065\n",
            "Epoch 92: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.1645 - mean_absolute_error: 1.7069 - val_loss: 7.1435 - val_mean_absolute_error: 7.7227\n",
            "Epoch 93/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.1853 - mean_absolute_error: 1.7281\n",
            "Epoch 93: val_mean_absolute_error did not improve from 6.98769\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.1853 - mean_absolute_error: 1.7281 - val_loss: 7.1741 - val_mean_absolute_error: 7.7447\n",
            "Epoch 93: early stopping\n",
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2023-05-26 11:04:58.374408: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_14/BiasAdd' id:10324 op device:{requested: '', assigned: ''} def:{{{node dense_14/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_14/MatMul, dense_14/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373}\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_17 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_55 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_56 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_3 (ZeroPaddi  (None, 1, 260, 32)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_15 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_57 (Activation)  (None, 1, 128, 63)        0         \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 1, 128, 63)       252       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_58 (Activation)  (None, 1, 128, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 1, 128, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_59 (Activation)  (None, 1, 128, 62)        0         \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 1, 128, 62)       248       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_4 (ZeroPaddi  (None, 1, 132, 62)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_16 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_60 (Activation)  (None, 1, 32, 120)        0         \n",
            "                                                                 \n",
            " batch_normalization_60 (Bat  (None, 1, 32, 120)       480       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_61 (Activation)  (None, 1, 32, 59)         0         \n",
            "                                                                 \n",
            " batch_normalization_61 (Bat  (None, 1, 32, 59)        236       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_24 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_62 (Activation)  (None, 1, 32, 27)         0         \n",
            "                                                                 \n",
            " batch_normalization_62 (Bat  (None, 1, 32, 27)        108       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_5 (ZeroPaddi  (None, 1, 37, 27)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_25 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_17 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_63 (Activation)  (None, 1, 4, 28)          0         \n",
            "                                                                 \n",
            " batch_normalization_63 (Bat  (None, 1, 4, 28)         112       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 112)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_64 (Activation)  (None, 38)                0         \n",
            "                                                                 \n",
            " batch_normalization_64 (Bat  (None, 38)               152       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_65 (Activation)  (None, 51)                0         \n",
            "                                                                 \n",
            " batch_normalization_65 (Bat  (None, 51)               204       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(47059, 4, 256)\n",
            "(12989, 4, 256)\n",
            "(4649, 4, 256)\n",
            "Train on 47059 samples, validate on 12989 samples\n",
            "2023-05-26 11:05:03.440576: W tensorflow/c/c_api.cc:300] Operation '{name:'training_6/Adam/dense_17/kernel/m/Assign' id:13390 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/dense_17/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/dense_17/kernel/m, training_6/Adam/dense_17/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 80.4536 - mean_absolute_error: 81.14682023-05-26 11:05:16.326663: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_3/mul' id:12635 op device:{requested: '', assigned: ''} def:{{{node loss_3/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_3/mul/x, loss_3/dense_17_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 62.22464, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg5.h5\n",
            "47059/47059 [==============================] - 20s 425us/sample - loss: 80.4536 - mean_absolute_error: 81.1468 - val_loss: 61.5315 - val_mean_absolute_error: 62.2246\n",
            "Epoch 2/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 44.1908 - mean_absolute_error: 44.8826\n",
            "Epoch 2: val_mean_absolute_error improved from 62.22464 to 38.04469, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg5.h5\n",
            "47059/47059 [==============================] - 12s 254us/sample - loss: 44.1908 - mean_absolute_error: 44.8826 - val_loss: 37.3552 - val_mean_absolute_error: 38.0447\n",
            "Epoch 3/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 5.7438 - mean_absolute_error: 6.3601\n",
            "Epoch 3: val_mean_absolute_error improved from 38.04469 to 4.59409, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg5.h5\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 5.7408 - mean_absolute_error: 6.3571 - val_loss: 4.0040 - val_mean_absolute_error: 4.5941\n",
            "Epoch 4/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 3.7557 - mean_absolute_error: 4.3463\n",
            "Epoch 4: val_mean_absolute_error did not improve from 4.59409\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 3.7557 - mean_absolute_error: 4.3463 - val_loss: 4.8864 - val_mean_absolute_error: 5.4685\n",
            "Epoch 5/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.4109 - mean_absolute_error: 3.9936\n",
            "Epoch 5: val_mean_absolute_error improved from 4.59409 to 3.46105, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg5.h5\n",
            "47059/47059 [==============================] - 11s 226us/sample - loss: 3.4149 - mean_absolute_error: 3.9976 - val_loss: 2.8992 - val_mean_absolute_error: 3.4611\n",
            "Epoch 6/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 3.2242 - mean_absolute_error: 3.8073\n",
            "Epoch 6: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 11s 229us/sample - loss: 3.2271 - mean_absolute_error: 3.8103 - val_loss: 3.5245 - val_mean_absolute_error: 4.0936\n",
            "Epoch 7/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.9709 - mean_absolute_error: 3.5499\n",
            "Epoch 7: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 11s 241us/sample - loss: 2.9723 - mean_absolute_error: 3.5514 - val_loss: 4.0959 - val_mean_absolute_error: 4.6626\n",
            "Epoch 8/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.7446 - mean_absolute_error: 3.3221\n",
            "Epoch 8: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 2.7446 - mean_absolute_error: 3.3221 - val_loss: 3.3418 - val_mean_absolute_error: 3.8866\n",
            "Epoch 9/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.5952 - mean_absolute_error: 3.1707\n",
            "Epoch 9: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 2.5959 - mean_absolute_error: 3.1715 - val_loss: 3.5193 - val_mean_absolute_error: 4.0865\n",
            "Epoch 10/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.4762 - mean_absolute_error: 3.0515\n",
            "Epoch 10: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 2.4760 - mean_absolute_error: 3.0513 - val_loss: 3.6244 - val_mean_absolute_error: 4.1656\n",
            "Epoch 11/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.4010 - mean_absolute_error: 2.9782\n",
            "Epoch 11: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 10s 222us/sample - loss: 2.4014 - mean_absolute_error: 2.9787 - val_loss: 3.1971 - val_mean_absolute_error: 3.7507\n",
            "Epoch 12/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.2556 - mean_absolute_error: 2.8271\n",
            "Epoch 12: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 2.2553 - mean_absolute_error: 2.8269 - val_loss: 3.5777 - val_mean_absolute_error: 4.1415\n",
            "Epoch 13/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 2.1488 - mean_absolute_error: 2.7202\n",
            "Epoch 13: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 2.1485 - mean_absolute_error: 2.7200 - val_loss: 3.3776 - val_mean_absolute_error: 3.9170\n",
            "Epoch 14/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.0801 - mean_absolute_error: 2.6522\n",
            "Epoch 14: val_mean_absolute_error did not improve from 3.46105\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 2.0801 - mean_absolute_error: 2.6522 - val_loss: 3.3977 - val_mean_absolute_error: 3.9521\n",
            "Epoch 15/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.9466 - mean_absolute_error: 2.5148\n",
            "Epoch 15: val_mean_absolute_error improved from 3.46105 to 3.40435, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg5.h5\n",
            "47059/47059 [==============================] - 12s 263us/sample - loss: 1.9466 - mean_absolute_error: 2.5148 - val_loss: 2.8596 - val_mean_absolute_error: 3.4044\n",
            "Epoch 16/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.8989 - mean_absolute_error: 2.4651\n",
            "Epoch 16: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 254us/sample - loss: 1.8995 - mean_absolute_error: 2.4657 - val_loss: 4.2362 - val_mean_absolute_error: 4.8003\n",
            "Epoch 17/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.9012 - mean_absolute_error: 2.4681\n",
            "Epoch 17: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 255us/sample - loss: 1.9070 - mean_absolute_error: 2.4741 - val_loss: 3.5851 - val_mean_absolute_error: 4.1472\n",
            "Epoch 18/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.8569 - mean_absolute_error: 2.4259\n",
            "Epoch 18: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 1.8554 - mean_absolute_error: 2.4242 - val_loss: 3.9064 - val_mean_absolute_error: 4.4691\n",
            "Epoch 19/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.7637 - mean_absolute_error: 2.3269\n",
            "Epoch 19: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.7638 - mean_absolute_error: 2.3270 - val_loss: 3.3977 - val_mean_absolute_error: 3.9395\n",
            "Epoch 20/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.7151 - mean_absolute_error: 2.2790\n",
            "Epoch 20: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 246us/sample - loss: 1.7151 - mean_absolute_error: 2.2790 - val_loss: 3.5692 - val_mean_absolute_error: 4.1154\n",
            "Epoch 21/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.7058 - mean_absolute_error: 2.2728\n",
            "Epoch 21: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 1.7075 - mean_absolute_error: 2.2747 - val_loss: 3.5510 - val_mean_absolute_error: 4.1090\n",
            "Epoch 22/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.6733 - mean_absolute_error: 2.2381\n",
            "Epoch 22: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 241us/sample - loss: 1.6733 - mean_absolute_error: 2.2381 - val_loss: 3.3136 - val_mean_absolute_error: 3.8659\n",
            "Epoch 23/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6432 - mean_absolute_error: 2.2082\n",
            "Epoch 23: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 1.6432 - mean_absolute_error: 2.2082 - val_loss: 3.8408 - val_mean_absolute_error: 4.3906\n",
            "Epoch 24/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6027 - mean_absolute_error: 2.1648\n",
            "Epoch 24: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 241us/sample - loss: 1.6027 - mean_absolute_error: 2.1648 - val_loss: 3.0369 - val_mean_absolute_error: 3.5840\n",
            "Epoch 25/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5634 - mean_absolute_error: 2.1263\n",
            "Epoch 25: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 10s 212us/sample - loss: 1.5623 - mean_absolute_error: 2.1251 - val_loss: 3.3305 - val_mean_absolute_error: 3.8772\n",
            "Epoch 26/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5653 - mean_absolute_error: 2.1273\n",
            "Epoch 26: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 245us/sample - loss: 1.5653 - mean_absolute_error: 2.1273 - val_loss: 3.2207 - val_mean_absolute_error: 3.7676\n",
            "Epoch 27/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5460 - mean_absolute_error: 2.1063\n",
            "Epoch 27: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 246us/sample - loss: 1.5460 - mean_absolute_error: 2.1063 - val_loss: 3.3247 - val_mean_absolute_error: 3.8762\n",
            "Epoch 28/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5964 - mean_absolute_error: 2.1614\n",
            "Epoch 28: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 1.5964 - mean_absolute_error: 2.1614 - val_loss: 3.5383 - val_mean_absolute_error: 4.1023\n",
            "Epoch 29/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5131 - mean_absolute_error: 2.0731\n",
            "Epoch 29: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 1.5131 - mean_absolute_error: 2.0731 - val_loss: 3.2564 - val_mean_absolute_error: 3.8034\n",
            "Epoch 30/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5478 - mean_absolute_error: 2.1106\n",
            "Epoch 30: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 1.5478 - mean_absolute_error: 2.1106 - val_loss: 3.0840 - val_mean_absolute_error: 3.6238\n",
            "Epoch 31/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4419 - mean_absolute_error: 1.9994\n",
            "Epoch 31: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 231us/sample - loss: 1.4419 - mean_absolute_error: 1.9994 - val_loss: 3.7536 - val_mean_absolute_error: 4.3406\n",
            "Epoch 32/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4733 - mean_absolute_error: 2.0319\n",
            "Epoch 32: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 13s 275us/sample - loss: 1.4748 - mean_absolute_error: 2.0335 - val_loss: 3.6160 - val_mean_absolute_error: 4.1689\n",
            "Epoch 33/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4123 - mean_absolute_error: 1.9674\n",
            "Epoch 33: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 228us/sample - loss: 1.4169 - mean_absolute_error: 1.9722 - val_loss: 3.8319 - val_mean_absolute_error: 4.3870\n",
            "Epoch 34/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5039 - mean_absolute_error: 2.0658\n",
            "Epoch 34: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.5033 - mean_absolute_error: 2.0651 - val_loss: 3.4600 - val_mean_absolute_error: 4.0159\n",
            "Epoch 35/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3965 - mean_absolute_error: 1.9503\n",
            "Epoch 35: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 247us/sample - loss: 1.3965 - mean_absolute_error: 1.9503 - val_loss: 3.7035 - val_mean_absolute_error: 4.2511\n",
            "Epoch 36/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4054 - mean_absolute_error: 1.9600\n",
            "Epoch 36: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 246us/sample - loss: 1.4075 - mean_absolute_error: 1.9623 - val_loss: 3.4756 - val_mean_absolute_error: 4.0375\n",
            "Epoch 37/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4228 - mean_absolute_error: 1.9793\n",
            "Epoch 37: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 244us/sample - loss: 1.4224 - mean_absolute_error: 1.9789 - val_loss: 3.4282 - val_mean_absolute_error: 3.9764\n",
            "Epoch 38/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3492 - mean_absolute_error: 1.9009\n",
            "Epoch 38: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 1.3483 - mean_absolute_error: 1.8999 - val_loss: 3.7634 - val_mean_absolute_error: 4.3119\n",
            "Epoch 39/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4158 - mean_absolute_error: 1.9746\n",
            "Epoch 39: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 245us/sample - loss: 1.4179 - mean_absolute_error: 1.9767 - val_loss: 3.3194 - val_mean_absolute_error: 3.8705\n",
            "Epoch 40/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3909 - mean_absolute_error: 1.9486\n",
            "Epoch 40: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 10s 223us/sample - loss: 1.3912 - mean_absolute_error: 1.9490 - val_loss: 3.6871 - val_mean_absolute_error: 4.2354\n",
            "Epoch 41/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3790 - mean_absolute_error: 1.9360\n",
            "Epoch 41: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 1.3790 - mean_absolute_error: 1.9360 - val_loss: 3.6191 - val_mean_absolute_error: 4.1706\n",
            "Epoch 42/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3804 - mean_absolute_error: 1.9341\n",
            "Epoch 42: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 248us/sample - loss: 1.3801 - mean_absolute_error: 1.9338 - val_loss: 3.4622 - val_mean_absolute_error: 4.0214\n",
            "Epoch 43/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3702 - mean_absolute_error: 1.9245\n",
            "Epoch 43: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 1.3681 - mean_absolute_error: 1.9221 - val_loss: 3.2089 - val_mean_absolute_error: 3.7467\n",
            "Epoch 44/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3495 - mean_absolute_error: 1.9046\n",
            "Epoch 44: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 244us/sample - loss: 1.3482 - mean_absolute_error: 1.9032 - val_loss: 3.3725 - val_mean_absolute_error: 3.9161\n",
            "Epoch 45/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3061 - mean_absolute_error: 1.8572\n",
            "Epoch 45: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 246us/sample - loss: 1.3061 - mean_absolute_error: 1.8572 - val_loss: 3.4174 - val_mean_absolute_error: 3.9634\n",
            "Epoch 46/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3255 - mean_absolute_error: 1.8792\n",
            "Epoch 46: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.3255 - mean_absolute_error: 1.8792 - val_loss: 3.2992 - val_mean_absolute_error: 3.8506\n",
            "Epoch 47/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3562 - mean_absolute_error: 1.9103\n",
            "Epoch 47: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 241us/sample - loss: 1.3562 - mean_absolute_error: 1.9103 - val_loss: 3.5890 - val_mean_absolute_error: 4.1327\n",
            "Epoch 48/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2761 - mean_absolute_error: 1.8240\n",
            "Epoch 48: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 11s 231us/sample - loss: 1.2761 - mean_absolute_error: 1.8240 - val_loss: 4.0170 - val_mean_absolute_error: 4.5693\n",
            "Epoch 49/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3839 - mean_absolute_error: 1.9410\n",
            "Epoch 49: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 1.3881 - mean_absolute_error: 1.9453 - val_loss: 3.7157 - val_mean_absolute_error: 4.2701\n",
            "Epoch 50/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2728 - mean_absolute_error: 1.8229\n",
            "Epoch 50: val_mean_absolute_error did not improve from 3.40435\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 1.2728 - mean_absolute_error: 1.8229 - val_loss: 3.0978 - val_mean_absolute_error: 3.6436\n",
            "Epoch 50: early stopping\n",
            "2023-05-26 11:14:45.578500: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_17/BiasAdd' id:12584 op device:{requested: '', assigned: ''} def:{{{node dense_17/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_17/MatMul, dense_17/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012}\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_26 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_66 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_66 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_67 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_67 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_6 (ZeroPaddi  (None, 1, 260, 32)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_18 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_68 (Activation)  (None, 1, 128, 63)        0         \n",
            "                                                                 \n",
            " batch_normalization_68 (Bat  (None, 1, 128, 63)       252       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_69 (Activation)  (None, 1, 128, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_69 (Bat  (None, 1, 128, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_70 (Activation)  (None, 1, 128, 62)        0         \n",
            "                                                                 \n",
            " batch_normalization_70 (Bat  (None, 1, 128, 62)       248       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_7 (ZeroPaddi  (None, 1, 132, 62)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_19 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_71 (Activation)  (None, 1, 32, 120)        0         \n",
            "                                                                 \n",
            " batch_normalization_71 (Bat  (None, 1, 32, 120)       480       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_72 (Activation)  (None, 1, 32, 59)         0         \n",
            "                                                                 \n",
            " batch_normalization_72 (Bat  (None, 1, 32, 59)        236       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_73 (Activation)  (None, 1, 32, 27)         0         \n",
            "                                                                 \n",
            " batch_normalization_73 (Bat  (None, 1, 32, 27)        108       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_8 (ZeroPaddi  (None, 1, 37, 27)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_20 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_74 (Activation)  (None, 1, 4, 28)          0         \n",
            "                                                                 \n",
            " batch_normalization_74 (Bat  (None, 1, 4, 28)         112       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 112)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_75 (Activation)  (None, 38)                0         \n",
            "                                                                 \n",
            " batch_normalization_75 (Bat  (None, 38)               152       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_76 (Activation)  (None, 51)                0         \n",
            "                                                                 \n",
            " batch_normalization_76 (Bat  (None, 51)               204       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(47059, 4, 256)\n",
            "(12970, 4, 256)\n",
            "(4668, 4, 256)\n",
            "Train on 47059 samples, validate on 12970 samples\n",
            "2023-05-26 11:14:51.591725: W tensorflow/c/c_api.cc:300] Operation '{name:'training_8/Adam/batch_normalization_67/gamma/v/Assign' id:15692 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/batch_normalization_67/gamma/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/batch_normalization_67/gamma/v, training_8/Adam/batch_normalization_67/gamma/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 80.3174 - mean_absolute_error: 81.01052023-05-26 11:15:05.183292: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_4/mul' id:14895 op device:{requested: '', assigned: ''} def:{{{node loss_4/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul/x, loss_4/dense_20_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 75.83824, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 22s 468us/sample - loss: 80.2576 - mean_absolute_error: 80.9507 - val_loss: 75.1451 - val_mean_absolute_error: 75.8382\n",
            "Epoch 2/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 43.8366 - mean_absolute_error: 44.5284\n",
            "Epoch 2: val_mean_absolute_error improved from 75.83824 to 28.19981, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 43.7912 - mean_absolute_error: 44.4831 - val_loss: 27.5149 - val_mean_absolute_error: 28.1998\n",
            "Epoch 3/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 5.8822 - mean_absolute_error: 6.5039\n",
            "Epoch 3: val_mean_absolute_error improved from 28.19981 to 8.34402, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 5.8804 - mean_absolute_error: 6.5021 - val_loss: 7.7365 - val_mean_absolute_error: 8.3440\n",
            "Epoch 4/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.9705 - mean_absolute_error: 4.5659\n",
            "Epoch 4: val_mean_absolute_error did not improve from 8.34402\n",
            "47059/47059 [==============================] - 12s 246us/sample - loss: 3.9740 - mean_absolute_error: 4.5694 - val_loss: 8.4809 - val_mean_absolute_error: 9.0600\n",
            "Epoch 5/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 3.5734 - mean_absolute_error: 4.1610\n",
            "Epoch 5: val_mean_absolute_error did not improve from 8.34402\n",
            "47059/47059 [==============================] - 11s 244us/sample - loss: 3.5734 - mean_absolute_error: 4.1610 - val_loss: 7.8562 - val_mean_absolute_error: 8.4260\n",
            "Epoch 6/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.2702 - mean_absolute_error: 3.8529\n",
            "Epoch 6: val_mean_absolute_error improved from 8.34402 to 8.20933, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 12s 258us/sample - loss: 3.2692 - mean_absolute_error: 3.8518 - val_loss: 7.6373 - val_mean_absolute_error: 8.2093\n",
            "Epoch 7/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.0085 - mean_absolute_error: 3.5870\n",
            "Epoch 7: val_mean_absolute_error improved from 8.20933 to 8.02762, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 12s 256us/sample - loss: 3.0096 - mean_absolute_error: 3.5882 - val_loss: 7.4651 - val_mean_absolute_error: 8.0276\n",
            "Epoch 8/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.8113 - mean_absolute_error: 3.3870\n",
            "Epoch 8: val_mean_absolute_error did not improve from 8.02762\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 2.8113 - mean_absolute_error: 3.3870 - val_loss: 7.5991 - val_mean_absolute_error: 8.1780\n",
            "Epoch 9/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.6226 - mean_absolute_error: 3.1952\n",
            "Epoch 9: val_mean_absolute_error did not improve from 8.02762\n",
            "47059/47059 [==============================] - 11s 229us/sample - loss: 2.6226 - mean_absolute_error: 3.1952 - val_loss: 8.4954 - val_mean_absolute_error: 9.0799\n",
            "Epoch 10/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.4454 - mean_absolute_error: 3.0146\n",
            "Epoch 10: val_mean_absolute_error did not improve from 8.02762\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 2.4454 - mean_absolute_error: 3.0146 - val_loss: 7.8507 - val_mean_absolute_error: 8.4237\n",
            "Epoch 11/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.3575 - mean_absolute_error: 2.9250\n",
            "Epoch 11: val_mean_absolute_error did not improve from 8.02762\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 2.3599 - mean_absolute_error: 2.9276 - val_loss: 7.9326 - val_mean_absolute_error: 8.4989\n",
            "Epoch 12/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.2030 - mean_absolute_error: 2.7710\n",
            "Epoch 12: val_mean_absolute_error did not improve from 8.02762\n",
            "47059/47059 [==============================] - 12s 251us/sample - loss: 2.2030 - mean_absolute_error: 2.7710 - val_loss: 7.7131 - val_mean_absolute_error: 8.2675\n",
            "Epoch 13/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.1586 - mean_absolute_error: 2.7277\n",
            "Epoch 13: val_mean_absolute_error did not improve from 8.02762\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 2.1594 - mean_absolute_error: 2.7286 - val_loss: 8.0322 - val_mean_absolute_error: 8.5920\n",
            "Epoch 14/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.0445 - mean_absolute_error: 2.6104\n",
            "Epoch 14: val_mean_absolute_error did not improve from 8.02762\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 2.0456 - mean_absolute_error: 2.6115 - val_loss: 8.0498 - val_mean_absolute_error: 8.6258\n",
            "Epoch 15/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.9775 - mean_absolute_error: 2.5417\n",
            "Epoch 15: val_mean_absolute_error improved from 8.02762 to 7.95331, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 12s 255us/sample - loss: 1.9772 - mean_absolute_error: 2.5414 - val_loss: 7.4039 - val_mean_absolute_error: 7.9533\n",
            "Epoch 16/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.9340 - mean_absolute_error: 2.4991\n",
            "Epoch 16: val_mean_absolute_error did not improve from 7.95331\n",
            "47059/47059 [==============================] - 12s 246us/sample - loss: 1.9340 - mean_absolute_error: 2.4991 - val_loss: 7.9775 - val_mean_absolute_error: 8.5452\n",
            "Epoch 17/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.8878 - mean_absolute_error: 2.4522\n",
            "Epoch 17: val_mean_absolute_error improved from 7.95331 to 7.62424, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 11s 234us/sample - loss: 1.8883 - mean_absolute_error: 2.4527 - val_loss: 7.0581 - val_mean_absolute_error: 7.6242\n",
            "Epoch 18/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.8472 - mean_absolute_error: 2.4114\n",
            "Epoch 18: val_mean_absolute_error did not improve from 7.62424\n",
            "47059/47059 [==============================] - 12s 245us/sample - loss: 1.8485 - mean_absolute_error: 2.4129 - val_loss: 7.4995 - val_mean_absolute_error: 8.0732\n",
            "Epoch 19/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.8134 - mean_absolute_error: 2.3771\n",
            "Epoch 19: val_mean_absolute_error did not improve from 7.62424\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.8134 - mean_absolute_error: 2.3772 - val_loss: 7.7195 - val_mean_absolute_error: 8.3048\n",
            "Epoch 20/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.7944 - mean_absolute_error: 2.3600\n",
            "Epoch 20: val_mean_absolute_error did not improve from 7.62424\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.8015 - mean_absolute_error: 2.3673 - val_loss: 8.2455 - val_mean_absolute_error: 8.8206\n",
            "Epoch 21/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.8086 - mean_absolute_error: 2.3767\n",
            "Epoch 21: val_mean_absolute_error did not improve from 7.62424\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 1.8148 - mean_absolute_error: 2.3832 - val_loss: 7.5704 - val_mean_absolute_error: 8.1285\n",
            "Epoch 22/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6385 - mean_absolute_error: 2.1962\n",
            "Epoch 22: val_mean_absolute_error did not improve from 7.62424\n",
            "47059/47059 [==============================] - 12s 249us/sample - loss: 1.6385 - mean_absolute_error: 2.1962 - val_loss: 7.1392 - val_mean_absolute_error: 7.6983\n",
            "Epoch 23/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6174 - mean_absolute_error: 2.1760\n",
            "Epoch 23: val_mean_absolute_error did not improve from 7.62424\n",
            "47059/47059 [==============================] - 12s 248us/sample - loss: 1.6174 - mean_absolute_error: 2.1760 - val_loss: 7.4318 - val_mean_absolute_error: 7.9928\n",
            "Epoch 24/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6725 - mean_absolute_error: 2.2370\n",
            "Epoch 24: val_mean_absolute_error did not improve from 7.62424\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.6725 - mean_absolute_error: 2.2370 - val_loss: 8.3051 - val_mean_absolute_error: 8.8640\n",
            "Epoch 25/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6289 - mean_absolute_error: 2.1888\n",
            "Epoch 25: val_mean_absolute_error improved from 7.62424 to 7.55998, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg7.h5\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 1.6289 - mean_absolute_error: 2.1888 - val_loss: 7.0042 - val_mean_absolute_error: 7.5600\n",
            "Epoch 26/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5574 - mean_absolute_error: 2.1161\n",
            "Epoch 26: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.5574 - mean_absolute_error: 2.1161 - val_loss: 7.8229 - val_mean_absolute_error: 8.3934\n",
            "Epoch 27/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5416 - mean_absolute_error: 2.0984\n",
            "Epoch 27: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 258us/sample - loss: 1.5412 - mean_absolute_error: 2.0980 - val_loss: 7.6831 - val_mean_absolute_error: 8.2570\n",
            "Epoch 28/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5181 - mean_absolute_error: 2.0734\n",
            "Epoch 28: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 255us/sample - loss: 1.5181 - mean_absolute_error: 2.0734 - val_loss: 7.0675 - val_mean_absolute_error: 7.6264\n",
            "Epoch 29/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5416 - mean_absolute_error: 2.1042\n",
            "Epoch 29: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 1.5433 - mean_absolute_error: 2.1061 - val_loss: 7.6907 - val_mean_absolute_error: 8.2584\n",
            "Epoch 30/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5662 - mean_absolute_error: 2.1303\n",
            "Epoch 30: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 254us/sample - loss: 1.5662 - mean_absolute_error: 2.1304 - val_loss: 7.4095 - val_mean_absolute_error: 7.9814\n",
            "Epoch 31/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5748 - mean_absolute_error: 2.1375\n",
            "Epoch 31: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.5737 - mean_absolute_error: 2.1364 - val_loss: 7.7154 - val_mean_absolute_error: 8.3012\n",
            "Epoch 32/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3948 - mean_absolute_error: 1.9455\n",
            "Epoch 32: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 254us/sample - loss: 1.3948 - mean_absolute_error: 1.9455 - val_loss: 7.1376 - val_mean_absolute_error: 7.6872\n",
            "Epoch 33/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4975 - mean_absolute_error: 2.0543\n",
            "Epoch 33: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.4964 - mean_absolute_error: 2.0530 - val_loss: 7.3793 - val_mean_absolute_error: 7.9361\n",
            "Epoch 34/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3952 - mean_absolute_error: 1.9447\n",
            "Epoch 34: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 229us/sample - loss: 1.3950 - mean_absolute_error: 1.9445 - val_loss: 7.9737 - val_mean_absolute_error: 8.5623\n",
            "Epoch 35/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4364 - mean_absolute_error: 1.9918\n",
            "Epoch 35: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 1.4353 - mean_absolute_error: 1.9907 - val_loss: 7.7788 - val_mean_absolute_error: 8.3285\n",
            "Epoch 36/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4568 - mean_absolute_error: 2.0123\n",
            "Epoch 36: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 257us/sample - loss: 1.4604 - mean_absolute_error: 2.0160 - val_loss: 7.2853 - val_mean_absolute_error: 7.8515\n",
            "Epoch 37/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4000 - mean_absolute_error: 1.9531\n",
            "Epoch 37: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 251us/sample - loss: 1.4013 - mean_absolute_error: 1.9545 - val_loss: 7.6529 - val_mean_absolute_error: 8.2208\n",
            "Epoch 38/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4250 - mean_absolute_error: 1.9792\n",
            "Epoch 38: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.4250 - mean_absolute_error: 1.9792 - val_loss: 7.6169 - val_mean_absolute_error: 8.1771\n",
            "Epoch 39/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3744 - mean_absolute_error: 1.9256\n",
            "Epoch 39: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.3736 - mean_absolute_error: 1.9247 - val_loss: 7.4278 - val_mean_absolute_error: 7.9787\n",
            "Epoch 40/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4392 - mean_absolute_error: 1.9981\n",
            "Epoch 40: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 251us/sample - loss: 1.4392 - mean_absolute_error: 1.9981 - val_loss: 7.7319 - val_mean_absolute_error: 8.3123\n",
            "Epoch 41/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3106 - mean_absolute_error: 1.8588\n",
            "Epoch 41: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.3106 - mean_absolute_error: 1.8588 - val_loss: 7.3995 - val_mean_absolute_error: 7.9528\n",
            "Epoch 42/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3237 - mean_absolute_error: 1.8724\n",
            "Epoch 42: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 244us/sample - loss: 1.3230 - mean_absolute_error: 1.8717 - val_loss: 7.6748 - val_mean_absolute_error: 8.2429\n",
            "Epoch 43/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4000 - mean_absolute_error: 1.9519\n",
            "Epoch 43: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 233us/sample - loss: 1.4000 - mean_absolute_error: 1.9519 - val_loss: 7.0138 - val_mean_absolute_error: 7.5855\n",
            "Epoch 44/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3778 - mean_absolute_error: 1.9293\n",
            "Epoch 44: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 1.3769 - mean_absolute_error: 1.9284 - val_loss: 7.8518 - val_mean_absolute_error: 8.4240\n",
            "Epoch 45/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3833 - mean_absolute_error: 1.9384\n",
            "Epoch 45: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.3808 - mean_absolute_error: 1.9355 - val_loss: 7.4850 - val_mean_absolute_error: 8.0562\n",
            "Epoch 46/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3727 - mean_absolute_error: 1.9260\n",
            "Epoch 46: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 254us/sample - loss: 1.3727 - mean_absolute_error: 1.9260 - val_loss: 7.3962 - val_mean_absolute_error: 7.9638\n",
            "Epoch 47/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2769 - mean_absolute_error: 1.8222\n",
            "Epoch 47: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.2758 - mean_absolute_error: 1.8209 - val_loss: 7.2961 - val_mean_absolute_error: 7.8545\n",
            "Epoch 48/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3356 - mean_absolute_error: 1.8900\n",
            "Epoch 48: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 252us/sample - loss: 1.3356 - mean_absolute_error: 1.8900 - val_loss: 7.7563 - val_mean_absolute_error: 8.3085\n",
            "Epoch 49/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3304 - mean_absolute_error: 1.8803\n",
            "Epoch 49: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.3348 - mean_absolute_error: 1.8850 - val_loss: 7.6404 - val_mean_absolute_error: 8.1982\n",
            "Epoch 50/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3553 - mean_absolute_error: 1.9083\n",
            "Epoch 50: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 1.3554 - mean_absolute_error: 1.9085 - val_loss: 7.7061 - val_mean_absolute_error: 8.2808\n",
            "Epoch 51/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2905 - mean_absolute_error: 1.8394\n",
            "Epoch 51: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 228us/sample - loss: 1.2912 - mean_absolute_error: 1.8402 - val_loss: 7.3126 - val_mean_absolute_error: 7.8852\n",
            "Epoch 52/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2477 - mean_absolute_error: 1.7950\n",
            "Epoch 52: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 248us/sample - loss: 1.2494 - mean_absolute_error: 1.7969 - val_loss: 7.6118 - val_mean_absolute_error: 8.1696\n",
            "Epoch 53/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3432 - mean_absolute_error: 1.8972\n",
            "Epoch 53: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.3427 - mean_absolute_error: 1.8967 - val_loss: 7.2854 - val_mean_absolute_error: 7.8427\n",
            "Epoch 54/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3054 - mean_absolute_error: 1.8577\n",
            "Epoch 54: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 1.3105 - mean_absolute_error: 1.8631 - val_loss: 7.2484 - val_mean_absolute_error: 7.8019\n",
            "Epoch 55/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2292 - mean_absolute_error: 1.7741\n",
            "Epoch 55: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 251us/sample - loss: 1.2282 - mean_absolute_error: 1.7729 - val_loss: 7.7405 - val_mean_absolute_error: 8.3042\n",
            "Epoch 56/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2834 - mean_absolute_error: 1.8317\n",
            "Epoch 56: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 248us/sample - loss: 1.2826 - mean_absolute_error: 1.8309 - val_loss: 7.8191 - val_mean_absolute_error: 8.3688\n",
            "Epoch 57/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2055 - mean_absolute_error: 1.7492\n",
            "Epoch 57: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 251us/sample - loss: 1.2055 - mean_absolute_error: 1.7492 - val_loss: 7.3198 - val_mean_absolute_error: 7.8764\n",
            "Epoch 58/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3140 - mean_absolute_error: 1.8644\n",
            "Epoch 58: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 232us/sample - loss: 1.3140 - mean_absolute_error: 1.8644 - val_loss: 7.3525 - val_mean_absolute_error: 7.9219\n",
            "Epoch 59/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2951 - mean_absolute_error: 1.8455\n",
            "Epoch 59: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.2959 - mean_absolute_error: 1.8465 - val_loss: 7.1398 - val_mean_absolute_error: 7.6923\n",
            "Epoch 60/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3116 - mean_absolute_error: 1.8666\n",
            "Epoch 60: val_mean_absolute_error did not improve from 7.55998\n",
            "47059/47059 [==============================] - 12s 254us/sample - loss: 1.3116 - mean_absolute_error: 1.8666 - val_loss: 7.5159 - val_mean_absolute_error: 8.0740\n",
            "Epoch 60: early stopping\n",
            "2023-05-26 11:26:43.562683: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_20/BiasAdd' id:14844 op device:{requested: '', assigned: ''} def:{{{node dense_20/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_20/MatMul, dense_20/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614}\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_35 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_77 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_77 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_36 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_78 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_78 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_9 (ZeroPaddi  (None, 1, 260, 32)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_21 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_79 (Activation)  (None, 1, 128, 63)        0         \n",
            "                                                                 \n",
            " batch_normalization_79 (Bat  (None, 1, 128, 63)       252       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_80 (Activation)  (None, 1, 128, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_80 (Bat  (None, 1, 128, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_39 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_81 (Activation)  (None, 1, 128, 62)        0         \n",
            "                                                                 \n",
            " batch_normalization_81 (Bat  (None, 1, 128, 62)       248       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_10 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_22 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_82 (Activation)  (None, 1, 32, 120)        0         \n",
            "                                                                 \n",
            " batch_normalization_82 (Bat  (None, 1, 32, 120)       480       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_83 (Activation)  (None, 1, 32, 59)         0         \n",
            "                                                                 \n",
            " batch_normalization_83 (Bat  (None, 1, 32, 59)        236       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_42 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_84 (Activation)  (None, 1, 32, 27)         0         \n",
            "                                                                 \n",
            " batch_normalization_84 (Bat  (None, 1, 32, 27)        108       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_11 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_23 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_85 (Activation)  (None, 1, 4, 28)          0         \n",
            "                                                                 \n",
            " batch_normalization_85 (Bat  (None, 1, 4, 28)         112       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 112)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_86 (Activation)  (None, 38)                0         \n",
            "                                                                 \n",
            " batch_normalization_86 (Bat  (None, 38)               152       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_87 (Activation)  (None, 51)                0         \n",
            "                                                                 \n",
            " batch_normalization_87 (Bat  (None, 51)               204       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(47059, 4, 256)\n",
            "(13684, 4, 256)\n",
            "(3954, 4, 256)\n",
            "Train on 47059 samples, validate on 13684 samples\n",
            "2023-05-26 11:26:50.026539: W tensorflow/c/c_api.cc:300] Operation '{name:'training_10/Adam/conv2d_37/kernel/v/Assign' id:17964 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/conv2d_37/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_10/Adam/conv2d_37/kernel/v, training_10/Adam/conv2d_37/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 80.3502 - mean_absolute_error: 81.04332023-05-26 11:27:04.423741: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_5/mul' id:17155 op device:{requested: '', assigned: ''} def:{{{node loss_5/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul/x, loss_5/dense_23_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 81.88050, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg12.h5\n",
            "47059/47059 [==============================] - 25s 528us/sample - loss: 80.3502 - mean_absolute_error: 81.0433 - val_loss: 81.1874 - val_mean_absolute_error: 81.8805\n",
            "Epoch 2/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 43.9819 - mean_absolute_error: 44.6734\n",
            "Epoch 2: val_mean_absolute_error improved from 81.88050 to 36.31271, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg12.h5\n",
            "47059/47059 [==============================] - 12s 263us/sample - loss: 43.9819 - mean_absolute_error: 44.6734 - val_loss: 35.6267 - val_mean_absolute_error: 36.3127\n",
            "Epoch 3/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 5.9967 - mean_absolute_error: 6.6187\n",
            "Epoch 3: val_mean_absolute_error improved from 36.31271 to 6.62846, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg12.h5\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 5.9940 - mean_absolute_error: 6.6159 - val_loss: 6.0409 - val_mean_absolute_error: 6.6285\n",
            "Epoch 4/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 3.9775 - mean_absolute_error: 4.5717\n",
            "Epoch 4: val_mean_absolute_error improved from 6.62846 to 5.85830, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg12.h5\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 3.9775 - mean_absolute_error: 4.5717 - val_loss: 5.2846 - val_mean_absolute_error: 5.8583\n",
            "Epoch 5/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.5581 - mean_absolute_error: 4.1449\n",
            "Epoch 5: val_mean_absolute_error did not improve from 5.85830\n",
            "47059/47059 [==============================] - 12s 250us/sample - loss: 3.5572 - mean_absolute_error: 4.1439 - val_loss: 7.9479 - val_mean_absolute_error: 8.5274\n",
            "Epoch 6/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 3.2764 - mean_absolute_error: 3.8597\n",
            "Epoch 6: val_mean_absolute_error did not improve from 5.85830\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 3.2764 - mean_absolute_error: 3.8597 - val_loss: 6.2461 - val_mean_absolute_error: 6.8323\n",
            "Epoch 7/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 3.0565 - mean_absolute_error: 3.6370\n",
            "Epoch 7: val_mean_absolute_error improved from 5.85830 to 5.84408, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg12.h5\n",
            "47059/47059 [==============================] - 12s 259us/sample - loss: 3.0563 - mean_absolute_error: 3.6369 - val_loss: 5.2782 - val_mean_absolute_error: 5.8441\n",
            "Epoch 8/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 2.8666 - mean_absolute_error: 3.4442\n",
            "Epoch 8: val_mean_absolute_error did not improve from 5.84408\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 2.8706 - mean_absolute_error: 3.4485 - val_loss: 6.7093 - val_mean_absolute_error: 7.2887\n",
            "Epoch 9/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.6967 - mean_absolute_error: 3.2722\n",
            "Epoch 9: val_mean_absolute_error did not improve from 5.84408\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 2.6967 - mean_absolute_error: 3.2722 - val_loss: 6.1359 - val_mean_absolute_error: 6.6923\n",
            "Epoch 10/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.4689 - mean_absolute_error: 3.0438\n",
            "Epoch 10: val_mean_absolute_error did not improve from 5.84408\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 2.4689 - mean_absolute_error: 3.0438 - val_loss: 5.9012 - val_mean_absolute_error: 6.4725\n",
            "Epoch 11/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 2.4763 - mean_absolute_error: 3.0540\n",
            "Epoch 11: val_mean_absolute_error improved from 5.84408 to 5.71240, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg12.h5\n",
            "47059/47059 [==============================] - 11s 234us/sample - loss: 2.4763 - mean_absolute_error: 3.0540 - val_loss: 5.1625 - val_mean_absolute_error: 5.7124\n",
            "Epoch 12/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.2627 - mean_absolute_error: 2.8355\n",
            "Epoch 12: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 10s 220us/sample - loss: 2.2643 - mean_absolute_error: 2.8372 - val_loss: 5.6266 - val_mean_absolute_error: 6.1912\n",
            "Epoch 13/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.1782 - mean_absolute_error: 2.7513\n",
            "Epoch 13: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 2.1867 - mean_absolute_error: 2.7599 - val_loss: 6.1284 - val_mean_absolute_error: 6.6917\n",
            "Epoch 14/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 2.0217 - mean_absolute_error: 2.5915\n",
            "Epoch 14: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 2.0223 - mean_absolute_error: 2.5920 - val_loss: 5.6460 - val_mean_absolute_error: 6.1967\n",
            "Epoch 15/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.9331 - mean_absolute_error: 2.4994\n",
            "Epoch 15: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 12s 245us/sample - loss: 1.9331 - mean_absolute_error: 2.4994 - val_loss: 6.1823 - val_mean_absolute_error: 6.7419\n",
            "Epoch 16/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.8675 - mean_absolute_error: 2.4318\n",
            "Epoch 16: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 12s 247us/sample - loss: 1.8689 - mean_absolute_error: 2.4333 - val_loss: 5.9149 - val_mean_absolute_error: 6.4679\n",
            "Epoch 17/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.9098 - mean_absolute_error: 2.4811\n",
            "Epoch 17: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 1.9098 - mean_absolute_error: 2.4811 - val_loss: 6.1858 - val_mean_absolute_error: 6.7537\n",
            "Epoch 18/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.8636 - mean_absolute_error: 2.4341\n",
            "Epoch 18: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 10s 214us/sample - loss: 1.8636 - mean_absolute_error: 2.4341 - val_loss: 5.1860 - val_mean_absolute_error: 5.7378\n",
            "Epoch 19/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.7825 - mean_absolute_error: 2.3493\n",
            "Epoch 19: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.7804 - mean_absolute_error: 2.3469 - val_loss: 5.2248 - val_mean_absolute_error: 5.7757\n",
            "Epoch 20/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.8207 - mean_absolute_error: 2.3917\n",
            "Epoch 20: val_mean_absolute_error did not improve from 5.71240\n",
            "47059/47059 [==============================] - 11s 241us/sample - loss: 1.8195 - mean_absolute_error: 2.3903 - val_loss: 6.1003 - val_mean_absolute_error: 6.6720\n",
            "Epoch 21/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.7514 - mean_absolute_error: 2.3173\n",
            "Epoch 21: val_mean_absolute_error improved from 5.71240 to 5.15005, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg12.h5\n",
            "47059/47059 [==============================] - 12s 251us/sample - loss: 1.7556 - mean_absolute_error: 2.3217 - val_loss: 4.6042 - val_mean_absolute_error: 5.1501\n",
            "Epoch 22/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.7411 - mean_absolute_error: 2.3107\n",
            "Epoch 22: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 1.7411 - mean_absolute_error: 2.3107 - val_loss: 5.4021 - val_mean_absolute_error: 5.9532\n",
            "Epoch 23/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.6469 - mean_absolute_error: 2.2096\n",
            "Epoch 23: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.6472 - mean_absolute_error: 2.2101 - val_loss: 5.7442 - val_mean_absolute_error: 6.3033\n",
            "Epoch 24/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.6467 - mean_absolute_error: 2.2118\n",
            "Epoch 24: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 10s 210us/sample - loss: 1.6467 - mean_absolute_error: 2.2118 - val_loss: 4.9019 - val_mean_absolute_error: 5.4585\n",
            "Epoch 25/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5945 - mean_absolute_error: 2.1562\n",
            "Epoch 25: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 228us/sample - loss: 1.5965 - mean_absolute_error: 2.1584 - val_loss: 5.1556 - val_mean_absolute_error: 5.7006\n",
            "Epoch 26/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5995 - mean_absolute_error: 2.1617\n",
            "Epoch 26: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.5995 - mean_absolute_error: 2.1617 - val_loss: 5.6541 - val_mean_absolute_error: 6.1961\n",
            "Epoch 27/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.5829 - mean_absolute_error: 2.1454\n",
            "Epoch 27: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.5808 - mean_absolute_error: 2.1432 - val_loss: 5.9342 - val_mean_absolute_error: 6.5108\n",
            "Epoch 28/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5351 - mean_absolute_error: 2.0956\n",
            "Epoch 28: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 1.5351 - mean_absolute_error: 2.0956 - val_loss: 4.8122 - val_mean_absolute_error: 5.3632\n",
            "Epoch 29/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5700 - mean_absolute_error: 2.1355\n",
            "Epoch 29: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.5700 - mean_absolute_error: 2.1355 - val_loss: 5.5712 - val_mean_absolute_error: 6.1209\n",
            "Epoch 30/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5026 - mean_absolute_error: 2.0616\n",
            "Epoch 30: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 10s 211us/sample - loss: 1.5054 - mean_absolute_error: 2.0647 - val_loss: 5.9821 - val_mean_absolute_error: 6.5284\n",
            "Epoch 31/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.5821 - mean_absolute_error: 2.1449\n",
            "Epoch 31: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 239us/sample - loss: 1.5821 - mean_absolute_error: 2.1449 - val_loss: 5.8697 - val_mean_absolute_error: 6.4181\n",
            "Epoch 32/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4167 - mean_absolute_error: 1.9718\n",
            "Epoch 32: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 1.4167 - mean_absolute_error: 1.9718 - val_loss: 5.7338 - val_mean_absolute_error: 6.2916\n",
            "Epoch 33/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.5080 - mean_absolute_error: 2.0675\n",
            "Epoch 33: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 1.5057 - mean_absolute_error: 2.0650 - val_loss: 5.4567 - val_mean_absolute_error: 6.0026\n",
            "Epoch 34/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4467 - mean_absolute_error: 2.0040\n",
            "Epoch 34: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.4463 - mean_absolute_error: 2.0035 - val_loss: 5.1295 - val_mean_absolute_error: 5.6743\n",
            "Epoch 35/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.4026 - mean_absolute_error: 1.9551\n",
            "Epoch 35: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 227us/sample - loss: 1.4016 - mean_absolute_error: 1.9539 - val_loss: 5.4832 - val_mean_absolute_error: 6.0412\n",
            "Epoch 36/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.5281 - mean_absolute_error: 2.0932\n",
            "Epoch 36: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 10s 207us/sample - loss: 1.5279 - mean_absolute_error: 2.0930 - val_loss: 5.4285 - val_mean_absolute_error: 5.9713\n",
            "Epoch 37/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3900 - mean_absolute_error: 1.9449\n",
            "Epoch 37: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 235us/sample - loss: 1.3900 - mean_absolute_error: 1.9449 - val_loss: 5.7316 - val_mean_absolute_error: 6.2764\n",
            "Epoch 38/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3490 - mean_absolute_error: 1.9015\n",
            "Epoch 38: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 1.3490 - mean_absolute_error: 1.9015 - val_loss: 5.9496 - val_mean_absolute_error: 6.4947\n",
            "Epoch 39/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4139 - mean_absolute_error: 1.9723\n",
            "Epoch 39: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 1.4139 - mean_absolute_error: 1.9723 - val_loss: 5.5367 - val_mean_absolute_error: 6.0987\n",
            "Epoch 40/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.4097 - mean_absolute_error: 1.9658\n",
            "Epoch 40: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 243us/sample - loss: 1.4097 - mean_absolute_error: 1.9658 - val_loss: 5.3052 - val_mean_absolute_error: 5.8492\n",
            "Epoch 41/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3891 - mean_absolute_error: 1.9452\n",
            "Epoch 41: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 232us/sample - loss: 1.3905 - mean_absolute_error: 1.9467 - val_loss: 6.1077 - val_mean_absolute_error: 6.6609\n",
            "Epoch 42/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3714 - mean_absolute_error: 1.9283\n",
            "Epoch 42: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 226us/sample - loss: 1.3742 - mean_absolute_error: 1.9313 - val_loss: 5.4898 - val_mean_absolute_error: 6.0362\n",
            "Epoch 43/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3058 - mean_absolute_error: 1.8578\n",
            "Epoch 43: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 12s 253us/sample - loss: 1.3067 - mean_absolute_error: 1.8587 - val_loss: 5.5526 - val_mean_absolute_error: 6.1004\n",
            "Epoch 44/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3624 - mean_absolute_error: 1.9186\n",
            "Epoch 44: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 242us/sample - loss: 1.3624 - mean_absolute_error: 1.9186 - val_loss: 5.6859 - val_mean_absolute_error: 6.2305\n",
            "Epoch 45/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3607 - mean_absolute_error: 1.9157\n",
            "Epoch 45: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 240us/sample - loss: 1.3632 - mean_absolute_error: 1.9184 - val_loss: 5.4954 - val_mean_absolute_error: 6.0370\n",
            "Epoch 46/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3371 - mean_absolute_error: 1.8877\n",
            "Epoch 46: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.3361 - mean_absolute_error: 1.8865 - val_loss: 5.2770 - val_mean_absolute_error: 5.8217\n",
            "Epoch 47/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3094 - mean_absolute_error: 1.8598\n",
            "Epoch 47: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 10s 220us/sample - loss: 1.3094 - mean_absolute_error: 1.8598 - val_loss: 5.4286 - val_mean_absolute_error: 5.9743\n",
            "Epoch 48/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3225 - mean_absolute_error: 1.8756\n",
            "Epoch 48: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 10s 213us/sample - loss: 1.3228 - mean_absolute_error: 1.8760 - val_loss: 5.6823 - val_mean_absolute_error: 6.2329\n",
            "Epoch 49/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2532 - mean_absolute_error: 1.7994\n",
            "Epoch 49: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.2532 - mean_absolute_error: 1.7994 - val_loss: 5.8740 - val_mean_absolute_error: 6.4276\n",
            "Epoch 50/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.2641 - mean_absolute_error: 1.8138\n",
            "Epoch 50: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.2702 - mean_absolute_error: 1.8201 - val_loss: 5.9789 - val_mean_absolute_error: 6.5267\n",
            "Epoch 51/500\n",
            "46848/47059 [============================>.] - ETA: 0s - loss: 1.3610 - mean_absolute_error: 1.9165\n",
            "Epoch 51: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 231us/sample - loss: 1.3595 - mean_absolute_error: 1.9149 - val_loss: 5.9680 - val_mean_absolute_error: 6.5191\n",
            "Epoch 52/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.3646 - mean_absolute_error: 1.9214\n",
            "Epoch 52: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 238us/sample - loss: 1.3646 - mean_absolute_error: 1.9214 - val_loss: 6.1105 - val_mean_absolute_error: 6.6549\n",
            "Epoch 53/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3352 - mean_absolute_error: 1.8863\n",
            "Epoch 53: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 10s 203us/sample - loss: 1.3345 - mean_absolute_error: 1.8856 - val_loss: 5.9393 - val_mean_absolute_error: 6.5036\n",
            "Epoch 54/500\n",
            "47059/47059 [==============================] - ETA: 0s - loss: 1.2154 - mean_absolute_error: 1.7629\n",
            "Epoch 54: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 230us/sample - loss: 1.2154 - mean_absolute_error: 1.7629 - val_loss: 5.4417 - val_mean_absolute_error: 5.9846\n",
            "Epoch 55/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.2367 - mean_absolute_error: 1.7818\n",
            "Epoch 55: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 236us/sample - loss: 1.2375 - mean_absolute_error: 1.7827 - val_loss: 5.2551 - val_mean_absolute_error: 5.8038\n",
            "Epoch 56/500\n",
            "46976/47059 [============================>.] - ETA: 0s - loss: 1.3233 - mean_absolute_error: 1.8755\n",
            "Epoch 56: val_mean_absolute_error did not improve from 5.15005\n",
            "47059/47059 [==============================] - 11s 232us/sample - loss: 1.3229 - mean_absolute_error: 1.8751 - val_loss: 5.1248 - val_mean_absolute_error: 5.6639\n",
            "Epoch 56: early stopping\n",
            "2023-05-26 11:37:27.899898: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_23/BiasAdd' id:17104 op device:{requested: '', assigned: ''} def:{{{node dense_23/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_23/MatMul, dense_23/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673}\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_44 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_88 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_88 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_45 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_89 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_89 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_12 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_24 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_90 (Activation)  (None, 1, 128, 63)        0         \n",
            "                                                                 \n",
            " batch_normalization_90 (Bat  (None, 1, 128, 63)       252       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_91 (Activation)  (None, 1, 128, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_91 (Bat  (None, 1, 128, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_48 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_92 (Activation)  (None, 1, 128, 62)        0         \n",
            "                                                                 \n",
            " batch_normalization_92 (Bat  (None, 1, 128, 62)       248       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_13 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_25 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_93 (Activation)  (None, 1, 32, 120)        0         \n",
            "                                                                 \n",
            " batch_normalization_93 (Bat  (None, 1, 32, 120)       480       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_94 (Activation)  (None, 1, 32, 59)         0         \n",
            "                                                                 \n",
            " batch_normalization_94 (Bat  (None, 1, 32, 59)        236       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_51 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_95 (Activation)  (None, 1, 32, 27)         0         \n",
            "                                                                 \n",
            " batch_normalization_95 (Bat  (None, 1, 32, 27)        108       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " zero_padding2d_14 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_52 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_26 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_96 (Activation)  (None, 1, 4, 28)          0         \n",
            "                                                                 \n",
            " batch_normalization_96 (Bat  (None, 1, 4, 28)         112       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 112)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_97 (Activation)  (None, 38)                0         \n",
            "                                                                 \n",
            " batch_normalization_97 (Bat  (None, 38)               152       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_98 (Activation)  (None, 51)                0         \n",
            "                                                                 \n",
            " batch_normalization_98 (Bat  (None, 51)               204       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "(48506, 4, 256)\n",
            "(11619, 4, 256)\n",
            "(4572, 4, 256)\n",
            "Train on 48506 samples, validate on 11619 samples\n",
            "2023-05-26 11:37:33.823018: W tensorflow/c/c_api.cc:300] Operation '{name:'batch_normalization_88/moving_mean/Assign' id:18502 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_88/moving_mean/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_88/moving_mean, batch_normalization_88/moving_mean/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 79.1145 - mean_absolute_error: 79.8077/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n",
            "2023-05-26 11:37:47.666231: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_6/mul' id:19415 op device:{requested: '', assigned: ''} def:{{{node loss_6/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul/x, loss_6/dense_26_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 81.67945, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 24s 503us/sample - loss: 79.1145 - mean_absolute_error: 79.8077 - val_loss: 80.9863 - val_mean_absolute_error: 81.6795\n",
            "Epoch 2/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 40.9191 - mean_absolute_error: 41.6095\n",
            "Epoch 2: val_mean_absolute_error improved from 81.67945 to 22.89752, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 40.7711 - mean_absolute_error: 41.4615 - val_loss: 22.2205 - val_mean_absolute_error: 22.8975\n",
            "Epoch 3/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 5.2756 - mean_absolute_error: 5.8867\n",
            "Epoch 3: val_mean_absolute_error improved from 22.89752 to 5.30062, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 12s 239us/sample - loss: 5.2678 - mean_absolute_error: 5.8787 - val_loss: 4.7019 - val_mean_absolute_error: 5.3006\n",
            "Epoch 4/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 3.9039 - mean_absolute_error: 4.4948\n",
            "Epoch 4: val_mean_absolute_error improved from 5.30062 to 4.94817, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 11s 237us/sample - loss: 3.9039 - mean_absolute_error: 4.4948 - val_loss: 4.3608 - val_mean_absolute_error: 4.9482\n",
            "Epoch 5/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 3.5060 - mean_absolute_error: 4.0896\n",
            "Epoch 5: val_mean_absolute_error did not improve from 4.94817\n",
            "48506/48506 [==============================] - 11s 231us/sample - loss: 3.5085 - mean_absolute_error: 4.0921 - val_loss: 6.2712 - val_mean_absolute_error: 6.8827\n",
            "Epoch 6/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 3.2179 - mean_absolute_error: 3.7983\n",
            "Epoch 6: val_mean_absolute_error improved from 4.94817 to 4.90858, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 12s 245us/sample - loss: 3.2179 - mean_absolute_error: 3.7983 - val_loss: 4.3359 - val_mean_absolute_error: 4.9086\n",
            "Epoch 7/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 3.0425 - mean_absolute_error: 3.6224\n",
            "Epoch 7: val_mean_absolute_error improved from 4.90858 to 4.28393, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 11s 231us/sample - loss: 3.0425 - mean_absolute_error: 3.6224 - val_loss: 3.7094 - val_mean_absolute_error: 4.2839\n",
            "Epoch 8/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.8040 - mean_absolute_error: 3.3816\n",
            "Epoch 8: val_mean_absolute_error did not improve from 4.28393\n",
            "48506/48506 [==============================] - 11s 219us/sample - loss: 2.8040 - mean_absolute_error: 3.3816 - val_loss: 4.9900 - val_mean_absolute_error: 5.5827\n",
            "Epoch 9/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.6829 - mean_absolute_error: 3.2600\n",
            "Epoch 9: val_mean_absolute_error did not improve from 4.28393\n",
            "48506/48506 [==============================] - 12s 244us/sample - loss: 2.6829 - mean_absolute_error: 3.2600 - val_loss: 3.8081 - val_mean_absolute_error: 4.3700\n",
            "Epoch 10/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.4791 - mean_absolute_error: 3.0513\n",
            "Epoch 10: val_mean_absolute_error did not improve from 4.28393\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 2.4778 - mean_absolute_error: 3.0499 - val_loss: 4.5023 - val_mean_absolute_error: 5.0738\n",
            "Epoch 11/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.3925 - mean_absolute_error: 2.9653\n",
            "Epoch 11: val_mean_absolute_error did not improve from 4.28393\n",
            "48506/48506 [==============================] - 13s 263us/sample - loss: 2.3925 - mean_absolute_error: 2.9653 - val_loss: 4.0388 - val_mean_absolute_error: 4.5992\n",
            "Epoch 12/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.1708 - mean_absolute_error: 2.7389\n",
            "Epoch 12: val_mean_absolute_error did not improve from 4.28393\n",
            "48506/48506 [==============================] - 13s 259us/sample - loss: 2.1740 - mean_absolute_error: 2.7423 - val_loss: 4.0405 - val_mean_absolute_error: 4.6148\n",
            "Epoch 13/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.2063 - mean_absolute_error: 2.7807\n",
            "Epoch 13: val_mean_absolute_error improved from 4.28393 to 4.18086, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 13s 277us/sample - loss: 2.2062 - mean_absolute_error: 2.7805 - val_loss: 3.6178 - val_mean_absolute_error: 4.1809\n",
            "Epoch 14/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.0046 - mean_absolute_error: 2.5690\n",
            "Epoch 14: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 247us/sample - loss: 2.0046 - mean_absolute_error: 2.5690 - val_loss: 3.6437 - val_mean_absolute_error: 4.2154\n",
            "Epoch 15/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.0281 - mean_absolute_error: 2.5997\n",
            "Epoch 15: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 248us/sample - loss: 2.0277 - mean_absolute_error: 2.5993 - val_loss: 4.6675 - val_mean_absolute_error: 5.2463\n",
            "Epoch 16/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.9708 - mean_absolute_error: 2.5368\n",
            "Epoch 16: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.9708 - mean_absolute_error: 2.5368 - val_loss: 4.3583 - val_mean_absolute_error: 4.9203\n",
            "Epoch 17/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.8508 - mean_absolute_error: 2.4156\n",
            "Epoch 17: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 257us/sample - loss: 1.8508 - mean_absolute_error: 2.4156 - val_loss: 4.0473 - val_mean_absolute_error: 4.6092\n",
            "Epoch 18/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.8947 - mean_absolute_error: 2.4632\n",
            "Epoch 18: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 248us/sample - loss: 1.8938 - mean_absolute_error: 2.4623 - val_loss: 4.1716 - val_mean_absolute_error: 4.7361\n",
            "Epoch 19/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.8476 - mean_absolute_error: 2.4153\n",
            "Epoch 19: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 11s 233us/sample - loss: 1.8476 - mean_absolute_error: 2.4153 - val_loss: 3.6935 - val_mean_absolute_error: 4.2535\n",
            "Epoch 20/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.7712 - mean_absolute_error: 2.3360\n",
            "Epoch 20: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.7712 - mean_absolute_error: 2.3360 - val_loss: 4.2928 - val_mean_absolute_error: 4.8708\n",
            "Epoch 21/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.7502 - mean_absolute_error: 2.3166\n",
            "Epoch 21: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 251us/sample - loss: 1.7502 - mean_absolute_error: 2.3166 - val_loss: 4.6980 - val_mean_absolute_error: 5.2738\n",
            "Epoch 22/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.7142 - mean_absolute_error: 2.2792\n",
            "Epoch 22: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.7134 - mean_absolute_error: 2.2782 - val_loss: 3.9101 - val_mean_absolute_error: 4.4843\n",
            "Epoch 23/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6438 - mean_absolute_error: 2.2066\n",
            "Epoch 23: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.6438 - mean_absolute_error: 2.2066 - val_loss: 4.1817 - val_mean_absolute_error: 4.7501\n",
            "Epoch 24/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6777 - mean_absolute_error: 2.2428\n",
            "Epoch 24: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 1.6791 - mean_absolute_error: 2.2443 - val_loss: 3.7952 - val_mean_absolute_error: 4.3497\n",
            "Epoch 25/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.6329 - mean_absolute_error: 2.1938\n",
            "Epoch 25: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.6329 - mean_absolute_error: 2.1938 - val_loss: 4.2806 - val_mean_absolute_error: 4.8415\n",
            "Epoch 26/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.6411 - mean_absolute_error: 2.2034\n",
            "Epoch 26: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.6411 - mean_absolute_error: 2.2034 - val_loss: 4.3309 - val_mean_absolute_error: 4.8864\n",
            "Epoch 27/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5168 - mean_absolute_error: 2.0731\n",
            "Epoch 27: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.5168 - mean_absolute_error: 2.0731 - val_loss: 3.9358 - val_mean_absolute_error: 4.5008\n",
            "Epoch 28/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5762 - mean_absolute_error: 2.1351\n",
            "Epoch 28: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.5762 - mean_absolute_error: 2.1351 - val_loss: 3.9322 - val_mean_absolute_error: 4.4905\n",
            "Epoch 29/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5736 - mean_absolute_error: 2.1360\n",
            "Epoch 29: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 10s 216us/sample - loss: 1.5736 - mean_absolute_error: 2.1360 - val_loss: 3.9550 - val_mean_absolute_error: 4.5073\n",
            "Epoch 30/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6187 - mean_absolute_error: 2.1825\n",
            "Epoch 30: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 1.6201 - mean_absolute_error: 2.1840 - val_loss: 4.0338 - val_mean_absolute_error: 4.6048\n",
            "Epoch 31/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.6275 - mean_absolute_error: 2.1941\n",
            "Epoch 31: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 244us/sample - loss: 1.6238 - mean_absolute_error: 2.1900 - val_loss: 3.9051 - val_mean_absolute_error: 4.4525\n",
            "Epoch 32/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.6149 - mean_absolute_error: 2.1811\n",
            "Epoch 32: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.6149 - mean_absolute_error: 2.1811 - val_loss: 3.9280 - val_mean_absolute_error: 4.4909\n",
            "Epoch 33/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6235 - mean_absolute_error: 2.1918\n",
            "Epoch 33: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 1.6216 - mean_absolute_error: 2.1897 - val_loss: 3.8934 - val_mean_absolute_error: 4.4498\n",
            "Epoch 34/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5239 - mean_absolute_error: 2.0850\n",
            "Epoch 34: val_mean_absolute_error did not improve from 4.18086\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 1.5239 - mean_absolute_error: 2.0850 - val_loss: 3.8937 - val_mean_absolute_error: 4.4504\n",
            "Epoch 35/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5093 - mean_absolute_error: 2.0723\n",
            "Epoch 35: val_mean_absolute_error improved from 4.18086 to 4.18025, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 12s 255us/sample - loss: 1.5093 - mean_absolute_error: 2.0723 - val_loss: 3.6234 - val_mean_absolute_error: 4.1803\n",
            "Epoch 36/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3916 - mean_absolute_error: 1.9448\n",
            "Epoch 36: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 12s 246us/sample - loss: 1.3916 - mean_absolute_error: 1.9448 - val_loss: 4.4691 - val_mean_absolute_error: 5.0232\n",
            "Epoch 37/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5210 - mean_absolute_error: 2.0823\n",
            "Epoch 37: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 11s 220us/sample - loss: 1.5210 - mean_absolute_error: 2.0823 - val_loss: 4.2189 - val_mean_absolute_error: 4.7847\n",
            "Epoch 38/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4884 - mean_absolute_error: 2.0478\n",
            "Epoch 38: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 11s 233us/sample - loss: 1.4927 - mean_absolute_error: 2.0524 - val_loss: 3.9495 - val_mean_absolute_error: 4.5181\n",
            "Epoch 39/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4538 - mean_absolute_error: 2.0119\n",
            "Epoch 39: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.4558 - mean_absolute_error: 2.0140 - val_loss: 3.8544 - val_mean_absolute_error: 4.4053\n",
            "Epoch 40/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4274 - mean_absolute_error: 1.9863\n",
            "Epoch 40: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.4274 - mean_absolute_error: 1.9863 - val_loss: 4.0373 - val_mean_absolute_error: 4.5907\n",
            "Epoch 41/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5290 - mean_absolute_error: 2.0950\n",
            "Epoch 41: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 12s 244us/sample - loss: 1.5264 - mean_absolute_error: 2.0920 - val_loss: 4.0948 - val_mean_absolute_error: 4.6466\n",
            "Epoch 42/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3544 - mean_absolute_error: 1.9075\n",
            "Epoch 42: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.3549 - mean_absolute_error: 1.9081 - val_loss: 4.1199 - val_mean_absolute_error: 4.6842\n",
            "Epoch 43/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4426 - mean_absolute_error: 2.0028\n",
            "Epoch 43: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 12s 245us/sample - loss: 1.4426 - mean_absolute_error: 2.0028 - val_loss: 3.9374 - val_mean_absolute_error: 4.4896\n",
            "Epoch 44/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4742 - mean_absolute_error: 2.0322\n",
            "Epoch 44: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.4742 - mean_absolute_error: 2.0322 - val_loss: 4.2279 - val_mean_absolute_error: 4.8023\n",
            "Epoch 45/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3624 - mean_absolute_error: 1.9126\n",
            "Epoch 45: val_mean_absolute_error did not improve from 4.18025\n",
            "48506/48506 [==============================] - 11s 221us/sample - loss: 1.3624 - mean_absolute_error: 1.9126 - val_loss: 3.7187 - val_mean_absolute_error: 4.2831\n",
            "Epoch 46/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4066 - mean_absolute_error: 1.9670\n",
            "Epoch 46: val_mean_absolute_error improved from 4.18025 to 4.09112, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 12s 249us/sample - loss: 1.4066 - mean_absolute_error: 1.9670 - val_loss: 3.5312 - val_mean_absolute_error: 4.0911\n",
            "Epoch 47/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3915 - mean_absolute_error: 1.9485\n",
            "Epoch 47: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.3886 - mean_absolute_error: 1.9452 - val_loss: 3.7009 - val_mean_absolute_error: 4.2453\n",
            "Epoch 48/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3770 - mean_absolute_error: 1.9342\n",
            "Epoch 48: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 247us/sample - loss: 1.3749 - mean_absolute_error: 1.9318 - val_loss: 4.4966 - val_mean_absolute_error: 5.0629\n",
            "Epoch 49/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3919 - mean_absolute_error: 1.9483\n",
            "Epoch 49: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 248us/sample - loss: 1.3914 - mean_absolute_error: 1.9479 - val_loss: 3.8102 - val_mean_absolute_error: 4.3734\n",
            "Epoch 50/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3469 - mean_absolute_error: 1.9020\n",
            "Epoch 50: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.3478 - mean_absolute_error: 1.9031 - val_loss: 3.7568 - val_mean_absolute_error: 4.3223\n",
            "Epoch 51/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3640 - mean_absolute_error: 1.9181\n",
            "Epoch 51: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 246us/sample - loss: 1.3640 - mean_absolute_error: 1.9181 - val_loss: 4.4228 - val_mean_absolute_error: 4.9887\n",
            "Epoch 52/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4091 - mean_absolute_error: 1.9701\n",
            "Epoch 52: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 245us/sample - loss: 1.4069 - mean_absolute_error: 1.9675 - val_loss: 4.5654 - val_mean_absolute_error: 5.1337\n",
            "Epoch 53/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4425 - mean_absolute_error: 2.0034\n",
            "Epoch 53: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.4432 - mean_absolute_error: 2.0042 - val_loss: 4.2687 - val_mean_absolute_error: 4.8315\n",
            "Epoch 54/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4228 - mean_absolute_error: 1.9843\n",
            "Epoch 54: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 11s 223us/sample - loss: 1.4228 - mean_absolute_error: 1.9843 - val_loss: 3.9057 - val_mean_absolute_error: 4.4653\n",
            "Epoch 55/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3035 - mean_absolute_error: 1.8546\n",
            "Epoch 55: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 1.3061 - mean_absolute_error: 1.8574 - val_loss: 3.8771 - val_mean_absolute_error: 4.4396\n",
            "Epoch 56/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3618 - mean_absolute_error: 1.9185\n",
            "Epoch 56: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 246us/sample - loss: 1.3628 - mean_absolute_error: 1.9199 - val_loss: 4.0291 - val_mean_absolute_error: 4.5906\n",
            "Epoch 57/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3239 - mean_absolute_error: 1.8727\n",
            "Epoch 57: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 246us/sample - loss: 1.3239 - mean_absolute_error: 1.8727 - val_loss: 3.9875 - val_mean_absolute_error: 4.5467\n",
            "Epoch 58/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3234 - mean_absolute_error: 1.8754\n",
            "Epoch 58: val_mean_absolute_error did not improve from 4.09112\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.3222 - mean_absolute_error: 1.8742 - val_loss: 3.8846 - val_mean_absolute_error: 4.4378\n",
            "Epoch 59/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4045 - mean_absolute_error: 1.9658\n",
            "Epoch 59: val_mean_absolute_error improved from 4.09112 to 3.93100, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg4.h5\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.4083 - mean_absolute_error: 1.9700 - val_loss: 3.3808 - val_mean_absolute_error: 3.9310\n",
            "Epoch 60/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4200 - mean_absolute_error: 1.9803\n",
            "Epoch 60: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 244us/sample - loss: 1.4183 - mean_absolute_error: 1.9784 - val_loss: 3.7456 - val_mean_absolute_error: 4.2974\n",
            "Epoch 61/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3447 - mean_absolute_error: 1.8979\n",
            "Epoch 61: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 248us/sample - loss: 1.3451 - mean_absolute_error: 1.8985 - val_loss: 4.2837 - val_mean_absolute_error: 4.8488\n",
            "Epoch 62/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3013 - mean_absolute_error: 1.8538\n",
            "Epoch 62: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.3013 - mean_absolute_error: 1.8538 - val_loss: 4.4082 - val_mean_absolute_error: 4.9747\n",
            "Epoch 63/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.2969 - mean_absolute_error: 1.8479\n",
            "Epoch 63: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 226us/sample - loss: 1.2959 - mean_absolute_error: 1.8469 - val_loss: 3.9222 - val_mean_absolute_error: 4.4829\n",
            "Epoch 64/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3573 - mean_absolute_error: 1.9155\n",
            "Epoch 64: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 247us/sample - loss: 1.3561 - mean_absolute_error: 1.9142 - val_loss: 3.9177 - val_mean_absolute_error: 4.4672\n",
            "Epoch 65/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3565 - mean_absolute_error: 1.9144\n",
            "Epoch 65: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.3565 - mean_absolute_error: 1.9144 - val_loss: 4.0083 - val_mean_absolute_error: 4.5758\n",
            "Epoch 66/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3885 - mean_absolute_error: 1.9523\n",
            "Epoch 66: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 244us/sample - loss: 1.3848 - mean_absolute_error: 1.9481 - val_loss: 3.8430 - val_mean_absolute_error: 4.3947\n",
            "Epoch 67/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3175 - mean_absolute_error: 1.8742\n",
            "Epoch 67: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 1.3175 - mean_absolute_error: 1.8742 - val_loss: 4.1950 - val_mean_absolute_error: 4.7549\n",
            "Epoch 68/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3080 - mean_absolute_error: 1.8615\n",
            "Epoch 68: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.3061 - mean_absolute_error: 1.8594 - val_loss: 4.1171 - val_mean_absolute_error: 4.6787\n",
            "Epoch 69/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3632 - mean_absolute_error: 1.9196\n",
            "Epoch 69: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.3632 - mean_absolute_error: 1.9196 - val_loss: 4.2268 - val_mean_absolute_error: 4.7789\n",
            "Epoch 70/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3564 - mean_absolute_error: 1.9112\n",
            "Epoch 70: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 1.3549 - mean_absolute_error: 1.9098 - val_loss: 3.8168 - val_mean_absolute_error: 4.3716\n",
            "Epoch 71/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3328 - mean_absolute_error: 1.8876\n",
            "Epoch 71: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 227us/sample - loss: 1.3375 - mean_absolute_error: 1.8926 - val_loss: 3.4193 - val_mean_absolute_error: 3.9622\n",
            "Epoch 72/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3244 - mean_absolute_error: 1.8750\n",
            "Epoch 72: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 222us/sample - loss: 1.3240 - mean_absolute_error: 1.8746 - val_loss: 3.9316 - val_mean_absolute_error: 4.4844\n",
            "Epoch 73/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.2881 - mean_absolute_error: 1.8416\n",
            "Epoch 73: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 1.2913 - mean_absolute_error: 1.8450 - val_loss: 4.1097 - val_mean_absolute_error: 4.6682\n",
            "Epoch 74/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.2975 - mean_absolute_error: 1.8502\n",
            "Epoch 74: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.2955 - mean_absolute_error: 1.8479 - val_loss: 3.8584 - val_mean_absolute_error: 4.4090\n",
            "Epoch 75/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.2886 - mean_absolute_error: 1.8390\n",
            "Epoch 75: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.2870 - mean_absolute_error: 1.8373 - val_loss: 4.2540 - val_mean_absolute_error: 4.8150\n",
            "Epoch 76/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3325 - mean_absolute_error: 1.8895\n",
            "Epoch 76: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.3322 - mean_absolute_error: 1.8892 - val_loss: 3.8513 - val_mean_absolute_error: 4.4076\n",
            "Epoch 77/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.2548 - mean_absolute_error: 1.8026\n",
            "Epoch 77: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.2528 - mean_absolute_error: 1.8003 - val_loss: 4.2246 - val_mean_absolute_error: 4.7948\n",
            "Epoch 78/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.2099 - mean_absolute_error: 1.7546\n",
            "Epoch 78: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 228us/sample - loss: 1.2122 - mean_absolute_error: 1.7571 - val_loss: 4.0184 - val_mean_absolute_error: 4.5798\n",
            "Epoch 79/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3010 - mean_absolute_error: 1.8567\n",
            "Epoch 79: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 226us/sample - loss: 1.3051 - mean_absolute_error: 1.8611 - val_loss: 4.2822 - val_mean_absolute_error: 4.8704\n",
            "Epoch 80/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.2354 - mean_absolute_error: 1.7803\n",
            "Epoch 80: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.2406 - mean_absolute_error: 1.7858 - val_loss: 3.9591 - val_mean_absolute_error: 4.5131\n",
            "Epoch 81/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.2918 - mean_absolute_error: 1.8448\n",
            "Epoch 81: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.2918 - mean_absolute_error: 1.8448 - val_loss: 3.9093 - val_mean_absolute_error: 4.4612\n",
            "Epoch 82/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.2755 - mean_absolute_error: 1.8251\n",
            "Epoch 82: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.2779 - mean_absolute_error: 1.8278 - val_loss: 3.7693 - val_mean_absolute_error: 4.3184\n",
            "Epoch 83/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.2629 - mean_absolute_error: 1.8170\n",
            "Epoch 83: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 1.2629 - mean_absolute_error: 1.8170 - val_loss: 4.1771 - val_mean_absolute_error: 4.7319\n",
            "Epoch 84/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3150 - mean_absolute_error: 1.8679\n",
            "Epoch 84: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.3142 - mean_absolute_error: 1.8673 - val_loss: 3.7158 - val_mean_absolute_error: 4.2815\n",
            "Epoch 85/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3034 - mean_absolute_error: 1.8553\n",
            "Epoch 85: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.3034 - mean_absolute_error: 1.8553 - val_loss: 4.1820 - val_mean_absolute_error: 4.7380\n",
            "Epoch 86/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3151 - mean_absolute_error: 1.8733\n",
            "Epoch 86: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 220us/sample - loss: 1.3151 - mean_absolute_error: 1.8733 - val_loss: 4.0247 - val_mean_absolute_error: 4.5806\n",
            "Epoch 87/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3416 - mean_absolute_error: 1.9002\n",
            "Epoch 87: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 226us/sample - loss: 1.3385 - mean_absolute_error: 1.8968 - val_loss: 3.3871 - val_mean_absolute_error: 3.9357\n",
            "Epoch 88/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.2315 - mean_absolute_error: 1.7749\n",
            "Epoch 88: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.2315 - mean_absolute_error: 1.7749 - val_loss: 3.7355 - val_mean_absolute_error: 4.2818\n",
            "Epoch 89/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.2556 - mean_absolute_error: 1.8060\n",
            "Epoch 89: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 235us/sample - loss: 1.2556 - mean_absolute_error: 1.8060 - val_loss: 4.1138 - val_mean_absolute_error: 4.6719\n",
            "Epoch 90/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3080 - mean_absolute_error: 1.8646\n",
            "Epoch 90: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.3091 - mean_absolute_error: 1.8660 - val_loss: 4.1200 - val_mean_absolute_error: 4.6782\n",
            "Epoch 91/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.2648 - mean_absolute_error: 1.8114\n",
            "Epoch 91: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 233us/sample - loss: 1.2648 - mean_absolute_error: 1.8114 - val_loss: 4.0811 - val_mean_absolute_error: 4.6331\n",
            "Epoch 92/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3075 - mean_absolute_error: 1.8627\n",
            "Epoch 92: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 226us/sample - loss: 1.3075 - mean_absolute_error: 1.8627 - val_loss: 3.8794 - val_mean_absolute_error: 4.4323\n",
            "Epoch 93/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.2855 - mean_absolute_error: 1.8403\n",
            "Epoch 93: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 10s 208us/sample - loss: 1.2892 - mean_absolute_error: 1.8445 - val_loss: 3.7220 - val_mean_absolute_error: 4.2744\n",
            "Epoch 94/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.2430 - mean_absolute_error: 1.7954\n",
            "Epoch 94: val_mean_absolute_error did not improve from 3.93100\n",
            "48506/48506 [==============================] - 11s 237us/sample - loss: 1.2430 - mean_absolute_error: 1.7954 - val_loss: 3.8697 - val_mean_absolute_error: 4.4309\n",
            "Epoch 94: early stopping\n",
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2023-05-26 11:56:09.343824: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_26/BiasAdd' id:19364 op device:{requested: '', assigned: ''} def:{{{node dense_26/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_26/MatMul, dense_26/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654}\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_53 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_99 (Activation)  (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_99 (Bat  (None, 1, 256, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_54 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_100 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_100 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_15 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_55 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_27 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_101 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_101 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_56 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_102 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_102 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_57 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_103 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_103 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_16 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_28 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_104 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_104 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_105 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_105 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_60 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_106 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_106 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_17 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_61 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_29 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_107 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_107 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 112)               0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_108 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_108 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_109 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_109 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(48506, 4, 256)\n",
            "(13569, 4, 256)\n",
            "(2622, 4, 256)\n",
            "Train on 48506 samples, validate on 13569 samples\n",
            "2023-05-26 11:56:16.333975: W tensorflow/c/c_api.cc:300] Operation '{name:'training_14/Adam/conv2d_53/kernel/v/Assign' id:22440 op device:{requested: '', assigned: ''} def:{{{node training_14/Adam/conv2d_53/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_14/Adam/conv2d_53/kernel/v, training_14/Adam/conv2d_53/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 79.0953 - mean_absolute_error: 79.78852023-05-26 11:56:31.253975: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_7/mul' id:21675 op device:{requested: '', assigned: ''} def:{{{node loss_7/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul/x, loss_7/dense_29_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 72.66481, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 28s 574us/sample - loss: 79.0652 - mean_absolute_error: 79.7583 - val_loss: 71.9717 - val_mean_absolute_error: 72.6648\n",
            "Epoch 2/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 40.6802 - mean_absolute_error: 41.3700\n",
            "Epoch 2: val_mean_absolute_error improved from 72.66481 to 18.08936, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 12s 256us/sample - loss: 40.6802 - mean_absolute_error: 41.3700 - val_loss: 17.4127 - val_mean_absolute_error: 18.0894\n",
            "Epoch 3/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 5.0756 - mean_absolute_error: 5.6841\n",
            "Epoch 3: val_mean_absolute_error improved from 18.08936 to 4.77298, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 12s 253us/sample - loss: 5.0683 - mean_absolute_error: 5.6767 - val_loss: 4.1762 - val_mean_absolute_error: 4.7730\n",
            "Epoch 4/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 3.7799 - mean_absolute_error: 4.3692\n",
            "Epoch 4: val_mean_absolute_error did not improve from 4.77298\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 3.7799 - mean_absolute_error: 4.3692 - val_loss: 5.3458 - val_mean_absolute_error: 5.9690\n",
            "Epoch 5/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 3.3900 - mean_absolute_error: 3.9749\n",
            "Epoch 5: val_mean_absolute_error did not improve from 4.77298\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 3.3926 - mean_absolute_error: 3.9774 - val_loss: 4.2667 - val_mean_absolute_error: 4.8629\n",
            "Epoch 6/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 3.1120 - mean_absolute_error: 3.6943\n",
            "Epoch 6: val_mean_absolute_error improved from 4.77298 to 4.67095, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 13s 258us/sample - loss: 3.1120 - mean_absolute_error: 3.6943 - val_loss: 4.0847 - val_mean_absolute_error: 4.6709\n",
            "Epoch 7/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.8385 - mean_absolute_error: 3.4151\n",
            "Epoch 7: val_mean_absolute_error did not improve from 4.67095\n",
            "48506/48506 [==============================] - 11s 222us/sample - loss: 2.8385 - mean_absolute_error: 3.4151 - val_loss: 4.1359 - val_mean_absolute_error: 4.7246\n",
            "Epoch 8/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 2.6196 - mean_absolute_error: 3.1954\n",
            "Epoch 8: val_mean_absolute_error improved from 4.67095 to 4.53888, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 11s 232us/sample - loss: 2.6219 - mean_absolute_error: 3.1978 - val_loss: 3.9511 - val_mean_absolute_error: 4.5389\n",
            "Epoch 9/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.4545 - mean_absolute_error: 3.0308\n",
            "Epoch 9: val_mean_absolute_error did not improve from 4.53888\n",
            "48506/48506 [==============================] - 12s 239us/sample - loss: 2.4567 - mean_absolute_error: 3.0331 - val_loss: 4.0519 - val_mean_absolute_error: 4.6418\n",
            "Epoch 10/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.3813 - mean_absolute_error: 2.9566\n",
            "Epoch 10: val_mean_absolute_error did not improve from 4.53888\n",
            "48506/48506 [==============================] - 11s 233us/sample - loss: 2.3813 - mean_absolute_error: 2.9566 - val_loss: 4.0086 - val_mean_absolute_error: 4.5804\n",
            "Epoch 11/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.2186 - mean_absolute_error: 2.7880\n",
            "Epoch 11: val_mean_absolute_error improved from 4.53888 to 4.50559, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 2.2186 - mean_absolute_error: 2.7881 - val_loss: 3.9380 - val_mean_absolute_error: 4.5056\n",
            "Epoch 12/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.2150 - mean_absolute_error: 2.7897\n",
            "Epoch 12: val_mean_absolute_error did not improve from 4.50559\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 2.2142 - mean_absolute_error: 2.7890 - val_loss: 4.1919 - val_mean_absolute_error: 4.7674\n",
            "Epoch 13/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.0416 - mean_absolute_error: 2.6110\n",
            "Epoch 13: val_mean_absolute_error did not improve from 4.50559\n",
            "48506/48506 [==============================] - 11s 235us/sample - loss: 2.0416 - mean_absolute_error: 2.6110 - val_loss: 4.3023 - val_mean_absolute_error: 4.8781\n",
            "Epoch 14/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.9987 - mean_absolute_error: 2.5692\n",
            "Epoch 14: val_mean_absolute_error improved from 4.50559 to 4.43994, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 11s 219us/sample - loss: 1.9987 - mean_absolute_error: 2.5692 - val_loss: 3.8683 - val_mean_absolute_error: 4.4399\n",
            "Epoch 15/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.9351 - mean_absolute_error: 2.5053\n",
            "Epoch 15: val_mean_absolute_error did not improve from 4.43994\n",
            "48506/48506 [==============================] - 11s 227us/sample - loss: 1.9351 - mean_absolute_error: 2.5053 - val_loss: 4.2785 - val_mean_absolute_error: 4.8575\n",
            "Epoch 16/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.8873 - mean_absolute_error: 2.4567\n",
            "Epoch 16: val_mean_absolute_error did not improve from 4.43994\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.8892 - mean_absolute_error: 2.4589 - val_loss: 4.3835 - val_mean_absolute_error: 4.9622\n",
            "Epoch 17/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.8522 - mean_absolute_error: 2.4212\n",
            "Epoch 17: val_mean_absolute_error did not improve from 4.43994\n",
            "48506/48506 [==============================] - 11s 235us/sample - loss: 1.8511 - mean_absolute_error: 2.4202 - val_loss: 4.2768 - val_mean_absolute_error: 4.8599\n",
            "Epoch 18/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.8242 - mean_absolute_error: 2.3925\n",
            "Epoch 18: val_mean_absolute_error did not improve from 4.43994\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.8251 - mean_absolute_error: 2.3936 - val_loss: 3.9850 - val_mean_absolute_error: 4.5574\n",
            "Epoch 19/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.7471 - mean_absolute_error: 2.3136\n",
            "Epoch 19: val_mean_absolute_error did not improve from 4.43994\n",
            "48506/48506 [==============================] - 11s 234us/sample - loss: 1.7477 - mean_absolute_error: 2.3143 - val_loss: 4.0179 - val_mean_absolute_error: 4.5955\n",
            "Epoch 20/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.7311 - mean_absolute_error: 2.2969\n",
            "Epoch 20: val_mean_absolute_error did not improve from 4.43994\n",
            "48506/48506 [==============================] - 11s 234us/sample - loss: 1.7305 - mean_absolute_error: 2.2962 - val_loss: 3.9469 - val_mean_absolute_error: 4.5138\n",
            "Epoch 21/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6810 - mean_absolute_error: 2.2454\n",
            "Epoch 21: val_mean_absolute_error improved from 4.43994 to 4.21804, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg6.h5\n",
            "48506/48506 [==============================] - 11s 221us/sample - loss: 1.6806 - mean_absolute_error: 2.2449 - val_loss: 3.6520 - val_mean_absolute_error: 4.2180\n",
            "Epoch 22/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6731 - mean_absolute_error: 2.2384\n",
            "Epoch 22: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 224us/sample - loss: 1.6732 - mean_absolute_error: 2.2385 - val_loss: 4.0676 - val_mean_absolute_error: 4.6327\n",
            "Epoch 23/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5573 - mean_absolute_error: 2.1158\n",
            "Epoch 23: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 235us/sample - loss: 1.5573 - mean_absolute_error: 2.1158 - val_loss: 4.0888 - val_mean_absolute_error: 4.6604\n",
            "Epoch 24/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5206 - mean_absolute_error: 2.0762\n",
            "Epoch 24: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 237us/sample - loss: 1.5212 - mean_absolute_error: 2.0768 - val_loss: 4.0268 - val_mean_absolute_error: 4.6045\n",
            "Epoch 25/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5621 - mean_absolute_error: 2.1243\n",
            "Epoch 25: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 234us/sample - loss: 1.5621 - mean_absolute_error: 2.1243 - val_loss: 3.9578 - val_mean_absolute_error: 4.5425\n",
            "Epoch 26/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5620 - mean_absolute_error: 2.1247\n",
            "Epoch 26: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 1.5620 - mean_absolute_error: 2.1247 - val_loss: 3.8910 - val_mean_absolute_error: 4.4646\n",
            "Epoch 27/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5232 - mean_absolute_error: 2.0826\n",
            "Epoch 27: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 222us/sample - loss: 1.5232 - mean_absolute_error: 2.0826 - val_loss: 4.2342 - val_mean_absolute_error: 4.8102\n",
            "Epoch 28/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.5062 - mean_absolute_error: 2.0656\n",
            "Epoch 28: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 219us/sample - loss: 1.5032 - mean_absolute_error: 2.0622 - val_loss: 4.0934 - val_mean_absolute_error: 4.6634\n",
            "Epoch 29/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.6192 - mean_absolute_error: 2.1842\n",
            "Epoch 29: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 232us/sample - loss: 1.6192 - mean_absolute_error: 2.1842 - val_loss: 3.8746 - val_mean_absolute_error: 4.4423\n",
            "Epoch 30/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4798 - mean_absolute_error: 2.0403\n",
            "Epoch 30: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 235us/sample - loss: 1.4912 - mean_absolute_error: 2.0521 - val_loss: 4.2000 - val_mean_absolute_error: 4.7841\n",
            "Epoch 31/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5201 - mean_absolute_error: 2.0845\n",
            "Epoch 31: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 1.5201 - mean_absolute_error: 2.0845 - val_loss: 3.8387 - val_mean_absolute_error: 4.4061\n",
            "Epoch 32/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4857 - mean_absolute_error: 2.0452\n",
            "Epoch 32: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 1.4857 - mean_absolute_error: 2.0452 - val_loss: 3.9617 - val_mean_absolute_error: 4.5289\n",
            "Epoch 33/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5557 - mean_absolute_error: 2.1200\n",
            "Epoch 33: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 224us/sample - loss: 1.5560 - mean_absolute_error: 2.1204 - val_loss: 4.3313 - val_mean_absolute_error: 4.9074\n",
            "Epoch 34/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5619 - mean_absolute_error: 2.1262\n",
            "Epoch 34: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 10s 212us/sample - loss: 1.5619 - mean_absolute_error: 2.1262 - val_loss: 3.8339 - val_mean_absolute_error: 4.4039\n",
            "Epoch 35/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4536 - mean_absolute_error: 2.0124\n",
            "Epoch 35: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 1.4540 - mean_absolute_error: 2.0129 - val_loss: 4.2621 - val_mean_absolute_error: 4.8393\n",
            "Epoch 36/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3766 - mean_absolute_error: 1.9313\n",
            "Epoch 36: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.3761 - mean_absolute_error: 1.9307 - val_loss: 3.9222 - val_mean_absolute_error: 4.4950\n",
            "Epoch 37/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4152 - mean_absolute_error: 1.9673\n",
            "Epoch 37: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 233us/sample - loss: 1.4146 - mean_absolute_error: 1.9668 - val_loss: 3.8308 - val_mean_absolute_error: 4.3989\n",
            "Epoch 38/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4124 - mean_absolute_error: 1.9707\n",
            "Epoch 38: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 233us/sample - loss: 1.4124 - mean_absolute_error: 1.9707 - val_loss: 4.2612 - val_mean_absolute_error: 4.8250\n",
            "Epoch 39/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4422 - mean_absolute_error: 1.9980\n",
            "Epoch 39: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 229us/sample - loss: 1.4448 - mean_absolute_error: 2.0008 - val_loss: 4.4300 - val_mean_absolute_error: 5.0071\n",
            "Epoch 40/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4120 - mean_absolute_error: 1.9727\n",
            "Epoch 40: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 10s 208us/sample - loss: 1.4112 - mean_absolute_error: 1.9719 - val_loss: 3.9270 - val_mean_absolute_error: 4.4983\n",
            "Epoch 41/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4841 - mean_absolute_error: 2.0458\n",
            "Epoch 41: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 234us/sample - loss: 1.4852 - mean_absolute_error: 2.0470 - val_loss: 4.0633 - val_mean_absolute_error: 4.6415\n",
            "Epoch 42/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4500 - mean_absolute_error: 2.0103\n",
            "Epoch 42: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 234us/sample - loss: 1.4500 - mean_absolute_error: 2.0103 - val_loss: 4.1199 - val_mean_absolute_error: 4.6955\n",
            "Epoch 43/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4494 - mean_absolute_error: 2.0129\n",
            "Epoch 43: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 235us/sample - loss: 1.4494 - mean_absolute_error: 2.0129 - val_loss: 3.7541 - val_mean_absolute_error: 4.3191\n",
            "Epoch 44/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4250 - mean_absolute_error: 1.9850\n",
            "Epoch 44: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.4341 - mean_absolute_error: 1.9941 - val_loss: 4.2612 - val_mean_absolute_error: 4.8406\n",
            "Epoch 45/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4173 - mean_absolute_error: 1.9756\n",
            "Epoch 45: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 232us/sample - loss: 1.4173 - mean_absolute_error: 1.9756 - val_loss: 3.9830 - val_mean_absolute_error: 4.5530\n",
            "Epoch 46/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3026 - mean_absolute_error: 1.8536\n",
            "Epoch 46: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 10s 211us/sample - loss: 1.3046 - mean_absolute_error: 1.8558 - val_loss: 4.0395 - val_mean_absolute_error: 4.6073\n",
            "Epoch 47/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3920 - mean_absolute_error: 1.9511\n",
            "Epoch 47: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 229us/sample - loss: 1.3920 - mean_absolute_error: 1.9511 - val_loss: 4.1292 - val_mean_absolute_error: 4.7050\n",
            "Epoch 48/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4429 - mean_absolute_error: 1.9998\n",
            "Epoch 48: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 1.4406 - mean_absolute_error: 1.9974 - val_loss: 4.4372 - val_mean_absolute_error: 5.0118\n",
            "Epoch 49/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4119 - mean_absolute_error: 1.9713\n",
            "Epoch 49: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.4097 - mean_absolute_error: 1.9690 - val_loss: 4.3468 - val_mean_absolute_error: 4.9329\n",
            "Epoch 50/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3294 - mean_absolute_error: 1.8827\n",
            "Epoch 50: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.3294 - mean_absolute_error: 1.8827 - val_loss: 4.1392 - val_mean_absolute_error: 4.7076\n",
            "Epoch 51/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4037 - mean_absolute_error: 1.9662\n",
            "Epoch 51: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.4037 - mean_absolute_error: 1.9662 - val_loss: 4.1044 - val_mean_absolute_error: 4.6741\n",
            "Epoch 52/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3962 - mean_absolute_error: 1.9546\n",
            "Epoch 52: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 228us/sample - loss: 1.3962 - mean_absolute_error: 1.9546 - val_loss: 4.3258 - val_mean_absolute_error: 4.9094\n",
            "Epoch 53/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3430 - mean_absolute_error: 1.8976\n",
            "Epoch 53: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 10s 215us/sample - loss: 1.3446 - mean_absolute_error: 1.8994 - val_loss: 3.9074 - val_mean_absolute_error: 4.4758\n",
            "Epoch 54/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3558 - mean_absolute_error: 1.9105\n",
            "Epoch 54: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 234us/sample - loss: 1.3541 - mean_absolute_error: 1.9087 - val_loss: 4.0406 - val_mean_absolute_error: 4.6077\n",
            "Epoch 55/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3159 - mean_absolute_error: 1.8699\n",
            "Epoch 55: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 1.3146 - mean_absolute_error: 1.8684 - val_loss: 4.0529 - val_mean_absolute_error: 4.6256\n",
            "Epoch 56/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3084 - mean_absolute_error: 1.8641\n",
            "Epoch 56: val_mean_absolute_error did not improve from 4.21804\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 1.3084 - mean_absolute_error: 1.8641 - val_loss: 4.0381 - val_mean_absolute_error: 4.6156\n",
            "Epoch 56: early stopping\n",
            "2023-05-26 12:07:08.120950: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_29/BiasAdd' id:21624 op device:{requested: '', assigned: ''} def:{{{node dense_29/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_29/MatMul, dense_29/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585}\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_62 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_110 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_110 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_63 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_111 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_111 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_18 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_64 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_30 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_112 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_112 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_65 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_113 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_113 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_66 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_114 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_114 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_19 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_67 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_31 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_115 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_115 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_68 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_116 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_116 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_69 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_117 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_117 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_20 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_70 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_32 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_118 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_118 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_119 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_119 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_120 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_120 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(48506, 4, 256)\n",
            "(11670, 4, 256)\n",
            "(4521, 4, 256)\n",
            "Train on 48506 samples, validate on 11670 samples\n",
            "2023-05-26 12:07:14.932023: W tensorflow/c/c_api.cc:300] Operation '{name:'conv2d_68/kernel/Assign' id:23476 op device:{requested: '', assigned: ''} def:{{{node conv2d_68/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_68/kernel, conv2d_68/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 79.2112 - mean_absolute_error: 79.90432023-05-26 12:07:29.799865: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_8/mul' id:23935 op device:{requested: '', assigned: ''} def:{{{node loss_8/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul/x, loss_8/dense_32_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 76.31335, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg11.h5\n",
            "48506/48506 [==============================] - 28s 573us/sample - loss: 79.1750 - mean_absolute_error: 79.8681 - val_loss: 75.6202 - val_mean_absolute_error: 76.3134\n",
            "Epoch 2/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 41.0387 - mean_absolute_error: 41.7286\n",
            "Epoch 2: val_mean_absolute_error improved from 76.31335 to 24.87821, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg11.h5\n",
            "48506/48506 [==============================] - 12s 245us/sample - loss: 40.8981 - mean_absolute_error: 41.5879 - val_loss: 24.1887 - val_mean_absolute_error: 24.8782\n",
            "Epoch 3/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 5.3481 - mean_absolute_error: 5.9592\n",
            "Epoch 3: val_mean_absolute_error improved from 24.87821 to 6.22174, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg11.h5\n",
            "48506/48506 [==============================] - 11s 230us/sample - loss: 5.3481 - mean_absolute_error: 5.9592 - val_loss: 5.5947 - val_mean_absolute_error: 6.2217\n",
            "Epoch 4/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 3.7682 - mean_absolute_error: 4.3589\n",
            "Epoch 4: val_mean_absolute_error improved from 6.22174 to 5.19349, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg11.h5\n",
            "48506/48506 [==============================] - 11s 218us/sample - loss: 3.7682 - mean_absolute_error: 4.3589 - val_loss: 4.6078 - val_mean_absolute_error: 5.1935\n",
            "Epoch 5/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 3.3578 - mean_absolute_error: 3.9438\n",
            "Epoch 5: val_mean_absolute_error improved from 5.19349 to 4.47476, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg11.h5\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 3.3570 - mean_absolute_error: 3.9431 - val_loss: 3.8891 - val_mean_absolute_error: 4.4748\n",
            "Epoch 6/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 3.0349 - mean_absolute_error: 3.6152\n",
            "Epoch 6: val_mean_absolute_error did not improve from 4.47476\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 3.0354 - mean_absolute_error: 3.6158 - val_loss: 4.1421 - val_mean_absolute_error: 4.7215\n",
            "Epoch 7/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.8211 - mean_absolute_error: 3.3981\n",
            "Epoch 7: val_mean_absolute_error did not improve from 4.47476\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 2.8211 - mean_absolute_error: 3.3981 - val_loss: 4.5190 - val_mean_absolute_error: 5.1094\n",
            "Epoch 8/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 2.5706 - mean_absolute_error: 3.1429\n",
            "Epoch 8: val_mean_absolute_error did not improve from 4.47476\n",
            "48506/48506 [==============================] - 11s 236us/sample - loss: 2.5697 - mean_absolute_error: 3.1420 - val_loss: 4.0191 - val_mean_absolute_error: 4.5945\n",
            "Epoch 9/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 2.5099 - mean_absolute_error: 3.0856\n",
            "Epoch 9: val_mean_absolute_error improved from 4.47476 to 4.20592, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg11.h5\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 2.5107 - mean_absolute_error: 3.0863 - val_loss: 3.6295 - val_mean_absolute_error: 4.2059\n",
            "Epoch 10/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.3302 - mean_absolute_error: 2.9030\n",
            "Epoch 10: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 230us/sample - loss: 2.3302 - mean_absolute_error: 2.9030 - val_loss: 3.9318 - val_mean_absolute_error: 4.4980\n",
            "Epoch 11/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.2800 - mean_absolute_error: 2.8566\n",
            "Epoch 11: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 10s 207us/sample - loss: 2.2800 - mean_absolute_error: 2.8566 - val_loss: 3.8404 - val_mean_absolute_error: 4.4207\n",
            "Epoch 12/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.1182 - mean_absolute_error: 2.6888\n",
            "Epoch 12: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 2.1182 - mean_absolute_error: 2.6888 - val_loss: 3.6709 - val_mean_absolute_error: 4.2412\n",
            "Epoch 13/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.0850 - mean_absolute_error: 2.6559\n",
            "Epoch 13: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 243us/sample - loss: 2.0850 - mean_absolute_error: 2.6559 - val_loss: 3.7070 - val_mean_absolute_error: 4.2720\n",
            "Epoch 14/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.0437 - mean_absolute_error: 2.6158\n",
            "Epoch 14: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 2.0420 - mean_absolute_error: 2.6141 - val_loss: 3.8244 - val_mean_absolute_error: 4.3893\n",
            "Epoch 15/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.9349 - mean_absolute_error: 2.5043\n",
            "Epoch 15: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 247us/sample - loss: 1.9349 - mean_absolute_error: 2.5043 - val_loss: 4.0686 - val_mean_absolute_error: 4.6366\n",
            "Epoch 16/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.9723 - mean_absolute_error: 2.5476\n",
            "Epoch 16: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 246us/sample - loss: 1.9721 - mean_absolute_error: 2.5472 - val_loss: 3.8619 - val_mean_absolute_error: 4.4403\n",
            "Epoch 17/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.8651 - mean_absolute_error: 2.4330\n",
            "Epoch 17: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.8651 - mean_absolute_error: 2.4330 - val_loss: 3.6544 - val_mean_absolute_error: 4.2179\n",
            "Epoch 18/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.8449 - mean_absolute_error: 2.4104\n",
            "Epoch 18: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 225us/sample - loss: 1.8449 - mean_absolute_error: 2.4104 - val_loss: 3.7108 - val_mean_absolute_error: 4.2691\n",
            "Epoch 19/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.8170 - mean_absolute_error: 2.3838\n",
            "Epoch 19: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 227us/sample - loss: 1.8170 - mean_absolute_error: 2.3838 - val_loss: 3.7133 - val_mean_absolute_error: 4.2761\n",
            "Epoch 20/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.7517 - mean_absolute_error: 2.3168\n",
            "Epoch 20: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 1.7569 - mean_absolute_error: 2.3222 - val_loss: 4.4834 - val_mean_absolute_error: 5.0562\n",
            "Epoch 21/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.7340 - mean_absolute_error: 2.2998\n",
            "Epoch 21: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 235us/sample - loss: 1.7340 - mean_absolute_error: 2.2998 - val_loss: 4.2318 - val_mean_absolute_error: 4.7968\n",
            "Epoch 22/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.7461 - mean_absolute_error: 2.3140\n",
            "Epoch 22: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 237us/sample - loss: 1.7454 - mean_absolute_error: 2.3133 - val_loss: 3.9360 - val_mean_absolute_error: 4.5020\n",
            "Epoch 23/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.7224 - mean_absolute_error: 2.2897\n",
            "Epoch 23: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.7224 - mean_absolute_error: 2.2897 - val_loss: 4.1722 - val_mean_absolute_error: 4.7531\n",
            "Epoch 24/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6681 - mean_absolute_error: 2.2343\n",
            "Epoch 24: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 1.6672 - mean_absolute_error: 2.2334 - val_loss: 3.7081 - val_mean_absolute_error: 4.2731\n",
            "Epoch 25/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.6565 - mean_absolute_error: 2.2207\n",
            "Epoch 25: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 227us/sample - loss: 1.6565 - mean_absolute_error: 2.2207 - val_loss: 4.1011 - val_mean_absolute_error: 4.6720\n",
            "Epoch 26/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6566 - mean_absolute_error: 2.2238\n",
            "Epoch 26: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 227us/sample - loss: 1.6548 - mean_absolute_error: 2.2218 - val_loss: 4.1896 - val_mean_absolute_error: 4.7476\n",
            "Epoch 27/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6350 - mean_absolute_error: 2.2015\n",
            "Epoch 27: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 237us/sample - loss: 1.6360 - mean_absolute_error: 2.2027 - val_loss: 4.1927 - val_mean_absolute_error: 4.7518\n",
            "Epoch 28/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.6037 - mean_absolute_error: 2.1676\n",
            "Epoch 28: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 245us/sample - loss: 1.6037 - mean_absolute_error: 2.1676 - val_loss: 3.7963 - val_mean_absolute_error: 4.3570\n",
            "Epoch 29/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5874 - mean_absolute_error: 2.1517\n",
            "Epoch 29: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 244us/sample - loss: 1.5875 - mean_absolute_error: 2.1518 - val_loss: 3.6967 - val_mean_absolute_error: 4.2526\n",
            "Epoch 30/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5783 - mean_absolute_error: 2.1418\n",
            "Epoch 30: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 246us/sample - loss: 1.5769 - mean_absolute_error: 2.1403 - val_loss: 3.7623 - val_mean_absolute_error: 4.3211\n",
            "Epoch 31/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5752 - mean_absolute_error: 2.1378\n",
            "Epoch 31: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.5752 - mean_absolute_error: 2.1378 - val_loss: 3.8311 - val_mean_absolute_error: 4.3903\n",
            "Epoch 32/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5036 - mean_absolute_error: 2.0633\n",
            "Epoch 32: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.5036 - mean_absolute_error: 2.0633 - val_loss: 4.0176 - val_mean_absolute_error: 4.5873\n",
            "Epoch 33/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5559 - mean_absolute_error: 2.1214\n",
            "Epoch 33: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 231us/sample - loss: 1.5555 - mean_absolute_error: 2.1211 - val_loss: 4.1740 - val_mean_absolute_error: 4.7360\n",
            "Epoch 34/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5200 - mean_absolute_error: 2.0790\n",
            "Epoch 34: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 220us/sample - loss: 1.5200 - mean_absolute_error: 2.0790 - val_loss: 3.8944 - val_mean_absolute_error: 4.4532\n",
            "Epoch 35/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.5365 - mean_absolute_error: 2.1021\n",
            "Epoch 35: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 248us/sample - loss: 1.5327 - mean_absolute_error: 2.0978 - val_loss: 4.0485 - val_mean_absolute_error: 4.6285\n",
            "Epoch 36/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5255 - mean_absolute_error: 2.0875\n",
            "Epoch 36: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 247us/sample - loss: 1.5238 - mean_absolute_error: 2.0856 - val_loss: 4.1202 - val_mean_absolute_error: 4.6796\n",
            "Epoch 37/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.4459 - mean_absolute_error: 2.0036\n",
            "Epoch 37: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 1.4519 - mean_absolute_error: 2.0100 - val_loss: 4.2214 - val_mean_absolute_error: 4.7997\n",
            "Epoch 38/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4815 - mean_absolute_error: 2.0416\n",
            "Epoch 38: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.4795 - mean_absolute_error: 2.0394 - val_loss: 3.6861 - val_mean_absolute_error: 4.2432\n",
            "Epoch 39/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4943 - mean_absolute_error: 2.0563\n",
            "Epoch 39: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 251us/sample - loss: 1.4943 - mean_absolute_error: 2.0563 - val_loss: 3.9014 - val_mean_absolute_error: 4.4667\n",
            "Epoch 40/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4140 - mean_absolute_error: 1.9710\n",
            "Epoch 40: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 1.4122 - mean_absolute_error: 1.9690 - val_loss: 3.9822 - val_mean_absolute_error: 4.5456\n",
            "Epoch 41/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5115 - mean_absolute_error: 2.0747\n",
            "Epoch 41: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 254us/sample - loss: 1.5122 - mean_absolute_error: 2.0755 - val_loss: 3.8736 - val_mean_absolute_error: 4.4350\n",
            "Epoch 42/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4897 - mean_absolute_error: 2.0487\n",
            "Epoch 42: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 239us/sample - loss: 1.4897 - mean_absolute_error: 2.0487 - val_loss: 4.4093 - val_mean_absolute_error: 4.9680\n",
            "Epoch 43/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4186 - mean_absolute_error: 1.9732\n",
            "Epoch 43: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 11s 217us/sample - loss: 1.4186 - mean_absolute_error: 1.9732 - val_loss: 4.2040 - val_mean_absolute_error: 4.7650\n",
            "Epoch 44/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3488 - mean_absolute_error: 1.9039\n",
            "Epoch 44: val_mean_absolute_error did not improve from 4.20592\n",
            "48506/48506 [==============================] - 12s 251us/sample - loss: 1.3479 - mean_absolute_error: 1.9029 - val_loss: 3.9823 - val_mean_absolute_error: 4.5388\n",
            "Epoch 44: early stopping\n",
            "2023-05-26 12:16:03.204534: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_32/BiasAdd' id:23884 op device:{requested: '', assigned: ''} def:{{{node dense_32/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_32/MatMul, dense_32/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507}\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_71 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_121 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_121 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_72 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_122 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_122 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_21 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_73 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_33 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_123 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_123 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_74 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_124 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_124 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_75 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_125 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_125 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_22 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_76 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_34 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_126 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_126 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_77 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_127 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_127 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_78 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_128 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_128 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_23 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_79 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_35 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_129 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_129 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_130 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_130 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_131 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_131 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(48506, 4, 256)\n",
            "(11715, 4, 256)\n",
            "(4476, 4, 256)\n",
            "Train on 48506 samples, validate on 11715 samples\n",
            "2023-05-26 12:16:10.924347: W tensorflow/c/c_api.cc:300] Operation '{name:'training_18/Adam/batch_normalization_128/beta/v/Assign' id:27129 op device:{requested: '', assigned: ''} def:{{{node training_18/Adam/batch_normalization_128/beta/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_18/Adam/batch_normalization_128/beta/v, training_18/Adam/batch_normalization_128/beta/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 79.1826 - mean_absolute_error: 79.87572023-05-26 12:16:25.850995: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_9/mul' id:26195 op device:{requested: '', assigned: ''} def:{{{node loss_9/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_9/mul/x, loss_9/dense_35_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 81.77674, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 30s 613us/sample - loss: 79.1641 - mean_absolute_error: 79.8573 - val_loss: 81.0836 - val_mean_absolute_error: 81.7767\n",
            "Epoch 2/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 41.0100 - mean_absolute_error: 41.7001\n",
            "Epoch 2: val_mean_absolute_error improved from 81.77674 to 25.67785, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 257us/sample - loss: 40.9306 - mean_absolute_error: 41.6206 - val_loss: 24.9959 - val_mean_absolute_error: 25.6778\n",
            "Epoch 3/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 5.3560 - mean_absolute_error: 5.9701\n",
            "Epoch 3: val_mean_absolute_error improved from 25.67785 to 7.84909, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 248us/sample - loss: 5.3560 - mean_absolute_error: 5.9701 - val_loss: 7.2022 - val_mean_absolute_error: 7.8491\n",
            "Epoch 4/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 3.8070 - mean_absolute_error: 4.3969\n",
            "Epoch 4: val_mean_absolute_error improved from 7.84909 to 6.02916, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 251us/sample - loss: 3.8070 - mean_absolute_error: 4.3969 - val_loss: 5.4312 - val_mean_absolute_error: 6.0292\n",
            "Epoch 5/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 3.4347 - mean_absolute_error: 4.0179\n",
            "Epoch 5: val_mean_absolute_error did not improve from 6.02916\n",
            "48506/48506 [==============================] - 12s 241us/sample - loss: 3.4325 - mean_absolute_error: 4.0156 - val_loss: 6.8839 - val_mean_absolute_error: 7.4927\n",
            "Epoch 6/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 3.1903 - mean_absolute_error: 3.7689\n",
            "Epoch 6: val_mean_absolute_error improved from 6.02916 to 5.75749, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 249us/sample - loss: 3.1859 - mean_absolute_error: 3.7642 - val_loss: 5.1692 - val_mean_absolute_error: 5.7575\n",
            "Epoch 7/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.9025 - mean_absolute_error: 3.4761\n",
            "Epoch 7: val_mean_absolute_error improved from 5.75749 to 5.57022, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 249us/sample - loss: 2.9048 - mean_absolute_error: 3.4784 - val_loss: 4.9851 - val_mean_absolute_error: 5.5702\n",
            "Epoch 8/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.7302 - mean_absolute_error: 3.3040\n",
            "Epoch 8: val_mean_absolute_error improved from 5.57022 to 5.41926, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 247us/sample - loss: 2.7302 - mean_absolute_error: 3.3040 - val_loss: 4.8334 - val_mean_absolute_error: 5.4193\n",
            "Epoch 9/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.5598 - mean_absolute_error: 3.1301\n",
            "Epoch 9: val_mean_absolute_error improved from 5.41926 to 5.26871, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 246us/sample - loss: 2.5593 - mean_absolute_error: 3.1297 - val_loss: 4.6776 - val_mean_absolute_error: 5.2687\n",
            "Epoch 10/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.4155 - mean_absolute_error: 2.9865\n",
            "Epoch 10: val_mean_absolute_error did not improve from 5.26871\n",
            "48506/48506 [==============================] - 11s 226us/sample - loss: 2.4147 - mean_absolute_error: 2.9856 - val_loss: 5.5980 - val_mean_absolute_error: 6.1791\n",
            "Epoch 11/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.3588 - mean_absolute_error: 2.9317\n",
            "Epoch 11: val_mean_absolute_error did not improve from 5.26871\n",
            "48506/48506 [==============================] - 11s 234us/sample - loss: 2.3585 - mean_absolute_error: 2.9314 - val_loss: 4.9709 - val_mean_absolute_error: 5.5546\n",
            "Epoch 12/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.1160 - mean_absolute_error: 2.6825\n",
            "Epoch 12: val_mean_absolute_error did not improve from 5.26871\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 2.1160 - mean_absolute_error: 2.6825 - val_loss: 5.3222 - val_mean_absolute_error: 5.9029\n",
            "Epoch 13/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 2.1928 - mean_absolute_error: 2.7622\n",
            "Epoch 13: val_mean_absolute_error did not improve from 5.26871\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 2.1928 - mean_absolute_error: 2.7622 - val_loss: 5.4818 - val_mean_absolute_error: 6.0703\n",
            "Epoch 14/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.0583 - mean_absolute_error: 2.6276\n",
            "Epoch 14: val_mean_absolute_error did not improve from 5.26871\n",
            "48506/48506 [==============================] - 12s 240us/sample - loss: 2.0571 - mean_absolute_error: 2.6263 - val_loss: 5.4007 - val_mean_absolute_error: 5.9854\n",
            "Epoch 15/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 2.0344 - mean_absolute_error: 2.6046\n",
            "Epoch 15: val_mean_absolute_error improved from 5.26871 to 5.03436, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 12s 256us/sample - loss: 2.0343 - mean_absolute_error: 2.6045 - val_loss: 4.4533 - val_mean_absolute_error: 5.0344\n",
            "Epoch 16/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.9343 - mean_absolute_error: 2.5025\n",
            "Epoch 16: val_mean_absolute_error did not improve from 5.03436\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 1.9343 - mean_absolute_error: 2.5025 - val_loss: 4.8034 - val_mean_absolute_error: 5.3797\n",
            "Epoch 17/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.9245 - mean_absolute_error: 2.4938\n",
            "Epoch 17: val_mean_absolute_error did not improve from 5.03436\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.9282 - mean_absolute_error: 2.4977 - val_loss: 4.7019 - val_mean_absolute_error: 5.2812\n",
            "Epoch 18/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.8478 - mean_absolute_error: 2.4122\n",
            "Epoch 18: val_mean_absolute_error did not improve from 5.03436\n",
            "48506/48506 [==============================] - 12s 245us/sample - loss: 1.8478 - mean_absolute_error: 2.4122 - val_loss: 4.5077 - val_mean_absolute_error: 5.0777\n",
            "Epoch 19/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.7834 - mean_absolute_error: 2.3497\n",
            "Epoch 19: val_mean_absolute_error did not improve from 5.03436\n",
            "48506/48506 [==============================] - 12s 247us/sample - loss: 1.7834 - mean_absolute_error: 2.3497 - val_loss: 4.8846 - val_mean_absolute_error: 5.4670\n",
            "Epoch 20/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.8516 - mean_absolute_error: 2.4208\n",
            "Epoch 20: val_mean_absolute_error did not improve from 5.03436\n",
            "48506/48506 [==============================] - 12s 253us/sample - loss: 1.8500 - mean_absolute_error: 2.4191 - val_loss: 6.2145 - val_mean_absolute_error: 6.8050\n",
            "Epoch 21/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.7525 - mean_absolute_error: 2.3163\n",
            "Epoch 21: val_mean_absolute_error did not improve from 5.03436\n",
            "48506/48506 [==============================] - 13s 265us/sample - loss: 1.7525 - mean_absolute_error: 2.3163 - val_loss: 5.2680 - val_mean_absolute_error: 5.8544\n",
            "Epoch 22/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.7612 - mean_absolute_error: 2.3288\n",
            "Epoch 22: val_mean_absolute_error improved from 5.03436 to 4.95705, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg14.h5\n",
            "48506/48506 [==============================] - 13s 270us/sample - loss: 1.7628 - mean_absolute_error: 2.3306 - val_loss: 4.3857 - val_mean_absolute_error: 4.9570\n",
            "Epoch 23/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6936 - mean_absolute_error: 2.2581\n",
            "Epoch 23: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 264us/sample - loss: 1.6922 - mean_absolute_error: 2.2566 - val_loss: 4.5863 - val_mean_absolute_error: 5.1595\n",
            "Epoch 24/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.7018 - mean_absolute_error: 2.2683\n",
            "Epoch 24: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 264us/sample - loss: 1.6987 - mean_absolute_error: 2.2650 - val_loss: 5.9600 - val_mean_absolute_error: 6.5408\n",
            "Epoch 25/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.6785 - mean_absolute_error: 2.2451\n",
            "Epoch 25: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 268us/sample - loss: 1.6785 - mean_absolute_error: 2.2451 - val_loss: 4.5444 - val_mean_absolute_error: 5.1204\n",
            "Epoch 26/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5615 - mean_absolute_error: 2.1230\n",
            "Epoch 26: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 261us/sample - loss: 1.5615 - mean_absolute_error: 2.1230 - val_loss: 4.9213 - val_mean_absolute_error: 5.4961\n",
            "Epoch 27/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.6300 - mean_absolute_error: 2.1954\n",
            "Epoch 27: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 258us/sample - loss: 1.6356 - mean_absolute_error: 2.2012 - val_loss: 4.8948 - val_mean_absolute_error: 5.4653\n",
            "Epoch 28/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.6251 - mean_absolute_error: 2.1906\n",
            "Epoch 28: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 253us/sample - loss: 1.6235 - mean_absolute_error: 2.1888 - val_loss: 4.5378 - val_mean_absolute_error: 5.1132\n",
            "Epoch 29/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.5916 - mean_absolute_error: 2.1539\n",
            "Epoch 29: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.5883 - mean_absolute_error: 2.1501 - val_loss: 4.8841 - val_mean_absolute_error: 5.4685\n",
            "Epoch 30/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5185 - mean_absolute_error: 2.0792\n",
            "Epoch 30: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.5184 - mean_absolute_error: 2.0792 - val_loss: 5.3741 - val_mean_absolute_error: 5.9516\n",
            "Epoch 31/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5071 - mean_absolute_error: 2.0695\n",
            "Epoch 31: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.5071 - mean_absolute_error: 2.0695 - val_loss: 5.2963 - val_mean_absolute_error: 5.8809\n",
            "Epoch 32/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.5618 - mean_absolute_error: 2.1254\n",
            "Epoch 32: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 11s 232us/sample - loss: 1.5618 - mean_absolute_error: 2.1254 - val_loss: 4.9376 - val_mean_absolute_error: 5.5075\n",
            "Epoch 33/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5122 - mean_absolute_error: 2.0730\n",
            "Epoch 33: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 251us/sample - loss: 1.5136 - mean_absolute_error: 2.0746 - val_loss: 4.9240 - val_mean_absolute_error: 5.4950\n",
            "Epoch 34/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.5689 - mean_absolute_error: 2.1348\n",
            "Epoch 34: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 260us/sample - loss: 1.5676 - mean_absolute_error: 2.1335 - val_loss: 4.7465 - val_mean_absolute_error: 5.3172\n",
            "Epoch 35/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4563 - mean_absolute_error: 2.0110\n",
            "Epoch 35: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 260us/sample - loss: 1.4563 - mean_absolute_error: 2.0110 - val_loss: 5.7304 - val_mean_absolute_error: 6.3183\n",
            "Epoch 36/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4957 - mean_absolute_error: 2.0521\n",
            "Epoch 36: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 252us/sample - loss: 1.4957 - mean_absolute_error: 2.0521 - val_loss: 5.2537 - val_mean_absolute_error: 5.8410\n",
            "Epoch 37/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4607 - mean_absolute_error: 2.0221\n",
            "Epoch 37: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 256us/sample - loss: 1.4607 - mean_absolute_error: 2.0221 - val_loss: 4.9371 - val_mean_absolute_error: 5.5127\n",
            "Epoch 38/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4431 - mean_absolute_error: 2.0014\n",
            "Epoch 38: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 257us/sample - loss: 1.4424 - mean_absolute_error: 2.0006 - val_loss: 5.4314 - val_mean_absolute_error: 6.0191\n",
            "Epoch 39/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4575 - mean_absolute_error: 2.0140\n",
            "Epoch 39: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 261us/sample - loss: 1.4572 - mean_absolute_error: 2.0138 - val_loss: 5.0893 - val_mean_absolute_error: 5.6652\n",
            "Epoch 40/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4152 - mean_absolute_error: 1.9720\n",
            "Epoch 40: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 251us/sample - loss: 1.4152 - mean_absolute_error: 1.9720 - val_loss: 4.9771 - val_mean_absolute_error: 5.5492\n",
            "Epoch 41/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4924 - mean_absolute_error: 2.0543\n",
            "Epoch 41: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 258us/sample - loss: 1.4922 - mean_absolute_error: 2.0542 - val_loss: 5.1109 - val_mean_absolute_error: 5.6898\n",
            "Epoch 42/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4448 - mean_absolute_error: 2.0025\n",
            "Epoch 42: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 253us/sample - loss: 1.4429 - mean_absolute_error: 2.0004 - val_loss: 4.7645 - val_mean_absolute_error: 5.3349\n",
            "Epoch 43/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4788 - mean_absolute_error: 2.0431\n",
            "Epoch 43: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 242us/sample - loss: 1.4788 - mean_absolute_error: 2.0431 - val_loss: 4.5201 - val_mean_absolute_error: 5.0960\n",
            "Epoch 44/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4313 - mean_absolute_error: 1.9898\n",
            "Epoch 44: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 238us/sample - loss: 1.4313 - mean_absolute_error: 1.9898 - val_loss: 4.8441 - val_mean_absolute_error: 5.4270\n",
            "Epoch 45/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4422 - mean_absolute_error: 2.0019\n",
            "Epoch 45: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 14s 285us/sample - loss: 1.4422 - mean_absolute_error: 2.0019 - val_loss: 4.9529 - val_mean_absolute_error: 5.5284\n",
            "Epoch 46/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4094 - mean_absolute_error: 1.9653\n",
            "Epoch 46: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 276us/sample - loss: 1.4090 - mean_absolute_error: 1.9649 - val_loss: 4.7359 - val_mean_absolute_error: 5.3066\n",
            "Epoch 47/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4081 - mean_absolute_error: 1.9676\n",
            "Epoch 47: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 267us/sample - loss: 1.4116 - mean_absolute_error: 1.9713 - val_loss: 5.1145 - val_mean_absolute_error: 5.6858\n",
            "Epoch 48/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4399 - mean_absolute_error: 2.0037\n",
            "Epoch 48: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 14s 279us/sample - loss: 1.4380 - mean_absolute_error: 2.0016 - val_loss: 5.3123 - val_mean_absolute_error: 5.8966\n",
            "Epoch 49/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4565 - mean_absolute_error: 2.0210\n",
            "Epoch 49: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 261us/sample - loss: 1.4565 - mean_absolute_error: 2.0210 - val_loss: 5.5147 - val_mean_absolute_error: 6.0878\n",
            "Epoch 50/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4090 - mean_absolute_error: 1.9695\n",
            "Epoch 50: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 271us/sample - loss: 1.4090 - mean_absolute_error: 1.9695 - val_loss: 5.3437 - val_mean_absolute_error: 5.9152\n",
            "Epoch 51/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.4097 - mean_absolute_error: 1.9667\n",
            "Epoch 51: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 248us/sample - loss: 1.4097 - mean_absolute_error: 1.9667 - val_loss: 4.7451 - val_mean_absolute_error: 5.3171\n",
            "Epoch 52/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.3561 - mean_absolute_error: 1.9089\n",
            "Epoch 52: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 271us/sample - loss: 1.3597 - mean_absolute_error: 1.9129 - val_loss: 5.2739 - val_mean_absolute_error: 5.8587\n",
            "Epoch 53/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3979 - mean_absolute_error: 1.9548\n",
            "Epoch 53: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 13s 261us/sample - loss: 1.3979 - mean_absolute_error: 1.9548 - val_loss: 5.3811 - val_mean_absolute_error: 5.9618\n",
            "Epoch 54/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3796 - mean_absolute_error: 1.9340\n",
            "Epoch 54: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 249us/sample - loss: 1.3796 - mean_absolute_error: 1.9340 - val_loss: 5.0323 - val_mean_absolute_error: 5.6125\n",
            "Epoch 55/500\n",
            "48256/48506 [============================>.] - ETA: 0s - loss: 1.3819 - mean_absolute_error: 1.9395\n",
            "Epoch 55: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 253us/sample - loss: 1.3814 - mean_absolute_error: 1.9390 - val_loss: 4.6748 - val_mean_absolute_error: 5.2436\n",
            "Epoch 56/500\n",
            "48384/48506 [============================>.] - ETA: 0s - loss: 1.4069 - mean_absolute_error: 1.9670\n",
            "Epoch 56: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 1.4046 - mean_absolute_error: 1.9644 - val_loss: 5.2102 - val_mean_absolute_error: 5.7822\n",
            "Epoch 57/500\n",
            "48506/48506 [==============================] - ETA: 0s - loss: 1.3087 - mean_absolute_error: 1.8627\n",
            "Epoch 57: val_mean_absolute_error did not improve from 4.95705\n",
            "48506/48506 [==============================] - 12s 250us/sample - loss: 1.3087 - mean_absolute_error: 1.8627 - val_loss: 5.2535 - val_mean_absolute_error: 5.8266\n",
            "Epoch 57: early stopping\n",
            "2023-05-26 12:28:11.364791: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_35/BiasAdd' id:26144 op device:{requested: '', assigned: ''} def:{{{node dense_35/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_35/MatMul, dense_35/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507, 14: 2.903993154764576}\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_80 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_132 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_132 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_81 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_133 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_133 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_24 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_82 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_36 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_134 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_134 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_83 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_135 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_135 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_84 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_136 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_136 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_25 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_85 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_37 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_137 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_137 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_86 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_138 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_138 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_87 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_139 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_139 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_26 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_88 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_38 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_140 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_140 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_141 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_141 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_142 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_142 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "(47752, 4, 256)\n",
            "(12342, 4, 256)\n",
            "(4603, 4, 256)\n",
            "Train on 47752 samples, validate on 12342 samples\n",
            "2023-05-26 12:28:19.004848: W tensorflow/c/c_api.cc:300] Operation '{name:'training_20/Adam/dense_36/kernel/v/Assign' id:29418 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/dense_36/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/dense_36/kernel/v, training_20/Adam/dense_36/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 84.7584 - mean_absolute_error: 85.4515/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n",
            "2023-05-26 12:28:35.916015: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_10/mul' id:28455 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/dense_38_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 62.25583, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 32s 680us/sample - loss: 84.7584 - mean_absolute_error: 85.4515 - val_loss: 61.5627 - val_mean_absolute_error: 62.2558\n",
            "Epoch 2/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 47.0976 - mean_absolute_error: 47.7896\n",
            "Epoch 2: val_mean_absolute_error improved from 62.25583 to 16.85280, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 13s 263us/sample - loss: 47.0931 - mean_absolute_error: 47.7851 - val_loss: 16.1745 - val_mean_absolute_error: 16.8528\n",
            "Epoch 3/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 6.2747 - mean_absolute_error: 6.8949\n",
            "Epoch 3: val_mean_absolute_error improved from 16.85280 to 5.98186, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 13s 264us/sample - loss: 6.2747 - mean_absolute_error: 6.8949 - val_loss: 5.3640 - val_mean_absolute_error: 5.9819\n",
            "Epoch 4/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 3.7253 - mean_absolute_error: 4.3175\n",
            "Epoch 4: val_mean_absolute_error improved from 5.98186 to 5.51565, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 12s 260us/sample - loss: 3.7231 - mean_absolute_error: 4.3152 - val_loss: 4.9085 - val_mean_absolute_error: 5.5157\n",
            "Epoch 5/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 3.3374 - mean_absolute_error: 3.9266\n",
            "Epoch 5: val_mean_absolute_error improved from 5.51565 to 5.21865, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 13s 263us/sample - loss: 3.3387 - mean_absolute_error: 3.9278 - val_loss: 4.6444 - val_mean_absolute_error: 5.2186\n",
            "Epoch 6/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 3.1643 - mean_absolute_error: 3.7508\n",
            "Epoch 6: val_mean_absolute_error did not improve from 5.21865\n",
            "47752/47752 [==============================] - 12s 251us/sample - loss: 3.1661 - mean_absolute_error: 3.7527 - val_loss: 4.9300 - val_mean_absolute_error: 5.5280\n",
            "Epoch 7/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.8911 - mean_absolute_error: 3.4726\n",
            "Epoch 7: val_mean_absolute_error did not improve from 5.21865\n",
            "47752/47752 [==============================] - 11s 233us/sample - loss: 2.8911 - mean_absolute_error: 3.4726 - val_loss: 4.8895 - val_mean_absolute_error: 5.4740\n",
            "Epoch 8/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.7472 - mean_absolute_error: 3.3280\n",
            "Epoch 8: val_mean_absolute_error did not improve from 5.21865\n",
            "47752/47752 [==============================] - 11s 231us/sample - loss: 2.7485 - mean_absolute_error: 3.3293 - val_loss: 4.8611 - val_mean_absolute_error: 5.4432\n",
            "Epoch 9/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.6211 - mean_absolute_error: 3.2011\n",
            "Epoch 9: val_mean_absolute_error did not improve from 5.21865\n",
            "47752/47752 [==============================] - 12s 252us/sample - loss: 2.6209 - mean_absolute_error: 3.2009 - val_loss: 4.7061 - val_mean_absolute_error: 5.2999\n",
            "Epoch 10/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.4245 - mean_absolute_error: 3.0002\n",
            "Epoch 10: val_mean_absolute_error improved from 5.21865 to 5.20222, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 12s 262us/sample - loss: 2.4257 - mean_absolute_error: 3.0014 - val_loss: 4.6303 - val_mean_absolute_error: 5.2022\n",
            "Epoch 11/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.6521 - mean_absolute_error: 3.2325\n",
            "Epoch 11: val_mean_absolute_error did not improve from 5.20222\n",
            "47752/47752 [==============================] - 12s 253us/sample - loss: 2.6531 - mean_absolute_error: 3.2335 - val_loss: 4.7829 - val_mean_absolute_error: 5.3837\n",
            "Epoch 12/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.3153 - mean_absolute_error: 2.8902\n",
            "Epoch 12: val_mean_absolute_error did not improve from 5.20222\n",
            "47752/47752 [==============================] - 12s 253us/sample - loss: 2.3153 - mean_absolute_error: 2.8902 - val_loss: 4.6751 - val_mean_absolute_error: 5.2344\n",
            "Epoch 13/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 2.2567 - mean_absolute_error: 2.8322\n",
            "Epoch 13: val_mean_absolute_error improved from 5.20222 to 5.06121, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 12s 259us/sample - loss: 2.2597 - mean_absolute_error: 2.8352 - val_loss: 4.4993 - val_mean_absolute_error: 5.0612\n",
            "Epoch 14/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.1687 - mean_absolute_error: 2.7432\n",
            "Epoch 14: val_mean_absolute_error did not improve from 5.06121\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 2.1687 - mean_absolute_error: 2.7432 - val_loss: 4.5332 - val_mean_absolute_error: 5.1126\n",
            "Epoch 15/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0003 - mean_absolute_error: 2.5682\n",
            "Epoch 15: val_mean_absolute_error did not improve from 5.06121\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 2.0023 - mean_absolute_error: 2.5702 - val_loss: 5.3061 - val_mean_absolute_error: 5.8897\n",
            "Epoch 16/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.1740 - mean_absolute_error: 2.7508\n",
            "Epoch 16: val_mean_absolute_error improved from 5.06121 to 5.03278, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg1.h5\n",
            "47752/47752 [==============================] - 11s 237us/sample - loss: 2.1753 - mean_absolute_error: 2.7520 - val_loss: 4.4744 - val_mean_absolute_error: 5.0328\n",
            "Epoch 17/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0155 - mean_absolute_error: 2.5870\n",
            "Epoch 17: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 226us/sample - loss: 2.0166 - mean_absolute_error: 2.5881 - val_loss: 4.8097 - val_mean_absolute_error: 5.4030\n",
            "Epoch 18/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 2.0161 - mean_absolute_error: 2.5885\n",
            "Epoch 18: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 2.0165 - mean_absolute_error: 2.5890 - val_loss: 4.6311 - val_mean_absolute_error: 5.1962\n",
            "Epoch 19/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0391 - mean_absolute_error: 2.6139\n",
            "Epoch 19: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 2.0411 - mean_absolute_error: 2.6159 - val_loss: 4.7199 - val_mean_absolute_error: 5.2976\n",
            "Epoch 20/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.8878 - mean_absolute_error: 2.4604\n",
            "Epoch 20: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 248us/sample - loss: 1.8863 - mean_absolute_error: 2.4587 - val_loss: 4.6394 - val_mean_absolute_error: 5.1997\n",
            "Epoch 21/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.9554 - mean_absolute_error: 2.5325\n",
            "Epoch 21: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 244us/sample - loss: 1.9554 - mean_absolute_error: 2.5325 - val_loss: 4.5923 - val_mean_absolute_error: 5.1507\n",
            "Epoch 22/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.8240 - mean_absolute_error: 2.3937\n",
            "Epoch 22: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 245us/sample - loss: 1.8254 - mean_absolute_error: 2.3951 - val_loss: 4.7089 - val_mean_absolute_error: 5.2784\n",
            "Epoch 23/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8378 - mean_absolute_error: 2.4102\n",
            "Epoch 23: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 241us/sample - loss: 1.8378 - mean_absolute_error: 2.4102 - val_loss: 4.7214 - val_mean_absolute_error: 5.2951\n",
            "Epoch 24/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8559 - mean_absolute_error: 2.4309\n",
            "Epoch 24: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 223us/sample - loss: 1.8559 - mean_absolute_error: 2.4309 - val_loss: 4.9110 - val_mean_absolute_error: 5.4888\n",
            "Epoch 25/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8111 - mean_absolute_error: 2.3849\n",
            "Epoch 25: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 235us/sample - loss: 1.8111 - mean_absolute_error: 2.3849 - val_loss: 4.7044 - val_mean_absolute_error: 5.2647\n",
            "Epoch 26/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7765 - mean_absolute_error: 2.3501\n",
            "Epoch 26: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 246us/sample - loss: 1.7765 - mean_absolute_error: 2.3501 - val_loss: 4.7878 - val_mean_absolute_error: 5.3637\n",
            "Epoch 27/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.7911 - mean_absolute_error: 2.3631\n",
            "Epoch 27: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.7895 - mean_absolute_error: 2.3613 - val_loss: 4.7369 - val_mean_absolute_error: 5.3220\n",
            "Epoch 28/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7649 - mean_absolute_error: 2.3386\n",
            "Epoch 28: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 241us/sample - loss: 1.7649 - mean_absolute_error: 2.3386 - val_loss: 4.5685 - val_mean_absolute_error: 5.1361\n",
            "Epoch 29/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0452 - mean_absolute_error: 2.6212\n",
            "Epoch 29: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 236us/sample - loss: 2.0472 - mean_absolute_error: 2.6232 - val_loss: 4.8171 - val_mean_absolute_error: 5.3837\n",
            "Epoch 30/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7984 - mean_absolute_error: 2.3708\n",
            "Epoch 30: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 234us/sample - loss: 1.7984 - mean_absolute_error: 2.3708 - val_loss: 4.5450 - val_mean_absolute_error: 5.1082\n",
            "Epoch 31/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7329 - mean_absolute_error: 2.3042\n",
            "Epoch 31: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 10s 216us/sample - loss: 1.7346 - mean_absolute_error: 2.3058 - val_loss: 5.0095 - val_mean_absolute_error: 5.5860\n",
            "Epoch 32/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6823 - mean_absolute_error: 2.2529\n",
            "Epoch 32: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.6824 - mean_absolute_error: 2.2530 - val_loss: 5.0612 - val_mean_absolute_error: 5.6662\n",
            "Epoch 33/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6286 - mean_absolute_error: 2.1988\n",
            "Epoch 33: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.6286 - mean_absolute_error: 2.1988 - val_loss: 4.6408 - val_mean_absolute_error: 5.2005\n",
            "Epoch 34/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5255 - mean_absolute_error: 2.0875\n",
            "Epoch 34: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.5260 - mean_absolute_error: 2.0880 - val_loss: 4.8786 - val_mean_absolute_error: 5.4655\n",
            "Epoch 35/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.6527 - mean_absolute_error: 2.2225\n",
            "Epoch 35: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 260us/sample - loss: 1.6581 - mean_absolute_error: 2.2282 - val_loss: 4.6708 - val_mean_absolute_error: 5.2354\n",
            "Epoch 36/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5913 - mean_absolute_error: 2.1549\n",
            "Epoch 36: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 242us/sample - loss: 1.5918 - mean_absolute_error: 2.1555 - val_loss: 4.7667 - val_mean_absolute_error: 5.3493\n",
            "Epoch 37/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7157 - mean_absolute_error: 2.2876\n",
            "Epoch 37: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 236us/sample - loss: 1.7157 - mean_absolute_error: 2.2876 - val_loss: 4.8879 - val_mean_absolute_error: 5.4646\n",
            "Epoch 38/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.8077 - mean_absolute_error: 2.3764\n",
            "Epoch 38: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 10s 214us/sample - loss: 1.8089 - mean_absolute_error: 2.3777 - val_loss: 4.7864 - val_mean_absolute_error: 5.3603\n",
            "Epoch 39/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0964 - mean_absolute_error: 2.6683\n",
            "Epoch 39: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 237us/sample - loss: 2.0976 - mean_absolute_error: 2.6695 - val_loss: 4.8141 - val_mean_absolute_error: 5.3993\n",
            "Epoch 40/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6131 - mean_absolute_error: 2.1765\n",
            "Epoch 40: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.6136 - mean_absolute_error: 2.1770 - val_loss: 4.6592 - val_mean_absolute_error: 5.2175\n",
            "Epoch 41/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.6642 - mean_absolute_error: 2.2348\n",
            "Epoch 41: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.6693 - mean_absolute_error: 2.2400 - val_loss: 4.8610 - val_mean_absolute_error: 5.4434\n",
            "Epoch 42/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5705 - mean_absolute_error: 2.1388\n",
            "Epoch 42: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.5721 - mean_absolute_error: 2.1404 - val_loss: 4.7148 - val_mean_absolute_error: 5.2861\n",
            "Epoch 43/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6048 - mean_absolute_error: 2.1718\n",
            "Epoch 43: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 233us/sample - loss: 1.6052 - mean_absolute_error: 2.1722 - val_loss: 4.8261 - val_mean_absolute_error: 5.3871\n",
            "Epoch 44/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6536 - mean_absolute_error: 2.2268\n",
            "Epoch 44: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 10s 213us/sample - loss: 1.6539 - mean_absolute_error: 2.2271 - val_loss: 4.7273 - val_mean_absolute_error: 5.2874\n",
            "Epoch 45/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.4972 - mean_absolute_error: 2.0622\n",
            "Epoch 45: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 232us/sample - loss: 1.4973 - mean_absolute_error: 2.0623 - val_loss: 4.5952 - val_mean_absolute_error: 5.1654\n",
            "Epoch 46/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.4896 - mean_absolute_error: 2.0514\n",
            "Epoch 46: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.4906 - mean_absolute_error: 2.0524 - val_loss: 4.6341 - val_mean_absolute_error: 5.2040\n",
            "Epoch 47/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6740 - mean_absolute_error: 2.2418\n",
            "Epoch 47: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.6740 - mean_absolute_error: 2.2418 - val_loss: 4.8573 - val_mean_absolute_error: 5.4257\n",
            "Epoch 48/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5571 - mean_absolute_error: 2.1249\n",
            "Epoch 48: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 256us/sample - loss: 1.5590 - mean_absolute_error: 2.1268 - val_loss: 4.6926 - val_mean_absolute_error: 5.2560\n",
            "Epoch 49/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.5554 - mean_absolute_error: 2.1176\n",
            "Epoch 49: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 248us/sample - loss: 1.5554 - mean_absolute_error: 2.1176 - val_loss: 4.7568 - val_mean_absolute_error: 5.3217\n",
            "Epoch 50/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.4573 - mean_absolute_error: 2.0179\n",
            "Epoch 50: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 255us/sample - loss: 1.4573 - mean_absolute_error: 2.0179 - val_loss: 4.6753 - val_mean_absolute_error: 5.2416\n",
            "Epoch 51/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.4471 - mean_absolute_error: 2.0074\n",
            "Epoch 51: val_mean_absolute_error did not improve from 5.03278\n",
            "47752/47752 [==============================] - 12s 244us/sample - loss: 1.4471 - mean_absolute_error: 2.0074 - val_loss: 4.6634 - val_mean_absolute_error: 5.2360\n",
            "Epoch 51: early stopping\n",
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2023-05-26 12:38:34.669368: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_38/BiasAdd' id:28404 op device:{requested: '', assigned: ''} def:{{{node dense_38/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_38/MatMul, dense_38/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507, 14: 2.903993154764576, 1: 4.195189173427488}\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_89 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_143 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_143 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_90 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_144 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_144 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_27 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_91 (Conv2D)          (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_39 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_145 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_145 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_92 (Conv2D)          (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_146 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_146 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_93 (Conv2D)          (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_147 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_147 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_28 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_94 (Conv2D)          (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_40 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_148 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_148 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_95 (Conv2D)          (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_149 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_149 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_96 (Conv2D)          (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_150 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_150 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_29 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_97 (Conv2D)          (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_41 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_151 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_151 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_152 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_152 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_153 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_153 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(47752, 4, 256)\n",
            "(12846, 4, 256)\n",
            "(4099, 4, 256)\n",
            "Train on 47752 samples, validate on 12846 samples\n",
            "2023-05-26 12:38:43.045275: W tensorflow/c/c_api.cc:300] Operation '{name:'conv2d_96/bias/Assign' id:30344 op device:{requested: '', assigned: ''} def:{{{node conv2d_96/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_96/bias, conv2d_96/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 84.8071 - mean_absolute_error: 85.50022023-05-26 12:38:58.005375: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_11/mul' id:30715 op device:{requested: '', assigned: ''} def:{{{node loss_11/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_11/mul/x, loss_11/dense_41_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 58.08142, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg2.h5\n",
            "47752/47752 [==============================] - 33s 690us/sample - loss: 84.8071 - mean_absolute_error: 85.5002 - val_loss: 57.3883 - val_mean_absolute_error: 58.0814\n",
            "Epoch 2/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 47.2629 - mean_absolute_error: 47.9550\n",
            "Epoch 2: val_mean_absolute_error improved from 58.08142 to 11.64314, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg2.h5\n",
            "47752/47752 [==============================] - 12s 252us/sample - loss: 47.1808 - mean_absolute_error: 47.8728 - val_loss: 10.9856 - val_mean_absolute_error: 11.6431\n",
            "Epoch 3/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 6.1257 - mean_absolute_error: 6.7456\n",
            "Epoch 3: val_mean_absolute_error improved from 11.64314 to 7.00273, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg2.h5\n",
            "47752/47752 [==============================] - 12s 250us/sample - loss: 6.1252 - mean_absolute_error: 6.7451 - val_loss: 6.3861 - val_mean_absolute_error: 7.0027\n",
            "Epoch 4/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 3.7918 - mean_absolute_error: 4.3856\n",
            "Epoch 4: val_mean_absolute_error improved from 7.00273 to 5.41324, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg2.h5\n",
            "47752/47752 [==============================] - 12s 250us/sample - loss: 3.7918 - mean_absolute_error: 4.3856 - val_loss: 4.8222 - val_mean_absolute_error: 5.4132\n",
            "Epoch 5/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 3.3479 - mean_absolute_error: 3.9342\n",
            "Epoch 5: val_mean_absolute_error did not improve from 5.41324\n",
            "47752/47752 [==============================] - 11s 235us/sample - loss: 3.3479 - mean_absolute_error: 3.9342 - val_loss: 5.2420 - val_mean_absolute_error: 5.8375\n",
            "Epoch 6/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 3.1966 - mean_absolute_error: 3.7849\n",
            "Epoch 6: val_mean_absolute_error did not improve from 5.41324\n",
            "47752/47752 [==============================] - 11s 223us/sample - loss: 3.1981 - mean_absolute_error: 3.7864 - val_loss: 4.9186 - val_mean_absolute_error: 5.5147\n",
            "Epoch 7/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.9889 - mean_absolute_error: 3.5751\n",
            "Epoch 7: val_mean_absolute_error improved from 5.41324 to 5.18916, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg2.h5\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 2.9889 - mean_absolute_error: 3.5751 - val_loss: 4.6054 - val_mean_absolute_error: 5.1892\n",
            "Epoch 8/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.7886 - mean_absolute_error: 3.3693\n",
            "Epoch 8: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 256us/sample - loss: 2.7886 - mean_absolute_error: 3.3693 - val_loss: 4.7085 - val_mean_absolute_error: 5.2871\n",
            "Epoch 9/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.6169 - mean_absolute_error: 3.1970\n",
            "Epoch 9: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 2.6169 - mean_absolute_error: 3.1970 - val_loss: 4.7050 - val_mean_absolute_error: 5.2955\n",
            "Epoch 10/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.5279 - mean_absolute_error: 3.1075\n",
            "Epoch 10: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 252us/sample - loss: 2.5292 - mean_absolute_error: 3.1087 - val_loss: 5.3523 - val_mean_absolute_error: 5.9327\n",
            "Epoch 11/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.3363 - mean_absolute_error: 2.9110\n",
            "Epoch 11: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 247us/sample - loss: 2.3363 - mean_absolute_error: 2.9110 - val_loss: 5.1752 - val_mean_absolute_error: 5.7722\n",
            "Epoch 12/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.2317 - mean_absolute_error: 2.8056\n",
            "Epoch 12: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 257us/sample - loss: 2.2319 - mean_absolute_error: 2.8058 - val_loss: 5.0225 - val_mean_absolute_error: 5.6013\n",
            "Epoch 13/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.2332 - mean_absolute_error: 2.8114\n",
            "Epoch 13: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 245us/sample - loss: 2.2332 - mean_absolute_error: 2.8114 - val_loss: 5.0176 - val_mean_absolute_error: 5.6122\n",
            "Epoch 14/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.2064 - mean_absolute_error: 2.7801\n",
            "Epoch 14: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 13s 262us/sample - loss: 2.2064 - mean_absolute_error: 2.7801 - val_loss: 5.5912 - val_mean_absolute_error: 6.2199\n",
            "Epoch 15/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.1605 - mean_absolute_error: 2.7389\n",
            "Epoch 15: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 228us/sample - loss: 2.1606 - mean_absolute_error: 2.7389 - val_loss: 4.6469 - val_mean_absolute_error: 5.2183\n",
            "Epoch 16/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9850 - mean_absolute_error: 2.5558\n",
            "Epoch 16: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 246us/sample - loss: 1.9873 - mean_absolute_error: 2.5581 - val_loss: 4.8436 - val_mean_absolute_error: 5.4228\n",
            "Epoch 17/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9684 - mean_absolute_error: 2.5404\n",
            "Epoch 17: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 1.9698 - mean_absolute_error: 2.5418 - val_loss: 5.5669 - val_mean_absolute_error: 6.1695\n",
            "Epoch 18/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.9222 - mean_absolute_error: 2.4939\n",
            "Epoch 18: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.9222 - mean_absolute_error: 2.4939 - val_loss: 4.8102 - val_mean_absolute_error: 5.3857\n",
            "Epoch 19/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9931 - mean_absolute_error: 2.5644\n",
            "Epoch 19: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.9939 - mean_absolute_error: 2.5652 - val_loss: 4.7973 - val_mean_absolute_error: 5.3654\n",
            "Epoch 20/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.8359 - mean_absolute_error: 2.4049\n",
            "Epoch 20: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 1.8395 - mean_absolute_error: 2.4088 - val_loss: 4.8420 - val_mean_absolute_error: 5.4214\n",
            "Epoch 21/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8939 - mean_absolute_error: 2.4645\n",
            "Epoch 21: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.8939 - mean_absolute_error: 2.4645 - val_loss: 5.1583 - val_mean_absolute_error: 5.7331\n",
            "Epoch 22/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7363 - mean_absolute_error: 2.3042\n",
            "Epoch 22: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 228us/sample - loss: 1.7363 - mean_absolute_error: 2.3042 - val_loss: 5.1470 - val_mean_absolute_error: 5.7275\n",
            "Epoch 23/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7612 - mean_absolute_error: 2.3318\n",
            "Epoch 23: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 222us/sample - loss: 1.7616 - mean_absolute_error: 2.3322 - val_loss: 5.0981 - val_mean_absolute_error: 5.6742\n",
            "Epoch 24/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.7535 - mean_absolute_error: 2.3260\n",
            "Epoch 24: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 242us/sample - loss: 1.7526 - mean_absolute_error: 2.3250 - val_loss: 5.0252 - val_mean_absolute_error: 5.6162\n",
            "Epoch 25/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7331 - mean_absolute_error: 2.3046\n",
            "Epoch 25: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 245us/sample - loss: 1.7336 - mean_absolute_error: 2.3052 - val_loss: 5.0983 - val_mean_absolute_error: 5.6787\n",
            "Epoch 26/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7678 - mean_absolute_error: 2.3423\n",
            "Epoch 26: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 1.7678 - mean_absolute_error: 2.3423 - val_loss: 5.0347 - val_mean_absolute_error: 5.6069\n",
            "Epoch 27/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6856 - mean_absolute_error: 2.2523\n",
            "Epoch 27: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.6855 - mean_absolute_error: 2.2522 - val_loss: 5.0072 - val_mean_absolute_error: 5.5933\n",
            "Epoch 28/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6411 - mean_absolute_error: 2.2087\n",
            "Epoch 28: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.6420 - mean_absolute_error: 2.2096 - val_loss: 4.8590 - val_mean_absolute_error: 5.4415\n",
            "Epoch 29/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6866 - mean_absolute_error: 2.2576\n",
            "Epoch 29: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 221us/sample - loss: 1.6868 - mean_absolute_error: 2.2577 - val_loss: 4.8789 - val_mean_absolute_error: 5.4505\n",
            "Epoch 30/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.6337 - mean_absolute_error: 2.2012\n",
            "Epoch 30: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 230us/sample - loss: 1.6352 - mean_absolute_error: 2.2029 - val_loss: 4.8755 - val_mean_absolute_error: 5.4518\n",
            "Epoch 31/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7367 - mean_absolute_error: 2.3090\n",
            "Epoch 31: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 1.7367 - mean_absolute_error: 2.3090 - val_loss: 4.8672 - val_mean_absolute_error: 5.4358\n",
            "Epoch 32/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7210 - mean_absolute_error: 2.2964\n",
            "Epoch 32: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 237us/sample - loss: 1.7210 - mean_absolute_error: 2.2964 - val_loss: 4.9705 - val_mean_absolute_error: 5.5528\n",
            "Epoch 33/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6206 - mean_absolute_error: 2.1911\n",
            "Epoch 33: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.6206 - mean_absolute_error: 2.1911 - val_loss: 4.9432 - val_mean_absolute_error: 5.5196\n",
            "Epoch 34/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6090 - mean_absolute_error: 2.1786\n",
            "Epoch 34: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.6109 - mean_absolute_error: 2.1805 - val_loss: 4.7694 - val_mean_absolute_error: 5.3372\n",
            "Epoch 35/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.5610 - mean_absolute_error: 2.1253\n",
            "Epoch 35: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 229us/sample - loss: 1.5610 - mean_absolute_error: 2.1253 - val_loss: 4.8223 - val_mean_absolute_error: 5.3927\n",
            "Epoch 36/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5965 - mean_absolute_error: 2.1649\n",
            "Epoch 36: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 222us/sample - loss: 1.5977 - mean_absolute_error: 2.1661 - val_loss: 4.9805 - val_mean_absolute_error: 5.5538\n",
            "Epoch 37/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6100 - mean_absolute_error: 2.1768\n",
            "Epoch 37: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 12s 241us/sample - loss: 1.6118 - mean_absolute_error: 2.1785 - val_loss: 4.8898 - val_mean_absolute_error: 5.4591\n",
            "Epoch 38/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6662 - mean_absolute_error: 2.2346\n",
            "Epoch 38: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 237us/sample - loss: 1.6662 - mean_absolute_error: 2.2346 - val_loss: 5.0453 - val_mean_absolute_error: 5.6268\n",
            "Epoch 39/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.5263 - mean_absolute_error: 2.0920\n",
            "Epoch 39: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.5263 - mean_absolute_error: 2.0920 - val_loss: 4.9801 - val_mean_absolute_error: 5.5514\n",
            "Epoch 40/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5912 - mean_absolute_error: 2.1619\n",
            "Epoch 40: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.5923 - mean_absolute_error: 2.1630 - val_loss: 5.2235 - val_mean_absolute_error: 5.8206\n",
            "Epoch 41/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.5039 - mean_absolute_error: 2.0669\n",
            "Epoch 41: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 11s 237us/sample - loss: 1.5039 - mean_absolute_error: 2.0669 - val_loss: 4.8352 - val_mean_absolute_error: 5.4083\n",
            "Epoch 42/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5493 - mean_absolute_error: 2.1150\n",
            "Epoch 42: val_mean_absolute_error did not improve from 5.18916\n",
            "47752/47752 [==============================] - 10s 217us/sample - loss: 1.5528 - mean_absolute_error: 2.1185 - val_loss: 4.9345 - val_mean_absolute_error: 5.5071\n",
            "Epoch 42: early stopping\n",
            "2023-05-26 12:47:09.318063: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_41/BiasAdd' id:30664 op device:{requested: '', assigned: ''} def:{{{node dense_41/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_41/MatMul, dense_41/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507, 14: 2.903993154764576, 1: 4.195189173427488, 2: 3.5173868851195462}\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_98 (Conv2D)          (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_154 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_154 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_99 (Conv2D)          (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_155 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_155 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_30 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_100 (Conv2D)         (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_42 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_156 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_156 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_101 (Conv2D)         (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_157 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_157 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_102 (Conv2D)         (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_158 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_158 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_31 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_103 (Conv2D)         (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_43 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_159 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_159 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_104 (Conv2D)         (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_160 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_160 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_105 (Conv2D)         (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_161 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_161 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_32 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_106 (Conv2D)         (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_44 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_162 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_162 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_163 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_163 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_164 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_164 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(47752, 4, 256)\n",
            "(12668, 4, 256)\n",
            "(4277, 4, 256)\n",
            "Train on 47752 samples, validate on 12668 samples\n",
            "2023-05-26 12:47:18.208190: W tensorflow/c/c_api.cc:300] Operation '{name:'conv2d_105/kernel/Assign' id:32599 op device:{requested: '', assigned: ''} def:{{{node conv2d_105/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv2d_105/kernel, conv2d_105/kernel/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 84.9317 - mean_absolute_error: 85.62492023-05-26 12:47:33.990698: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_12/mul' id:32975 op device:{requested: '', assigned: ''} def:{{{node loss_12/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_12/mul/x, loss_12/dense_44_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 66.41836, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5\n",
            "47752/47752 [==============================] - 35s 739us/sample - loss: 84.9318 - mean_absolute_error: 85.6250 - val_loss: 65.7252 - val_mean_absolute_error: 66.4184\n",
            "Epoch 2/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 47.4894 - mean_absolute_error: 48.1815\n",
            "Epoch 2: val_mean_absolute_error improved from 66.41836 to 19.93173, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5\n",
            "47752/47752 [==============================] - 13s 268us/sample - loss: 47.4060 - mean_absolute_error: 48.0980 - val_loss: 19.2467 - val_mean_absolute_error: 19.9317\n",
            "Epoch 3/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 6.3414 - mean_absolute_error: 6.9624\n",
            "Epoch 3: val_mean_absolute_error improved from 19.93173 to 5.16140, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5\n",
            "47752/47752 [==============================] - 12s 256us/sample - loss: 6.3414 - mean_absolute_error: 6.9624 - val_loss: 4.5687 - val_mean_absolute_error: 5.1614\n",
            "Epoch 4/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 3.8940 - mean_absolute_error: 4.4907\n",
            "Epoch 4: val_mean_absolute_error improved from 5.16140 to 3.79309, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5\n",
            "47752/47752 [==============================] - 12s 250us/sample - loss: 3.8942 - mean_absolute_error: 4.4910 - val_loss: 3.2361 - val_mean_absolute_error: 3.7931\n",
            "Epoch 5/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 3.4208 - mean_absolute_error: 4.0114\n",
            "Epoch 5: val_mean_absolute_error did not improve from 3.79309\n",
            "47752/47752 [==============================] - 12s 256us/sample - loss: 3.4207 - mean_absolute_error: 4.0113 - val_loss: 3.9606 - val_mean_absolute_error: 4.5568\n",
            "Epoch 6/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 3.1308 - mean_absolute_error: 3.7167\n",
            "Epoch 6: val_mean_absolute_error improved from 3.79309 to 3.78011, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5\n",
            "47752/47752 [==============================] - 13s 271us/sample - loss: 3.1308 - mean_absolute_error: 3.7167 - val_loss: 3.2262 - val_mean_absolute_error: 3.7801\n",
            "Epoch 7/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.9667 - mean_absolute_error: 3.5525\n",
            "Epoch 7: val_mean_absolute_error did not improve from 3.78011\n",
            "47752/47752 [==============================] - 12s 255us/sample - loss: 2.9667 - mean_absolute_error: 3.5525 - val_loss: 3.5221 - val_mean_absolute_error: 4.0980\n",
            "Epoch 8/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.7509 - mean_absolute_error: 3.3327\n",
            "Epoch 8: val_mean_absolute_error did not improve from 3.78011\n",
            "47752/47752 [==============================] - 10s 214us/sample - loss: 2.7535 - mean_absolute_error: 3.3353 - val_loss: 3.3474 - val_mean_absolute_error: 3.9290\n",
            "Epoch 9/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 2.6864 - mean_absolute_error: 3.2689\n",
            "Epoch 9: val_mean_absolute_error did not improve from 3.78011\n",
            "47752/47752 [==============================] - 11s 231us/sample - loss: 2.6902 - mean_absolute_error: 3.2728 - val_loss: 3.4017 - val_mean_absolute_error: 3.9611\n",
            "Epoch 10/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 2.5115 - mean_absolute_error: 3.0885\n",
            "Epoch 10: val_mean_absolute_error did not improve from 3.78011\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 2.5153 - mean_absolute_error: 3.0926 - val_loss: 3.3486 - val_mean_absolute_error: 3.9150\n",
            "Epoch 11/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 2.4322 - mean_absolute_error: 3.0128\n",
            "Epoch 11: val_mean_absolute_error did not improve from 3.78011\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 2.4397 - mean_absolute_error: 3.0206 - val_loss: 3.3148 - val_mean_absolute_error: 3.8771\n",
            "Epoch 12/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.3931 - mean_absolute_error: 2.9752\n",
            "Epoch 12: val_mean_absolute_error improved from 3.78011 to 3.66901, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5\n",
            "47752/47752 [==============================] - 12s 252us/sample - loss: 2.3931 - mean_absolute_error: 2.9752 - val_loss: 3.1269 - val_mean_absolute_error: 3.6690\n",
            "Epoch 13/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.2953 - mean_absolute_error: 2.8758\n",
            "Epoch 13: val_mean_absolute_error improved from 3.66901 to 3.57697, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5\n",
            "47752/47752 [==============================] - 12s 253us/sample - loss: 2.2964 - mean_absolute_error: 2.8768 - val_loss: 3.0274 - val_mean_absolute_error: 3.5770\n",
            "Epoch 14/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 2.2073 - mean_absolute_error: 2.7900\n",
            "Epoch 14: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 241us/sample - loss: 2.2066 - mean_absolute_error: 2.7893 - val_loss: 3.1606 - val_mean_absolute_error: 3.7177\n",
            "Epoch 15/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0745 - mean_absolute_error: 2.6515\n",
            "Epoch 15: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 227us/sample - loss: 2.0745 - mean_absolute_error: 2.6514 - val_loss: 3.4106 - val_mean_absolute_error: 3.9696\n",
            "Epoch 16/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0926 - mean_absolute_error: 2.6712\n",
            "Epoch 16: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 221us/sample - loss: 2.0932 - mean_absolute_error: 2.6719 - val_loss: 3.3982 - val_mean_absolute_error: 3.9686\n",
            "Epoch 17/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0799 - mean_absolute_error: 2.6564\n",
            "Epoch 17: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 2.0805 - mean_absolute_error: 2.6570 - val_loss: 3.5415 - val_mean_absolute_error: 4.0962\n",
            "Epoch 18/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9986 - mean_absolute_error: 2.5770\n",
            "Epoch 18: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.9991 - mean_absolute_error: 2.5775 - val_loss: 3.1578 - val_mean_absolute_error: 3.7069\n",
            "Epoch 19/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9907 - mean_absolute_error: 2.5684\n",
            "Epoch 19: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 241us/sample - loss: 1.9915 - mean_absolute_error: 2.5692 - val_loss: 3.6696 - val_mean_absolute_error: 4.2524\n",
            "Epoch 20/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.8723 - mean_absolute_error: 2.4437\n",
            "Epoch 20: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.8743 - mean_absolute_error: 2.4460 - val_loss: 3.0904 - val_mean_absolute_error: 3.6416\n",
            "Epoch 21/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9142 - mean_absolute_error: 2.4899\n",
            "Epoch 21: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 237us/sample - loss: 1.9148 - mean_absolute_error: 2.4905 - val_loss: 3.6277 - val_mean_absolute_error: 4.1968\n",
            "Epoch 22/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.8174 - mean_absolute_error: 2.3892\n",
            "Epoch 22: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 10s 218us/sample - loss: 1.8220 - mean_absolute_error: 2.3942 - val_loss: 3.2136 - val_mean_absolute_error: 3.7809\n",
            "Epoch 23/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.7967 - mean_absolute_error: 2.3689\n",
            "Epoch 23: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.7984 - mean_absolute_error: 2.3707 - val_loss: 3.1471 - val_mean_absolute_error: 3.6997\n",
            "Epoch 24/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0315 - mean_absolute_error: 2.6069\n",
            "Epoch 24: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 2.0320 - mean_absolute_error: 2.6074 - val_loss: 3.3587 - val_mean_absolute_error: 3.9190\n",
            "Epoch 25/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7704 - mean_absolute_error: 2.3410\n",
            "Epoch 25: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.7705 - mean_absolute_error: 2.3412 - val_loss: 3.1179 - val_mean_absolute_error: 3.6588\n",
            "Epoch 26/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7832 - mean_absolute_error: 2.3597\n",
            "Epoch 26: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.7832 - mean_absolute_error: 2.3597 - val_loss: 3.4000 - val_mean_absolute_error: 3.9682\n",
            "Epoch 27/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6954 - mean_absolute_error: 2.2670\n",
            "Epoch 27: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 241us/sample - loss: 1.6954 - mean_absolute_error: 2.2670 - val_loss: 3.1573 - val_mean_absolute_error: 3.7017\n",
            "Epoch 28/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6329 - mean_absolute_error: 2.1995\n",
            "Epoch 28: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 231us/sample - loss: 1.6332 - mean_absolute_error: 2.1998 - val_loss: 3.2503 - val_mean_absolute_error: 3.8003\n",
            "Epoch 29/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7085 - mean_absolute_error: 2.2809\n",
            "Epoch 29: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 10s 218us/sample - loss: 1.7085 - mean_absolute_error: 2.2809 - val_loss: 3.1544 - val_mean_absolute_error: 3.6983\n",
            "Epoch 30/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6814 - mean_absolute_error: 2.2525\n",
            "Epoch 30: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.6814 - mean_absolute_error: 2.2525 - val_loss: 3.4261 - val_mean_absolute_error: 3.9795\n",
            "Epoch 31/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.3534 - mean_absolute_error: 2.9303\n",
            "Epoch 31: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 2.3541 - mean_absolute_error: 2.9310 - val_loss: 3.2666 - val_mean_absolute_error: 3.8148\n",
            "Epoch 32/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9204 - mean_absolute_error: 2.4922\n",
            "Epoch 32: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.9206 - mean_absolute_error: 2.4924 - val_loss: 3.4797 - val_mean_absolute_error: 4.0424\n",
            "Epoch 33/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6479 - mean_absolute_error: 2.2125\n",
            "Epoch 33: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.6488 - mean_absolute_error: 2.2134 - val_loss: 3.6559 - val_mean_absolute_error: 4.2420\n",
            "Epoch 34/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6876 - mean_absolute_error: 2.2575\n",
            "Epoch 34: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 242us/sample - loss: 1.6876 - mean_absolute_error: 2.2575 - val_loss: 3.1471 - val_mean_absolute_error: 3.6946\n",
            "Epoch 35/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5966 - mean_absolute_error: 2.1638\n",
            "Epoch 35: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 10s 218us/sample - loss: 1.5987 - mean_absolute_error: 2.1660 - val_loss: 3.3803 - val_mean_absolute_error: 3.9284\n",
            "Epoch 36/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6454 - mean_absolute_error: 2.2182\n",
            "Epoch 36: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 230us/sample - loss: 1.6461 - mean_absolute_error: 2.2188 - val_loss: 3.3190 - val_mean_absolute_error: 3.8905\n",
            "Epoch 37/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6529 - mean_absolute_error: 2.2271\n",
            "Epoch 37: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 1.6529 - mean_absolute_error: 2.2271 - val_loss: 3.3722 - val_mean_absolute_error: 3.9196\n",
            "Epoch 38/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6482 - mean_absolute_error: 2.2156\n",
            "Epoch 38: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 244us/sample - loss: 1.6491 - mean_absolute_error: 2.2165 - val_loss: 3.6457 - val_mean_absolute_error: 4.2217\n",
            "Epoch 39/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6528 - mean_absolute_error: 2.2200\n",
            "Epoch 39: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.6528 - mean_absolute_error: 2.2200 - val_loss: 3.3558 - val_mean_absolute_error: 3.9267\n",
            "Epoch 40/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7319 - mean_absolute_error: 2.3061\n",
            "Epoch 40: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 244us/sample - loss: 1.7319 - mean_absolute_error: 2.3061 - val_loss: 3.2132 - val_mean_absolute_error: 3.7567\n",
            "Epoch 41/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5447 - mean_absolute_error: 2.1123\n",
            "Epoch 41: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 243us/sample - loss: 1.5456 - mean_absolute_error: 2.1132 - val_loss: 3.4326 - val_mean_absolute_error: 3.9908\n",
            "Epoch 42/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5598 - mean_absolute_error: 2.1288\n",
            "Epoch 42: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 246us/sample - loss: 1.5611 - mean_absolute_error: 2.1301 - val_loss: 3.1837 - val_mean_absolute_error: 3.7352\n",
            "Epoch 43/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6364 - mean_absolute_error: 2.2045\n",
            "Epoch 43: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 227us/sample - loss: 1.6364 - mean_absolute_error: 2.2045 - val_loss: 3.3694 - val_mean_absolute_error: 3.9168\n",
            "Epoch 44/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.6033 - mean_absolute_error: 2.1739\n",
            "Epoch 44: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.6026 - mean_absolute_error: 2.1729 - val_loss: 3.4073 - val_mean_absolute_error: 3.9757\n",
            "Epoch 45/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.7412 - mean_absolute_error: 2.3172\n",
            "Epoch 45: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 1.7391 - mean_absolute_error: 2.3150 - val_loss: 3.8647 - val_mean_absolute_error: 4.4591\n",
            "Epoch 46/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.7385 - mean_absolute_error: 2.3101\n",
            "Epoch 46: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 1.7460 - mean_absolute_error: 2.3179 - val_loss: 3.3386 - val_mean_absolute_error: 3.8913\n",
            "Epoch 47/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5132 - mean_absolute_error: 2.0775\n",
            "Epoch 47: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 242us/sample - loss: 1.5139 - mean_absolute_error: 2.0782 - val_loss: 3.5258 - val_mean_absolute_error: 4.0844\n",
            "Epoch 48/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5476 - mean_absolute_error: 2.1135\n",
            "Epoch 48: val_mean_absolute_error did not improve from 3.57697\n",
            "47752/47752 [==============================] - 12s 245us/sample - loss: 1.5476 - mean_absolute_error: 2.1135 - val_loss: 3.2652 - val_mean_absolute_error: 3.8187\n",
            "Epoch 48: early stopping\n",
            "2023-05-26 12:56:55.216078: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_44/BiasAdd' id:32924 op device:{requested: '', assigned: ''} def:{{{node dense_44/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_44/MatMul, dense_44/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507, 14: 2.903993154764576, 1: 4.195189173427488, 2: 3.5173868851195462, 9: 9.428751184134068}\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_107 (Conv2D)         (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_165 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_165 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_108 (Conv2D)         (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_166 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_166 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_33 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_109 (Conv2D)         (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_45 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_167 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_167 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_110 (Conv2D)         (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_168 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_168 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_111 (Conv2D)         (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_169 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_169 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_34 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_112 (Conv2D)         (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_46 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_170 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_170 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_113 (Conv2D)         (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_171 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_171 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_114 (Conv2D)         (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_172 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_172 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_35 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_115 (Conv2D)         (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_47 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_173 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_173 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_174 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_174 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_175 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_175 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(47752, 4, 256)\n",
            "(12979, 4, 256)\n",
            "(3966, 4, 256)\n",
            "Train on 47752 samples, validate on 12979 samples\n",
            "2023-05-26 12:57:03.715278: W tensorflow/c/c_api.cc:300] Operation '{name:'training_26/Adam/batch_normalization_169/gamma/m/Assign' id:35848 op device:{requested: '', assigned: ''} def:{{{node training_26/Adam/batch_normalization_169/gamma/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_26/Adam/batch_normalization_169/gamma/m, training_26/Adam/batch_normalization_169/gamma/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 84.8667 - mean_absolute_error: 85.55982023-05-26 12:57:20.346497: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_13/mul' id:35235 op device:{requested: '', assigned: ''} def:{{{node loss_13/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_13/mul/x, loss_13/dense_47_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 61.56057, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 35s 739us/sample - loss: 84.8667 - mean_absolute_error: 85.5598 - val_loss: 60.8674 - val_mean_absolute_error: 61.5606\n",
            "Epoch 2/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 47.3017 - mean_absolute_error: 47.9937\n",
            "Epoch 2: val_mean_absolute_error improved from 61.56057 to 53.32269, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 12s 253us/sample - loss: 47.2966 - mean_absolute_error: 47.9886 - val_loss: 52.6299 - val_mean_absolute_error: 53.3227\n",
            "Epoch 3/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 6.2356 - mean_absolute_error: 6.8532\n",
            "Epoch 3: val_mean_absolute_error improved from 53.32269 to 6.15436, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 12s 251us/sample - loss: 6.2356 - mean_absolute_error: 6.8532 - val_loss: 5.5416 - val_mean_absolute_error: 6.1544\n",
            "Epoch 4/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 3.7619 - mean_absolute_error: 4.3532\n",
            "Epoch 4: val_mean_absolute_error improved from 6.15436 to 5.73026, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 12s 253us/sample - loss: 3.7625 - mean_absolute_error: 4.3538 - val_loss: 5.1383 - val_mean_absolute_error: 5.7303\n",
            "Epoch 5/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 3.3701 - mean_absolute_error: 3.9564\n",
            "Epoch 5: val_mean_absolute_error improved from 5.73026 to 5.55177, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 12s 250us/sample - loss: 3.3701 - mean_absolute_error: 3.9564 - val_loss: 4.9602 - val_mean_absolute_error: 5.5518\n",
            "Epoch 6/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 3.1504 - mean_absolute_error: 3.7337\n",
            "Epoch 6: val_mean_absolute_error did not improve from 5.55177\n",
            "47752/47752 [==============================] - 12s 242us/sample - loss: 3.1508 - mean_absolute_error: 3.7341 - val_loss: 4.9823 - val_mean_absolute_error: 5.5916\n",
            "Epoch 7/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.9517 - mean_absolute_error: 3.5310\n",
            "Epoch 7: val_mean_absolute_error did not improve from 5.55177\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 2.9522 - mean_absolute_error: 3.5315 - val_loss: 5.0048 - val_mean_absolute_error: 5.5925\n",
            "Epoch 8/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.7717 - mean_absolute_error: 3.3518\n",
            "Epoch 8: val_mean_absolute_error improved from 5.55177 to 5.41410, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 11s 239us/sample - loss: 2.7717 - mean_absolute_error: 3.3519 - val_loss: 4.8234 - val_mean_absolute_error: 5.4141\n",
            "Epoch 9/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.7559 - mean_absolute_error: 3.3360\n",
            "Epoch 9: val_mean_absolute_error improved from 5.41410 to 5.26542, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 11s 224us/sample - loss: 2.7559 - mean_absolute_error: 3.3360 - val_loss: 4.6711 - val_mean_absolute_error: 5.2654\n",
            "Epoch 10/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.5815 - mean_absolute_error: 3.1574\n",
            "Epoch 10: val_mean_absolute_error did not improve from 5.26542\n",
            "47752/47752 [==============================] - 11s 236us/sample - loss: 2.5815 - mean_absolute_error: 3.1574 - val_loss: 5.4208 - val_mean_absolute_error: 6.0138\n",
            "Epoch 11/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.4141 - mean_absolute_error: 2.9886\n",
            "Epoch 11: val_mean_absolute_error did not improve from 5.26542\n",
            "47752/47752 [==============================] - 12s 245us/sample - loss: 2.4141 - mean_absolute_error: 2.9886 - val_loss: 5.6592 - val_mean_absolute_error: 6.2664\n",
            "Epoch 12/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.3760 - mean_absolute_error: 2.9525\n",
            "Epoch 12: val_mean_absolute_error did not improve from 5.26542\n",
            "47752/47752 [==============================] - 12s 252us/sample - loss: 2.3760 - mean_absolute_error: 2.9525 - val_loss: 4.9675 - val_mean_absolute_error: 5.5500\n",
            "Epoch 13/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 2.5362 - mean_absolute_error: 3.1092\n",
            "Epoch 13: val_mean_absolute_error improved from 5.26542 to 5.20803, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 12s 251us/sample - loss: 2.5366 - mean_absolute_error: 3.1096 - val_loss: 4.6342 - val_mean_absolute_error: 5.2080\n",
            "Epoch 14/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.2260 - mean_absolute_error: 2.7986\n",
            "Epoch 14: val_mean_absolute_error did not improve from 5.20803\n",
            "47752/47752 [==============================] - 12s 247us/sample - loss: 2.2260 - mean_absolute_error: 2.7986 - val_loss: 4.7891 - val_mean_absolute_error: 5.3811\n",
            "Epoch 15/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.1080 - mean_absolute_error: 2.6796\n",
            "Epoch 15: val_mean_absolute_error improved from 5.20803 to 5.20737, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5\n",
            "47752/47752 [==============================] - 12s 249us/sample - loss: 2.1080 - mean_absolute_error: 2.6796 - val_loss: 4.6329 - val_mean_absolute_error: 5.2074\n",
            "Epoch 16/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.1529 - mean_absolute_error: 2.7299\n",
            "Epoch 16: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 11s 226us/sample - loss: 2.1529 - mean_absolute_error: 2.7299 - val_loss: 4.6919 - val_mean_absolute_error: 5.2625\n",
            "Epoch 17/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 2.0337 - mean_absolute_error: 2.6063\n",
            "Epoch 17: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 11s 229us/sample - loss: 2.0337 - mean_absolute_error: 2.6063 - val_loss: 4.6886 - val_mean_absolute_error: 5.2641\n",
            "Epoch 18/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.9774 - mean_absolute_error: 2.5499\n",
            "Epoch 18: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.9773 - mean_absolute_error: 2.5499 - val_loss: 5.0691 - val_mean_absolute_error: 5.6741\n",
            "Epoch 19/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 2.0492 - mean_absolute_error: 2.6256\n",
            "Epoch 19: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 11s 238us/sample - loss: 2.0497 - mean_absolute_error: 2.6261 - val_loss: 5.1368 - val_mean_absolute_error: 5.7396\n",
            "Epoch 20/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8663 - mean_absolute_error: 2.4338\n",
            "Epoch 20: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 11s 240us/sample - loss: 1.8663 - mean_absolute_error: 2.4338 - val_loss: 4.8965 - val_mean_absolute_error: 5.4756\n",
            "Epoch 21/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8948 - mean_absolute_error: 2.4678\n",
            "Epoch 21: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 242us/sample - loss: 1.8948 - mean_absolute_error: 2.4678 - val_loss: 4.9207 - val_mean_absolute_error: 5.5082\n",
            "Epoch 22/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.9055 - mean_absolute_error: 2.4788\n",
            "Epoch 22: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 250us/sample - loss: 1.9041 - mean_absolute_error: 2.4772 - val_loss: 5.3722 - val_mean_absolute_error: 5.9677\n",
            "Epoch 23/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8732 - mean_absolute_error: 2.4468\n",
            "Epoch 23: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 246us/sample - loss: 1.8732 - mean_absolute_error: 2.4468 - val_loss: 4.9346 - val_mean_absolute_error: 5.5192\n",
            "Epoch 24/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.8083 - mean_absolute_error: 2.3771\n",
            "Epoch 24: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 11s 234us/sample - loss: 1.8083 - mean_absolute_error: 2.3771 - val_loss: 5.0708 - val_mean_absolute_error: 5.6452\n",
            "Epoch 25/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.7141 - mean_absolute_error: 2.2801\n",
            "Epoch 25: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 252us/sample - loss: 1.7142 - mean_absolute_error: 2.2802 - val_loss: 4.9135 - val_mean_absolute_error: 5.5000\n",
            "Epoch 26/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7754 - mean_absolute_error: 2.3467\n",
            "Epoch 26: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 270us/sample - loss: 1.7760 - mean_absolute_error: 2.3474 - val_loss: 5.3197 - val_mean_absolute_error: 5.9261\n",
            "Epoch 27/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7854 - mean_absolute_error: 2.3551\n",
            "Epoch 27: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 277us/sample - loss: 1.7854 - mean_absolute_error: 2.3551 - val_loss: 4.6679 - val_mean_absolute_error: 5.2401\n",
            "Epoch 28/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.6467 - mean_absolute_error: 2.2115\n",
            "Epoch 28: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 282us/sample - loss: 1.6467 - mean_absolute_error: 2.2115 - val_loss: 4.8608 - val_mean_absolute_error: 5.4411\n",
            "Epoch 29/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6040 - mean_absolute_error: 2.1674\n",
            "Epoch 29: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 14s 293us/sample - loss: 1.6051 - mean_absolute_error: 2.1684 - val_loss: 5.0037 - val_mean_absolute_error: 5.5967\n",
            "Epoch 30/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.8105 - mean_absolute_error: 2.3806\n",
            "Epoch 30: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 271us/sample - loss: 1.8107 - mean_absolute_error: 2.3809 - val_loss: 4.6903 - val_mean_absolute_error: 5.2812\n",
            "Epoch 31/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7519 - mean_absolute_error: 2.3232\n",
            "Epoch 31: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 271us/sample - loss: 1.7524 - mean_absolute_error: 2.3236 - val_loss: 5.1101 - val_mean_absolute_error: 5.7052\n",
            "Epoch 32/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7031 - mean_absolute_error: 2.2708\n",
            "Epoch 32: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 14s 287us/sample - loss: 1.7031 - mean_absolute_error: 2.2708 - val_loss: 4.7627 - val_mean_absolute_error: 5.3392\n",
            "Epoch 33/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7510 - mean_absolute_error: 2.3220\n",
            "Epoch 33: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 264us/sample - loss: 1.7511 - mean_absolute_error: 2.3221 - val_loss: 4.8576 - val_mean_absolute_error: 5.4337\n",
            "Epoch 34/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7147 - mean_absolute_error: 2.2869\n",
            "Epoch 34: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 261us/sample - loss: 1.7164 - mean_absolute_error: 2.2886 - val_loss: 4.9321 - val_mean_absolute_error: 5.5172\n",
            "Epoch 35/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5995 - mean_absolute_error: 2.1642\n",
            "Epoch 35: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 257us/sample - loss: 1.6004 - mean_absolute_error: 2.1651 - val_loss: 5.2069 - val_mean_absolute_error: 5.7860\n",
            "Epoch 36/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7111 - mean_absolute_error: 2.2789\n",
            "Epoch 36: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 264us/sample - loss: 1.7111 - mean_absolute_error: 2.2789 - val_loss: 4.8639 - val_mean_absolute_error: 5.4376\n",
            "Epoch 37/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.5926 - mean_absolute_error: 2.1598\n",
            "Epoch 37: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 275us/sample - loss: 1.5926 - mean_absolute_error: 2.1598 - val_loss: 5.1605 - val_mean_absolute_error: 5.7454\n",
            "Epoch 38/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7192 - mean_absolute_error: 2.2859\n",
            "Epoch 38: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 14s 283us/sample - loss: 1.7192 - mean_absolute_error: 2.2859 - val_loss: 5.0530 - val_mean_absolute_error: 5.6307\n",
            "Epoch 39/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6407 - mean_absolute_error: 2.2078\n",
            "Epoch 39: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 277us/sample - loss: 1.6417 - mean_absolute_error: 2.2088 - val_loss: 4.7954 - val_mean_absolute_error: 5.3850\n",
            "Epoch 40/500\n",
            "47752/47752 [==============================] - ETA: 0s - loss: 1.7341 - mean_absolute_error: 2.3040\n",
            "Epoch 40: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 263us/sample - loss: 1.7341 - mean_absolute_error: 2.3040 - val_loss: 4.7950 - val_mean_absolute_error: 5.3690\n",
            "Epoch 41/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5766 - mean_absolute_error: 2.1424\n",
            "Epoch 41: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 247us/sample - loss: 1.5769 - mean_absolute_error: 2.1427 - val_loss: 4.9071 - val_mean_absolute_error: 5.4959\n",
            "Epoch 42/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5555 - mean_absolute_error: 2.1192\n",
            "Epoch 42: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 247us/sample - loss: 1.5557 - mean_absolute_error: 2.1194 - val_loss: 4.9384 - val_mean_absolute_error: 5.5180\n",
            "Epoch 43/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5812 - mean_absolute_error: 2.1498\n",
            "Epoch 43: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 12s 256us/sample - loss: 1.5827 - mean_absolute_error: 2.1514 - val_loss: 4.8624 - val_mean_absolute_error: 5.4573\n",
            "Epoch 44/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.7210 - mean_absolute_error: 2.2900\n",
            "Epoch 44: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 264us/sample - loss: 1.7216 - mean_absolute_error: 2.2906 - val_loss: 4.6900 - val_mean_absolute_error: 5.2662\n",
            "Epoch 45/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.6712 - mean_absolute_error: 2.2393\n",
            "Epoch 45: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 267us/sample - loss: 1.6719 - mean_absolute_error: 2.2400 - val_loss: 4.8807 - val_mean_absolute_error: 5.4625\n",
            "Epoch 46/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.4927 - mean_absolute_error: 2.0524\n",
            "Epoch 46: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 281us/sample - loss: 1.4929 - mean_absolute_error: 2.0526 - val_loss: 4.7803 - val_mean_absolute_error: 5.3526\n",
            "Epoch 47/500\n",
            "47616/47752 [============================>.] - ETA: 0s - loss: 1.4860 - mean_absolute_error: 2.0474\n",
            "Epoch 47: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 276us/sample - loss: 1.4852 - mean_absolute_error: 2.0464 - val_loss: 5.1717 - val_mean_absolute_error: 5.7522\n",
            "Epoch 48/500\n",
            "47744/47752 [============================>.] - ETA: 0s - loss: 1.5318 - mean_absolute_error: 2.0993\n",
            "Epoch 48: val_mean_absolute_error did not improve from 5.20737\n",
            "47752/47752 [==============================] - 13s 273us/sample - loss: 1.5323 - mean_absolute_error: 2.0997 - val_loss: 4.8199 - val_mean_absolute_error: 5.4007\n",
            "Epoch 48: early stopping\n",
            "2023-05-26 13:07:17.972334: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_47/BiasAdd' id:35184 op device:{requested: '', assigned: ''} def:{{{node dense_47/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_47/MatMul, dense_47/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507, 14: 2.903993154764576, 1: 4.195189173427488, 2: 3.5173868851195462, 9: 9.428751184134068, 15: 3.3131905275922575}\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_116 (Conv2D)         (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_176 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_176 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_117 (Conv2D)         (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_177 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_177 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_36 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_118 (Conv2D)         (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_48 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_178 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_178 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_119 (Conv2D)         (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_179 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_179 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_120 (Conv2D)         (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_180 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_180 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_37 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_121 (Conv2D)         (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_49 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_181 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_181 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_122 (Conv2D)         (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_182 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_182 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_123 (Conv2D)         (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_183 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_183 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_38 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_124 (Conv2D)         (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_50 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_184 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_184 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_185 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_185 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_186 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_186 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n",
            "(50774, 4, 256)\n",
            "(9886, 4, 256)\n",
            "(4037, 4, 256)\n",
            "Train on 50774 samples, validate on 9886 samples\n",
            "2023-05-26 13:07:28.329835: W tensorflow/c/c_api.cc:300] Operation '{name:'batch_normalization_176/moving_variance/Assign' id:36587 op device:{requested: '', assigned: ''} def:{{{node batch_normalization_176/moving_variance/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](batch_normalization_176/moving_variance, batch_normalization_176/moving_variance/Initializer/ones)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 82.5349 - mean_absolute_error: 83.2280/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n",
            "2023-05-26 13:07:47.448942: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_14/mul' id:37495 op device:{requested: '', assigned: ''} def:{{{node loss_14/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_14/mul/x, loss_14/dense_50_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 67.35680, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg8.h5\n",
            "50774/50774 [==============================] - 44s 867us/sample - loss: 82.5349 - mean_absolute_error: 83.2280 - val_loss: 66.6637 - val_mean_absolute_error: 67.3568\n",
            "Epoch 2/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 40.6601 - mean_absolute_error: 41.3487\n",
            "Epoch 2: val_mean_absolute_error improved from 67.35680 to 12.92626, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg8.h5\n",
            "50774/50774 [==============================] - 15s 286us/sample - loss: 40.6601 - mean_absolute_error: 41.3487 - val_loss: 12.2585 - val_mean_absolute_error: 12.9263\n",
            "Epoch 3/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 5.0285 - mean_absolute_error: 5.6383\n",
            "Epoch 3: val_mean_absolute_error improved from 12.92626 to 5.35498, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg8.h5\n",
            "50774/50774 [==============================] - 15s 290us/sample - loss: 5.0270 - mean_absolute_error: 5.6367 - val_loss: 4.7397 - val_mean_absolute_error: 5.3550\n",
            "Epoch 4/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 3.8841 - mean_absolute_error: 4.4786\n",
            "Epoch 4: val_mean_absolute_error improved from 5.35498 to 2.63472, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg8.h5\n",
            "50774/50774 [==============================] - 14s 276us/sample - loss: 3.8848 - mean_absolute_error: 4.4793 - val_loss: 2.0990 - val_mean_absolute_error: 2.6347\n",
            "Epoch 5/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 3.4663 - mean_absolute_error: 4.0537\n",
            "Epoch 5: val_mean_absolute_error did not improve from 2.63472\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 3.4657 - mean_absolute_error: 4.0530 - val_loss: 2.3605 - val_mean_absolute_error: 2.9264\n",
            "Epoch 6/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 3.2412 - mean_absolute_error: 3.8304\n",
            "Epoch 6: val_mean_absolute_error did not improve from 2.63472\n",
            "50774/50774 [==============================] - 13s 252us/sample - loss: 3.2461 - mean_absolute_error: 3.8354 - val_loss: 2.5782 - val_mean_absolute_error: 3.1357\n",
            "Epoch 7/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.9050 - mean_absolute_error: 3.4865\n",
            "Epoch 7: val_mean_absolute_error did not improve from 2.63472\n",
            "50774/50774 [==============================] - 13s 251us/sample - loss: 2.9050 - mean_absolute_error: 3.4865 - val_loss: 2.1475 - val_mean_absolute_error: 2.6703\n",
            "Epoch 8/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.7578 - mean_absolute_error: 3.3403\n",
            "Epoch 8: val_mean_absolute_error improved from 2.63472 to 2.43196, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg8.h5\n",
            "50774/50774 [==============================] - 13s 259us/sample - loss: 2.7597 - mean_absolute_error: 3.3423 - val_loss: 1.9202 - val_mean_absolute_error: 2.4320\n",
            "Epoch 9/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.6255 - mean_absolute_error: 3.2093\n",
            "Epoch 9: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 2.6278 - mean_absolute_error: 3.2117 - val_loss: 2.1232 - val_mean_absolute_error: 2.6392\n",
            "Epoch 10/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.4580 - mean_absolute_error: 3.0387\n",
            "Epoch 10: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 235us/sample - loss: 2.4580 - mean_absolute_error: 3.0387 - val_loss: 2.2107 - val_mean_absolute_error: 2.7258\n",
            "Epoch 11/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.2881 - mean_absolute_error: 2.8656\n",
            "Epoch 11: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 11s 226us/sample - loss: 2.2881 - mean_absolute_error: 2.8656 - val_loss: 2.2873 - val_mean_absolute_error: 2.8284\n",
            "Epoch 12/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 2.2020 - mean_absolute_error: 2.7787\n",
            "Epoch 12: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 234us/sample - loss: 2.2027 - mean_absolute_error: 2.7795 - val_loss: 2.1458 - val_mean_absolute_error: 2.6776\n",
            "Epoch 13/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 2.2210 - mean_absolute_error: 2.8034\n",
            "Epoch 13: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 239us/sample - loss: 2.2244 - mean_absolute_error: 2.8068 - val_loss: 2.1690 - val_mean_absolute_error: 2.7012\n",
            "Epoch 14/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 2.1102 - mean_absolute_error: 2.6871\n",
            "Epoch 14: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 242us/sample - loss: 2.1135 - mean_absolute_error: 2.6908 - val_loss: 2.1705 - val_mean_absolute_error: 2.6755\n",
            "Epoch 15/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.9878 - mean_absolute_error: 2.5610\n",
            "Epoch 15: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 242us/sample - loss: 1.9877 - mean_absolute_error: 2.5609 - val_loss: 2.2429 - val_mean_absolute_error: 2.7940\n",
            "Epoch 16/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.0073 - mean_absolute_error: 2.5839\n",
            "Epoch 16: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 239us/sample - loss: 2.0067 - mean_absolute_error: 2.5832 - val_loss: 2.1290 - val_mean_absolute_error: 2.6419\n",
            "Epoch 17/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.8995 - mean_absolute_error: 2.4713\n",
            "Epoch 17: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 240us/sample - loss: 1.9014 - mean_absolute_error: 2.4733 - val_loss: 2.0834 - val_mean_absolute_error: 2.6044\n",
            "Epoch 18/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.8211 - mean_absolute_error: 2.3905\n",
            "Epoch 18: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 243us/sample - loss: 1.8211 - mean_absolute_error: 2.3904 - val_loss: 2.2715 - val_mean_absolute_error: 2.7914\n",
            "Epoch 19/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.8523 - mean_absolute_error: 2.4223\n",
            "Epoch 19: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 240us/sample - loss: 1.8511 - mean_absolute_error: 2.4210 - val_loss: 2.1360 - val_mean_absolute_error: 2.6701\n",
            "Epoch 20/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.8023 - mean_absolute_error: 2.3737\n",
            "Epoch 20: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 236us/sample - loss: 1.8017 - mean_absolute_error: 2.3730 - val_loss: 2.0070 - val_mean_absolute_error: 2.5104\n",
            "Epoch 21/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.8255 - mean_absolute_error: 2.4006\n",
            "Epoch 21: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 229us/sample - loss: 1.8245 - mean_absolute_error: 2.3995 - val_loss: 2.2770 - val_mean_absolute_error: 2.7923\n",
            "Epoch 22/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.7655 - mean_absolute_error: 2.3347\n",
            "Epoch 22: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 235us/sample - loss: 1.7655 - mean_absolute_error: 2.3348 - val_loss: 2.1934 - val_mean_absolute_error: 2.7189\n",
            "Epoch 23/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.7495 - mean_absolute_error: 2.3211\n",
            "Epoch 23: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 247us/sample - loss: 1.7464 - mean_absolute_error: 2.3178 - val_loss: 2.0784 - val_mean_absolute_error: 2.6024\n",
            "Epoch 24/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.7114 - mean_absolute_error: 2.2816\n",
            "Epoch 24: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 243us/sample - loss: 1.7178 - mean_absolute_error: 2.2881 - val_loss: 2.2086 - val_mean_absolute_error: 2.7374\n",
            "Epoch 25/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.6651 - mean_absolute_error: 2.2330\n",
            "Epoch 25: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 250us/sample - loss: 1.6651 - mean_absolute_error: 2.2330 - val_loss: 2.3111 - val_mean_absolute_error: 2.8548\n",
            "Epoch 26/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.7480 - mean_absolute_error: 2.3218\n",
            "Epoch 26: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 1.7480 - mean_absolute_error: 2.3219 - val_loss: 2.2062 - val_mean_absolute_error: 2.7297\n",
            "Epoch 27/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5874 - mean_absolute_error: 2.1521\n",
            "Epoch 27: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 252us/sample - loss: 1.5874 - mean_absolute_error: 2.1521 - val_loss: 2.1130 - val_mean_absolute_error: 2.6286\n",
            "Epoch 28/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.6862 - mean_absolute_error: 2.2558\n",
            "Epoch 28: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 252us/sample - loss: 1.6830 - mean_absolute_error: 2.2523 - val_loss: 2.1195 - val_mean_absolute_error: 2.6388\n",
            "Epoch 29/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.6556 - mean_absolute_error: 2.2249\n",
            "Epoch 29: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 249us/sample - loss: 1.6568 - mean_absolute_error: 2.2263 - val_loss: 2.2180 - val_mean_absolute_error: 2.7318\n",
            "Epoch 30/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.6509 - mean_absolute_error: 2.2173\n",
            "Epoch 30: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 252us/sample - loss: 1.6490 - mean_absolute_error: 2.2154 - val_loss: 2.2321 - val_mean_absolute_error: 2.7738\n",
            "Epoch 31/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5102 - mean_absolute_error: 2.0722\n",
            "Epoch 31: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 1.5092 - mean_absolute_error: 2.0710 - val_loss: 2.1969 - val_mean_absolute_error: 2.7269\n",
            "Epoch 32/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5663 - mean_absolute_error: 2.1324\n",
            "Epoch 32: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 1.5692 - mean_absolute_error: 2.1355 - val_loss: 2.0894 - val_mean_absolute_error: 2.6335\n",
            "Epoch 33/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.6135 - mean_absolute_error: 2.1843\n",
            "Epoch 33: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 250us/sample - loss: 1.6124 - mean_absolute_error: 2.1831 - val_loss: 2.1118 - val_mean_absolute_error: 2.6235\n",
            "Epoch 34/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.6013 - mean_absolute_error: 2.1688\n",
            "Epoch 34: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 251us/sample - loss: 1.6055 - mean_absolute_error: 2.1731 - val_loss: 2.0732 - val_mean_absolute_error: 2.5943\n",
            "Epoch 35/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.5915 - mean_absolute_error: 2.1600\n",
            "Epoch 35: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 249us/sample - loss: 1.5921 - mean_absolute_error: 2.1607 - val_loss: 2.1370 - val_mean_absolute_error: 2.6556\n",
            "Epoch 36/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.6099 - mean_absolute_error: 2.1802\n",
            "Epoch 36: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 248us/sample - loss: 1.6126 - mean_absolute_error: 2.1831 - val_loss: 2.1966 - val_mean_absolute_error: 2.7220\n",
            "Epoch 37/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5972 - mean_absolute_error: 2.1656\n",
            "Epoch 37: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 251us/sample - loss: 1.5963 - mean_absolute_error: 2.1646 - val_loss: 2.0494 - val_mean_absolute_error: 2.5693\n",
            "Epoch 38/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5105 - mean_absolute_error: 2.0751\n",
            "Epoch 38: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 236us/sample - loss: 1.5105 - mean_absolute_error: 2.0751 - val_loss: 2.1644 - val_mean_absolute_error: 2.6804\n",
            "Epoch 39/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5466 - mean_absolute_error: 2.1127\n",
            "Epoch 39: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 231us/sample - loss: 1.5457 - mean_absolute_error: 2.1117 - val_loss: 2.1452 - val_mean_absolute_error: 2.6620\n",
            "Epoch 40/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.5610 - mean_absolute_error: 2.1262\n",
            "Epoch 40: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 242us/sample - loss: 1.5581 - mean_absolute_error: 2.1230 - val_loss: 2.1893 - val_mean_absolute_error: 2.7293\n",
            "Epoch 41/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5367 - mean_absolute_error: 2.1021\n",
            "Epoch 41: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 246us/sample - loss: 1.5367 - mean_absolute_error: 2.1021 - val_loss: 2.1425 - val_mean_absolute_error: 2.6612\n",
            "Epoch 42/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.4374 - mean_absolute_error: 1.9955\n",
            "Epoch 42: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 13s 247us/sample - loss: 1.4368 - mean_absolute_error: 1.9950 - val_loss: 2.1341 - val_mean_absolute_error: 2.6495\n",
            "Epoch 43/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4824 - mean_absolute_error: 2.0456\n",
            "Epoch 43: val_mean_absolute_error did not improve from 2.43196\n",
            "50774/50774 [==============================] - 12s 246us/sample - loss: 1.4809 - mean_absolute_error: 2.0439 - val_loss: 2.1155 - val_mean_absolute_error: 2.6266\n",
            "Epoch 43: early stopping\n",
            "/usr/local/lib/python3.10/dist-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2023-05-26 13:17:05.113971: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_50/BiasAdd' id:37444 op device:{requested: '', assigned: ''} def:{{{node dense_50/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_50/MatMul, dense_50/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507, 14: 2.903993154764576, 1: 4.195189173427488, 2: 3.5173868851195462, 9: 9.428751184134068, 15: 3.3131905275922575, 8: 9.092191611656256}\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_125 (Conv2D)         (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_187 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_187 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_126 (Conv2D)         (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_188 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_188 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_39 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_127 (Conv2D)         (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_51 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_189 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_189 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_128 (Conv2D)         (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_190 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_190 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_129 (Conv2D)         (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_191 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_191 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_40 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_130 (Conv2D)         (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_52 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_192 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_192 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_131 (Conv2D)         (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_193 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_193 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_132 (Conv2D)         (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_194 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_194 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_41 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_133 (Conv2D)         (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_53 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_195 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_195 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_196 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_196 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_197 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_197 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(50774, 4, 256)\n",
            "(8602, 4, 256)\n",
            "(5321, 4, 256)\n",
            "Train on 50774 samples, validate on 8602 samples\n",
            "2023-05-26 13:17:15.063627: W tensorflow/c/c_api.cc:300] Operation '{name:'training_30/Adam/iter/Assign' id:40245 op device:{requested: '', assigned: ''} def:{{{node training_30/Adam/iter/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_INT64, validate_shape=false](training_30/Adam/iter, training_30/Adam/iter/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 82.4405 - mean_absolute_error: 83.13362023-05-26 13:17:32.709049: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_15/mul' id:39755 op device:{requested: '', assigned: ''} def:{{{node loss_15/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_15/mul/x, loss_15/dense_53_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 61.06824, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 40s 796us/sample - loss: 82.4405 - mean_absolute_error: 83.1336 - val_loss: 60.3751 - val_mean_absolute_error: 61.0682\n",
            "Epoch 2/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 40.6586 - mean_absolute_error: 41.3472\n",
            "Epoch 2: val_mean_absolute_error improved from 61.06824 to 31.02477, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 261us/sample - loss: 40.5327 - mean_absolute_error: 41.2211 - val_loss: 30.3363 - val_mean_absolute_error: 31.0248\n",
            "Epoch 3/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 5.2037 - mean_absolute_error: 5.8166\n",
            "Epoch 3: val_mean_absolute_error improved from 31.02477 to 5.79465, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 261us/sample - loss: 5.2044 - mean_absolute_error: 5.8172 - val_loss: 5.2033 - val_mean_absolute_error: 5.7947\n",
            "Epoch 4/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 3.9090 - mean_absolute_error: 4.5019\n",
            "Epoch 4: val_mean_absolute_error improved from 5.79465 to 5.11664, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 3.9090 - mean_absolute_error: 4.5019 - val_loss: 4.5363 - val_mean_absolute_error: 5.1166\n",
            "Epoch 5/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 3.5309 - mean_absolute_error: 4.1185\n",
            "Epoch 5: val_mean_absolute_error improved from 5.11664 to 4.90835, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 3.5309 - mean_absolute_error: 4.1185 - val_loss: 4.3145 - val_mean_absolute_error: 4.9084\n",
            "Epoch 6/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 3.2969 - mean_absolute_error: 3.8810\n",
            "Epoch 6: val_mean_absolute_error did not improve from 4.90835\n",
            "50774/50774 [==============================] - 13s 248us/sample - loss: 3.2959 - mean_absolute_error: 3.8798 - val_loss: 4.3440 - val_mean_absolute_error: 4.9144\n",
            "Epoch 7/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 3.0532 - mean_absolute_error: 3.6355\n",
            "Epoch 7: val_mean_absolute_error improved from 4.90835 to 4.62139, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 255us/sample - loss: 3.0512 - mean_absolute_error: 3.6334 - val_loss: 4.0457 - val_mean_absolute_error: 4.6214\n",
            "Epoch 8/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.8550 - mean_absolute_error: 3.4366\n",
            "Epoch 8: val_mean_absolute_error did not improve from 4.62139\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 2.8550 - mean_absolute_error: 3.4366 - val_loss: 4.2365 - val_mean_absolute_error: 4.8098\n",
            "Epoch 9/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.7580 - mean_absolute_error: 3.3382\n",
            "Epoch 9: val_mean_absolute_error did not improve from 4.62139\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 2.7585 - mean_absolute_error: 3.3387 - val_loss: 4.7108 - val_mean_absolute_error: 5.2753\n",
            "Epoch 10/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.6365 - mean_absolute_error: 3.2161\n",
            "Epoch 10: val_mean_absolute_error did not improve from 4.62139\n",
            "50774/50774 [==============================] - 13s 256us/sample - loss: 2.6383 - mean_absolute_error: 3.2180 - val_loss: 4.7375 - val_mean_absolute_error: 5.3237\n",
            "Epoch 11/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.5164 - mean_absolute_error: 3.0967\n",
            "Epoch 11: val_mean_absolute_error improved from 4.62139 to 4.57542, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 2.5157 - mean_absolute_error: 3.0960 - val_loss: 3.9861 - val_mean_absolute_error: 4.5754\n",
            "Epoch 12/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.3507 - mean_absolute_error: 2.9279\n",
            "Epoch 12: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 249us/sample - loss: 2.3511 - mean_absolute_error: 2.9282 - val_loss: 4.4924 - val_mean_absolute_error: 5.0751\n",
            "Epoch 13/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.3595 - mean_absolute_error: 2.9397\n",
            "Epoch 13: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 248us/sample - loss: 2.3595 - mean_absolute_error: 2.9397 - val_loss: 4.4331 - val_mean_absolute_error: 5.0021\n",
            "Epoch 14/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.2771 - mean_absolute_error: 2.8556\n",
            "Epoch 14: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 253us/sample - loss: 2.2771 - mean_absolute_error: 2.8556 - val_loss: 4.2386 - val_mean_absolute_error: 4.8268\n",
            "Epoch 15/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 2.1048 - mean_absolute_error: 2.6775\n",
            "Epoch 15: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 2.1044 - mean_absolute_error: 2.6770 - val_loss: 4.3955 - val_mean_absolute_error: 4.9605\n",
            "Epoch 16/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.1075 - mean_absolute_error: 2.6810\n",
            "Epoch 16: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 2.1075 - mean_absolute_error: 2.6810 - val_loss: 4.5800 - val_mean_absolute_error: 5.1505\n",
            "Epoch 17/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 2.0144 - mean_absolute_error: 2.5872\n",
            "Epoch 17: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 259us/sample - loss: 2.0144 - mean_absolute_error: 2.5872 - val_loss: 4.7560 - val_mean_absolute_error: 5.3220\n",
            "Epoch 18/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.9837 - mean_absolute_error: 2.5584\n",
            "Epoch 18: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 250us/sample - loss: 1.9837 - mean_absolute_error: 2.5584 - val_loss: 4.1817 - val_mean_absolute_error: 4.7535\n",
            "Epoch 19/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.9525 - mean_absolute_error: 2.5283\n",
            "Epoch 19: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 253us/sample - loss: 1.9525 - mean_absolute_error: 2.5283 - val_loss: 4.2568 - val_mean_absolute_error: 4.8207\n",
            "Epoch 20/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.9128 - mean_absolute_error: 2.4855\n",
            "Epoch 20: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 255us/sample - loss: 1.9127 - mean_absolute_error: 2.4855 - val_loss: 4.6566 - val_mean_absolute_error: 5.2274\n",
            "Epoch 21/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.8679 - mean_absolute_error: 2.4400\n",
            "Epoch 21: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 250us/sample - loss: 1.8679 - mean_absolute_error: 2.4400 - val_loss: 4.7129 - val_mean_absolute_error: 5.3025\n",
            "Epoch 22/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.8655 - mean_absolute_error: 2.4393\n",
            "Epoch 22: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 12s 242us/sample - loss: 1.8655 - mean_absolute_error: 2.4393 - val_loss: 4.4391 - val_mean_absolute_error: 5.0070\n",
            "Epoch 23/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.7772 - mean_absolute_error: 2.3480\n",
            "Epoch 23: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 12s 228us/sample - loss: 1.7766 - mean_absolute_error: 2.3474 - val_loss: 4.0229 - val_mean_absolute_error: 4.5868\n",
            "Epoch 24/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.7092 - mean_absolute_error: 2.2746\n",
            "Epoch 24: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 11s 225us/sample - loss: 1.7092 - mean_absolute_error: 2.2746 - val_loss: 4.3226 - val_mean_absolute_error: 4.8869\n",
            "Epoch 25/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.7700 - mean_absolute_error: 2.3396\n",
            "Epoch 25: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 12s 244us/sample - loss: 1.7694 - mean_absolute_error: 2.3391 - val_loss: 4.3506 - val_mean_absolute_error: 4.9252\n",
            "Epoch 26/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.7981 - mean_absolute_error: 2.3724\n",
            "Epoch 26: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 1.7990 - mean_absolute_error: 2.3735 - val_loss: 4.6865 - val_mean_absolute_error: 5.2614\n",
            "Epoch 27/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.6787 - mean_absolute_error: 2.2450\n",
            "Epoch 27: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 12s 246us/sample - loss: 1.6787 - mean_absolute_error: 2.2450 - val_loss: 4.5053 - val_mean_absolute_error: 5.0622\n",
            "Epoch 28/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.7488 - mean_absolute_error: 2.3199\n",
            "Epoch 28: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 12s 244us/sample - loss: 1.7490 - mean_absolute_error: 2.3202 - val_loss: 4.2409 - val_mean_absolute_error: 4.8381\n",
            "Epoch 29/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.8046 - mean_absolute_error: 2.3771\n",
            "Epoch 29: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 1.8020 - mean_absolute_error: 2.3744 - val_loss: 4.3862 - val_mean_absolute_error: 4.9494\n",
            "Epoch 30/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.6971 - mean_absolute_error: 2.2649\n",
            "Epoch 30: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 14s 268us/sample - loss: 1.6972 - mean_absolute_error: 2.2651 - val_loss: 4.3214 - val_mean_absolute_error: 4.8932\n",
            "Epoch 31/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5571 - mean_absolute_error: 2.1161\n",
            "Epoch 31: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 249us/sample - loss: 1.5561 - mean_absolute_error: 2.1150 - val_loss: 4.1002 - val_mean_absolute_error: 4.6591\n",
            "Epoch 32/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.6683 - mean_absolute_error: 2.2402\n",
            "Epoch 32: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 12s 241us/sample - loss: 1.6683 - mean_absolute_error: 2.2402 - val_loss: 4.2916 - val_mean_absolute_error: 4.8614\n",
            "Epoch 33/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.6306 - mean_absolute_error: 2.1995\n",
            "Epoch 33: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 12s 244us/sample - loss: 1.6304 - mean_absolute_error: 2.1993 - val_loss: 4.0522 - val_mean_absolute_error: 4.6239\n",
            "Epoch 34/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5764 - mean_absolute_error: 2.1411\n",
            "Epoch 34: val_mean_absolute_error did not improve from 4.57542\n",
            "50774/50774 [==============================] - 13s 250us/sample - loss: 1.5773 - mean_absolute_error: 2.1420 - val_loss: 4.4730 - val_mean_absolute_error: 5.0425\n",
            "Epoch 35/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.6220 - mean_absolute_error: 2.1897\n",
            "Epoch 35: val_mean_absolute_error improved from 4.57542 to 4.41239, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 1.6220 - mean_absolute_error: 2.1897 - val_loss: 3.8504 - val_mean_absolute_error: 4.4124\n",
            "Epoch 36/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5904 - mean_absolute_error: 2.1580\n",
            "Epoch 36: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 13s 253us/sample - loss: 1.5904 - mean_absolute_error: 2.1580 - val_loss: 4.5475 - val_mean_absolute_error: 5.1236\n",
            "Epoch 37/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.6204 - mean_absolute_error: 2.1907\n",
            "Epoch 37: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 1.6283 - mean_absolute_error: 2.1988 - val_loss: 4.1271 - val_mean_absolute_error: 4.6897\n",
            "Epoch 38/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5288 - mean_absolute_error: 2.0924\n",
            "Epoch 38: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 13s 249us/sample - loss: 1.5288 - mean_absolute_error: 2.0924 - val_loss: 4.0340 - val_mean_absolute_error: 4.6073\n",
            "Epoch 39/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5446 - mean_absolute_error: 2.1113\n",
            "Epoch 39: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 1.5446 - mean_absolute_error: 2.1113 - val_loss: 4.3199 - val_mean_absolute_error: 4.8974\n",
            "Epoch 40/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.6262 - mean_absolute_error: 2.1957\n",
            "Epoch 40: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 12s 242us/sample - loss: 1.6262 - mean_absolute_error: 2.1957 - val_loss: 4.2574 - val_mean_absolute_error: 4.8355\n",
            "Epoch 41/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5202 - mean_absolute_error: 2.0834\n",
            "Epoch 41: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 12s 234us/sample - loss: 1.5202 - mean_absolute_error: 2.0834 - val_loss: 4.2495 - val_mean_absolute_error: 4.8194\n",
            "Epoch 42/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5805 - mean_absolute_error: 2.1472\n",
            "Epoch 42: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 12s 228us/sample - loss: 1.5805 - mean_absolute_error: 2.1472 - val_loss: 4.1540 - val_mean_absolute_error: 4.7345\n",
            "Epoch 43/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5791 - mean_absolute_error: 2.1434\n",
            "Epoch 43: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 12s 242us/sample - loss: 1.5791 - mean_absolute_error: 2.1434 - val_loss: 4.0186 - val_mean_absolute_error: 4.5801\n",
            "Epoch 44/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5023 - mean_absolute_error: 2.0630\n",
            "Epoch 44: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 13s 256us/sample - loss: 1.5023 - mean_absolute_error: 2.0630 - val_loss: 4.0052 - val_mean_absolute_error: 4.5872\n",
            "Epoch 45/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5820 - mean_absolute_error: 2.1519\n",
            "Epoch 45: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 13s 251us/sample - loss: 1.5820 - mean_absolute_error: 2.1519 - val_loss: 4.4068 - val_mean_absolute_error: 4.9810\n",
            "Epoch 46/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5273 - mean_absolute_error: 2.0941\n",
            "Epoch 46: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 12s 240us/sample - loss: 1.5264 - mean_absolute_error: 2.0931 - val_loss: 4.0959 - val_mean_absolute_error: 4.6681\n",
            "Epoch 47/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5306 - mean_absolute_error: 2.0981\n",
            "Epoch 47: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 12s 246us/sample - loss: 1.5296 - mean_absolute_error: 2.0971 - val_loss: 4.1120 - val_mean_absolute_error: 4.6783\n",
            "Epoch 48/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5303 - mean_absolute_error: 2.0987\n",
            "Epoch 48: val_mean_absolute_error did not improve from 4.41239\n",
            "50774/50774 [==============================] - 12s 244us/sample - loss: 1.5317 - mean_absolute_error: 2.1002 - val_loss: 4.3503 - val_mean_absolute_error: 4.9157\n",
            "Epoch 49/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.5181 - mean_absolute_error: 2.0845\n",
            "Epoch 49: val_mean_absolute_error improved from 4.41239 to 4.36375, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg10.h5\n",
            "50774/50774 [==============================] - 13s 251us/sample - loss: 1.5176 - mean_absolute_error: 2.0840 - val_loss: 3.7986 - val_mean_absolute_error: 4.3638\n",
            "Epoch 50/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5920 - mean_absolute_error: 2.1623\n",
            "Epoch 50: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 251us/sample - loss: 1.5920 - mean_absolute_error: 2.1623 - val_loss: 4.1576 - val_mean_absolute_error: 4.7193\n",
            "Epoch 51/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4939 - mean_absolute_error: 2.0589\n",
            "Epoch 51: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 12s 246us/sample - loss: 1.4939 - mean_absolute_error: 2.0589 - val_loss: 4.2309 - val_mean_absolute_error: 4.7951\n",
            "Epoch 52/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4861 - mean_absolute_error: 2.0519\n",
            "Epoch 52: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 12s 243us/sample - loss: 1.4861 - mean_absolute_error: 2.0519 - val_loss: 4.0675 - val_mean_absolute_error: 4.6298\n",
            "Epoch 53/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4776 - mean_absolute_error: 2.0375\n",
            "Epoch 53: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 247us/sample - loss: 1.4776 - mean_absolute_error: 2.0375 - val_loss: 4.1476 - val_mean_absolute_error: 4.7144\n",
            "Epoch 54/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4946 - mean_absolute_error: 2.0584\n",
            "Epoch 54: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 260us/sample - loss: 1.4946 - mean_absolute_error: 2.0584 - val_loss: 4.2733 - val_mean_absolute_error: 4.8419\n",
            "Epoch 55/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4634 - mean_absolute_error: 2.0237\n",
            "Epoch 55: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 257us/sample - loss: 1.4645 - mean_absolute_error: 2.0249 - val_loss: 4.2355 - val_mean_absolute_error: 4.8154\n",
            "Epoch 56/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5920 - mean_absolute_error: 2.1649\n",
            "Epoch 56: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 248us/sample - loss: 1.5920 - mean_absolute_error: 2.1649 - val_loss: 4.2413 - val_mean_absolute_error: 4.7983\n",
            "Epoch 57/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5150 - mean_absolute_error: 2.0811\n",
            "Epoch 57: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 1.5150 - mean_absolute_error: 2.0811 - val_loss: 4.4350 - val_mean_absolute_error: 5.0093\n",
            "Epoch 58/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4058 - mean_absolute_error: 1.9646\n",
            "Epoch 58: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 247us/sample - loss: 1.4058 - mean_absolute_error: 1.9646 - val_loss: 4.0208 - val_mean_absolute_error: 4.5782\n",
            "Epoch 59/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4429 - mean_absolute_error: 2.0048\n",
            "Epoch 59: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 12s 246us/sample - loss: 1.4417 - mean_absolute_error: 2.0035 - val_loss: 4.1767 - val_mean_absolute_error: 4.7551\n",
            "Epoch 60/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4346 - mean_absolute_error: 2.0015\n",
            "Epoch 60: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 12s 244us/sample - loss: 1.4359 - mean_absolute_error: 2.0030 - val_loss: 4.0850 - val_mean_absolute_error: 4.6387\n",
            "Epoch 61/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4680 - mean_absolute_error: 2.0327\n",
            "Epoch 61: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 253us/sample - loss: 1.4668 - mean_absolute_error: 2.0315 - val_loss: 4.3797 - val_mean_absolute_error: 4.9426\n",
            "Epoch 62/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4767 - mean_absolute_error: 2.0437\n",
            "Epoch 62: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 256us/sample - loss: 1.4767 - mean_absolute_error: 2.0437 - val_loss: 4.1836 - val_mean_absolute_error: 4.7785\n",
            "Epoch 63/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.5447 - mean_absolute_error: 2.1147\n",
            "Epoch 63: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 1.5447 - mean_absolute_error: 2.1147 - val_loss: 4.2283 - val_mean_absolute_error: 4.7865\n",
            "Epoch 64/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4227 - mean_absolute_error: 1.9811\n",
            "Epoch 64: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 256us/sample - loss: 1.4225 - mean_absolute_error: 1.9809 - val_loss: 3.9284 - val_mean_absolute_error: 4.4941\n",
            "Epoch 65/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.3437 - mean_absolute_error: 1.8984\n",
            "Epoch 65: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 262us/sample - loss: 1.3479 - mean_absolute_error: 1.9029 - val_loss: 4.0626 - val_mean_absolute_error: 4.6329\n",
            "Epoch 66/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4832 - mean_absolute_error: 2.0483\n",
            "Epoch 66: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 256us/sample - loss: 1.4832 - mean_absolute_error: 2.0483 - val_loss: 4.1671 - val_mean_absolute_error: 4.7317\n",
            "Epoch 67/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4427 - mean_absolute_error: 2.0053\n",
            "Epoch 67: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 1.4414 - mean_absolute_error: 2.0039 - val_loss: 4.0544 - val_mean_absolute_error: 4.6179\n",
            "Epoch 68/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.4117 - mean_absolute_error: 1.9775\n",
            "Epoch 68: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 255us/sample - loss: 1.4170 - mean_absolute_error: 1.9828 - val_loss: 3.9855 - val_mean_absolute_error: 4.5415\n",
            "Epoch 69/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.4757 - mean_absolute_error: 2.0403\n",
            "Epoch 69: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 1.4764 - mean_absolute_error: 2.0410 - val_loss: 4.2531 - val_mean_absolute_error: 4.8197\n",
            "Epoch 70/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4154 - mean_absolute_error: 1.9801\n",
            "Epoch 70: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 249us/sample - loss: 1.4142 - mean_absolute_error: 1.9787 - val_loss: 4.5595 - val_mean_absolute_error: 5.1181\n",
            "Epoch 71/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.3531 - mean_absolute_error: 1.9116\n",
            "Epoch 71: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 255us/sample - loss: 1.3531 - mean_absolute_error: 1.9116 - val_loss: 4.2401 - val_mean_absolute_error: 4.8049\n",
            "Epoch 72/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.3898 - mean_absolute_error: 1.9491\n",
            "Epoch 72: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 1.3871 - mean_absolute_error: 1.9460 - val_loss: 3.8864 - val_mean_absolute_error: 4.4731\n",
            "Epoch 73/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4095 - mean_absolute_error: 1.9744\n",
            "Epoch 73: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 264us/sample - loss: 1.4082 - mean_absolute_error: 1.9730 - val_loss: 4.2508 - val_mean_absolute_error: 4.8115\n",
            "Epoch 74/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.4332 - mean_absolute_error: 1.9981\n",
            "Epoch 74: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 14s 267us/sample - loss: 1.4393 - mean_absolute_error: 2.0045 - val_loss: 4.2038 - val_mean_absolute_error: 4.7674\n",
            "Epoch 75/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.3264 - mean_absolute_error: 1.8786\n",
            "Epoch 75: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 263us/sample - loss: 1.3257 - mean_absolute_error: 1.8778 - val_loss: 4.2255 - val_mean_absolute_error: 4.7901\n",
            "Epoch 76/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4035 - mean_absolute_error: 1.9665\n",
            "Epoch 76: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 266us/sample - loss: 1.4035 - mean_absolute_error: 1.9665 - val_loss: 4.1877 - val_mean_absolute_error: 4.7510\n",
            "Epoch 77/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.3246 - mean_absolute_error: 1.8832\n",
            "Epoch 77: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 264us/sample - loss: 1.3310 - mean_absolute_error: 1.8898 - val_loss: 4.2938 - val_mean_absolute_error: 4.8583\n",
            "Epoch 78/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 1.3837 - mean_absolute_error: 1.9429\n",
            "Epoch 78: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 266us/sample - loss: 1.3832 - mean_absolute_error: 1.9425 - val_loss: 4.2885 - val_mean_absolute_error: 4.8472\n",
            "Epoch 79/500\n",
            "50560/50774 [============================>.] - ETA: 0s - loss: 1.3318 - mean_absolute_error: 1.8866\n",
            "Epoch 79: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 14s 269us/sample - loss: 1.3282 - mean_absolute_error: 1.8826 - val_loss: 4.3146 - val_mean_absolute_error: 4.8834\n",
            "Epoch 80/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4389 - mean_absolute_error: 2.0037\n",
            "Epoch 80: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 261us/sample - loss: 1.4389 - mean_absolute_error: 2.0037 - val_loss: 4.3368 - val_mean_absolute_error: 4.9017\n",
            "Epoch 81/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.3333 - mean_absolute_error: 1.8891\n",
            "Epoch 81: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 256us/sample - loss: 1.3333 - mean_absolute_error: 1.8891 - val_loss: 4.7332 - val_mean_absolute_error: 5.3262\n",
            "Epoch 82/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4197 - mean_absolute_error: 1.9823\n",
            "Epoch 82: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 13s 250us/sample - loss: 1.4197 - mean_absolute_error: 1.9823 - val_loss: 4.0004 - val_mean_absolute_error: 4.5692\n",
            "Epoch 83/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.3619 - mean_absolute_error: 1.9190\n",
            "Epoch 83: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 12s 246us/sample - loss: 1.3619 - mean_absolute_error: 1.9190 - val_loss: 4.1510 - val_mean_absolute_error: 4.7140\n",
            "Epoch 84/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 1.4197 - mean_absolute_error: 1.9903\n",
            "Epoch 84: val_mean_absolute_error did not improve from 4.36375\n",
            "50774/50774 [==============================] - 12s 239us/sample - loss: 1.4197 - mean_absolute_error: 1.9903 - val_loss: 3.9458 - val_mean_absolute_error: 4.5233\n",
            "Epoch 84: early stopping\n",
            "2023-05-26 13:35:41.105504: W tensorflow/c/c_api.cc:300] Operation '{name:'dense_53/BiasAdd' id:39704 op device:{requested: '', assigned: ''} def:{{{node dense_53/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_53/MatMul, dense_53/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "{3: 2.441629311456373, 5: 16.573788379616012, 7: 2.257703518178614, 12: 8.531119039162673, 4: 5.687065909414654, 6: 6.437033652325585, 11: 4.819157946574507, 14: 2.903993154764576, 1: 4.195189173427488, 2: 3.5173868851195462, 9: 9.428751184134068, 15: 3.3131905275922575, 8: 9.092191611656256, 10: 3.332788538466232}\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_134 (Conv2D)         (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation_198 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_198 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_135 (Conv2D)         (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_199 (Activation)  (None, 1, 256, 32)       0         \n",
            "                                                                 \n",
            " batch_normalization_199 (Ba  (None, 1, 256, 32)       128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_42 (ZeroPadd  (None, 1, 260, 32)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_136 (Conv2D)         (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d_54 (Avera  (None, 1, 128, 63)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_200 (Activation)  (None, 1, 128, 63)       0         \n",
            "                                                                 \n",
            " batch_normalization_200 (Ba  (None, 1, 128, 63)       252       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_137 (Conv2D)         (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_201 (Activation)  (None, 1, 128, 64)       0         \n",
            "                                                                 \n",
            " batch_normalization_201 (Ba  (None, 1, 128, 64)       256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_138 (Conv2D)         (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_202 (Activation)  (None, 1, 128, 62)       0         \n",
            "                                                                 \n",
            " batch_normalization_202 (Ba  (None, 1, 128, 62)       248       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_43 (ZeroPadd  (None, 1, 132, 62)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_139 (Conv2D)         (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_55 (Avera  (None, 1, 32, 120)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_203 (Activation)  (None, 1, 32, 120)       0         \n",
            "                                                                 \n",
            " batch_normalization_203 (Ba  (None, 1, 32, 120)       480       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_140 (Conv2D)         (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_204 (Activation)  (None, 1, 32, 59)        0         \n",
            "                                                                 \n",
            " batch_normalization_204 (Ba  (None, 1, 32, 59)        236       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_141 (Conv2D)         (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_205 (Activation)  (None, 1, 32, 27)        0         \n",
            "                                                                 \n",
            " batch_normalization_205 (Ba  (None, 1, 32, 27)        108       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " zero_padding2d_44 (ZeroPadd  (None, 1, 37, 27)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_142 (Conv2D)         (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_56 (Avera  (None, 1, 4, 28)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " activation_206 (Activation)  (None, 1, 4, 28)         0         \n",
            "                                                                 \n",
            " batch_normalization_206 (Ba  (None, 1, 4, 28)         112       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 112)               0         \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_207 (Activation)  (None, 38)               0         \n",
            "                                                                 \n",
            " batch_normalization_207 (Ba  (None, 38)               152       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_208 (Activation)  (None, 51)               0         \n",
            "                                                                 \n",
            " batch_normalization_208 (Ba  (None, 51)               204       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "(50774, 4, 256)\n",
            "(9358, 4, 256)\n",
            "(4565, 4, 256)\n",
            "Train on 50774 samples, validate on 9358 samples\n",
            "2023-05-26 13:35:52.300511: W tensorflow/c/c_api.cc:300] Operation '{name:'training_32/Adam/conv2d_136/bias/v/Assign' id:42829 op device:{requested: '', assigned: ''} def:{{{node training_32/Adam/conv2d_136/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_32/Adam/conv2d_136/bias/v, training_32/Adam/conv2d_136/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "Epoch 1/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 82.4729 - mean_absolute_error: 83.16612023-05-26 13:36:10.801040: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_16/mul' id:42015 op device:{requested: '', assigned: ''} def:{{{node loss_16/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_16/mul/x, loss_16/dense_56_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
            "\n",
            "Epoch 1: val_mean_absolute_error improved from inf to 65.02441, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg13.h5\n",
            "50774/50774 [==============================] - 45s 890us/sample - loss: 82.4434 - mean_absolute_error: 83.1365 - val_loss: 64.3313 - val_mean_absolute_error: 65.0244\n",
            "Epoch 2/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 40.4569 - mean_absolute_error: 41.1455\n",
            "Epoch 2: val_mean_absolute_error improved from 65.02441 to 34.92459, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg13.h5\n",
            "50774/50774 [==============================] - 13s 254us/sample - loss: 40.4569 - mean_absolute_error: 41.1455 - val_loss: 34.2336 - val_mean_absolute_error: 34.9246\n",
            "Epoch 3/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 5.0855 - mean_absolute_error: 5.6945\n",
            "Epoch 3: val_mean_absolute_error improved from 34.92459 to 5.75518, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg13.h5\n",
            "50774/50774 [==============================] - 13s 258us/sample - loss: 5.0852 - mean_absolute_error: 5.6943 - val_loss: 5.1525 - val_mean_absolute_error: 5.7552\n",
            "Epoch 4/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 4.0314 - mean_absolute_error: 4.6265\n",
            "Epoch 4: val_mean_absolute_error improved from 5.75518 to 5.30182, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg13.h5\n",
            "50774/50774 [==============================] - 13s 255us/sample - loss: 4.0314 - mean_absolute_error: 4.6265 - val_loss: 4.7189 - val_mean_absolute_error: 5.3018\n",
            "Epoch 5/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 3.7419 - mean_absolute_error: 4.3338\n",
            "Epoch 5: val_mean_absolute_error improved from 5.30182 to 5.21100, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg13.h5\n",
            "50774/50774 [==============================] - 13s 248us/sample - loss: 3.7418 - mean_absolute_error: 4.3337 - val_loss: 4.6295 - val_mean_absolute_error: 5.2110\n",
            "Epoch 6/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 3.3769 - mean_absolute_error: 3.9622\n",
            "Epoch 6: val_mean_absolute_error improved from 5.21100 to 5.16548, saving model to /content/drive/MyDrive/slimmed/saved_models_PIT/test_reg13.h5\n",
            "50774/50774 [==============================] - 12s 239us/sample - loss: 3.3763 - mean_absolute_error: 3.9614 - val_loss: 4.6315 - val_mean_absolute_error: 5.1655\n",
            "Epoch 7/500\n",
            "50774/50774 [==============================] - ETA: 0s - loss: 3.2322 - mean_absolute_error: 3.8165\n",
            "Epoch 7: val_mean_absolute_error did not improve from 5.16548\n",
            "50774/50774 [==============================] - 12s 236us/sample - loss: 3.2322 - mean_absolute_error: 3.8165 - val_loss: 4.6307 - val_mean_absolute_error: 5.1871\n",
            "Epoch 8/500\n",
            "50688/50774 [============================>.] - ETA: 0s - loss: 3.0613 - mean_absolute_error: 3.6449\n",
            "Epoch 8: val_mean_absolute_error did not improve from 5.16548\n",
            "50774/50774 [==============================] - 12s 236us/sample - loss: 3.0611 - mean_absolute_error: 3.6445 - val_loss: 4.7484 - val_mean_absolute_error: 5.3011\n",
            "Epoch 9/500\n",
            "18432/50774 [=========>....................] - ETA: 8s - loss: 2.8320 - mean_absolute_error: 3.4140"
          ]
        }
      ],
      "source": [
        "!python architecture_search/pit_mn.py --root /content/drive/MyDrive/slimmed/ --NAS PIT --learned_ch 32 32 63 64 62 120 59 27 28 38 51 1 --strength 1e-6 --warmup 1 # pour MN lambda =1e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV6QuooAFqYq"
      },
      "source": [
        "# Nouvelle section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "DlDYGyi1FrJC",
        "outputId": "8d3db2a4-01d6-40b6-8746-1f8b4a0ae1cc"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-be060d8d16a6>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdictio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Thiziri/saved_models_PIT/1.0e-04_1_data.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mS7pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'P5_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mS7GT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'P5_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \"\"\"\n\u001b[1;32m    189\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Thiziri/saved_models_PIT/1.0e-04_1_data.pickle'"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "dictio = pd.read_pickle(\"/content/drive/MyDrive/Thiziri/saved_models_PIT/1.0e-04_1_data.pickle\")\n",
        "S7pred = dictio['P5_pred']\n",
        "S7GT = dictio['P5_label']\n",
        "S7act = dictio['P5_activity']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ncRdQBsI1Id",
        "outputId": "4b3ad202-1206-4890-cc1b-ddd818ede2df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4649"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.size(S7pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMK5B0f3JSnT",
        "outputId": "35f4f2a0-6a2c-49d3-dccf-ea1def8adf4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4649,)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "S7pred[:, 0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "45YRJWljITI1",
        "outputId": "b21ed290-390f-4ef9-df5e-0d21adcbb2d2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "r = sns.lineplot(x=np.arange(0, np.size(S7pred)), y=S7GT[:, 0], linewidth=1, legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Azt8w1YO9Xe6"
      },
      "source": [
        "# Nouvelle section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr2oPoUx9Ybm",
        "outputId": "63f69db9-46d4-458b-db67-347192b28efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean-Pre: 5.051735118561203\n",
            "Median-Pre: 4.018176949249696\n",
            "Mean-Post: 4.609760219962286\n",
            "Median-Post: 3.6169149201858244\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "from sklearn.model_selection import LeaveOneGroupOut, GroupKFold\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from scipy import signal\n",
        "\n",
        "def obtain_MAE(dataset, fine=False):\n",
        "    pred = '_pred' if not fine else '_pred_fine'\n",
        "    label = '_label' if not fine else '_label_fine'\n",
        "    MAE = []\n",
        "    for pat in np.arange(1,16):\n",
        "        dataset['P' +str(pat) +pred] = dataset['P' +str(pat) +pred]\n",
        "        dataset['P' +str(pat) +label] = dataset['P' +str(pat) +label]\n",
        "        MAE_pat = np.mean(np.abs(dataset['P' +str(pat) +label]-dataset['P' +str(pat) +pred]))\n",
        "        MAE.append(MAE_pat)\n",
        "    MAE = np.asarray(MAE)\n",
        "    print('Mean-Pre: {}'.format(np.mean(MAE)))\n",
        "    print('Median-Pre: {}'.format(np.median(MAE)))\n",
        "    return MAE\n",
        "\n",
        "def post_processing(dataset, fine=False):\n",
        "    n = 10\n",
        "    f_h = 10\n",
        "    f_l = 10\n",
        "    pred = '_pred' if not fine else '_pred_fine'\n",
        "    label = '_label' if not fine else '_label_fine'\n",
        "    MAE_postprocessing = []\n",
        "    for pat in np.arange(1,16):\n",
        "    \told_value = dataset['P' +str(pat) +pred][0]\n",
        "    \tfor i in np.arange(n,len(dataset['P' +str(pat) +label])):\n",
        "    \t\tif np.mean(dataset['P' +str(pat) +pred][i]) > np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100+f_h)/100.0:\n",
        "    \t\t    dataset['P' +str(pat) +pred][i] = np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100+f_h)/100\n",
        "    \t\tif np.mean(dataset['P' +str(pat) +pred][i]) < np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100-f_l)/100.0:\n",
        "    \t\t    dataset['P' +str(pat) +pred][i] = np.mean(dataset['P' +str(pat) +pred][(i-n):i])*(100-f_l)/100\n",
        "    \tMAE_pat = np.mean(np.abs(dataset['P' +str(pat) +label]-dataset['P' +str(pat) +pred]))\n",
        "    \tMAE_postprocessing.append(MAE_pat)\n",
        "    MAE_postprocessing = np.asarray(MAE_postprocessing)\n",
        "    print('Mean-Post: {}'.format(np.mean(MAE_postprocessing)))\n",
        "    print('Median-Post: {}'.format(np.median(MAE_postprocessing)))\n",
        "    return MAE_postprocessing\n",
        "\n",
        "path = '/content/drive/MyDrive/slimmed/saved_models_PIT/5.0e-07_1_data.pickle'\n",
        "with open(path, 'rb') as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "MAE = obtain_MAE(dataset, fine=False)\n",
        "MAE_post = post_processing(dataset, fine=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "h5"
      ],
      "metadata": {
        "id": "JYAA533nXjvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test"
      ],
      "metadata": {
        "id": "ZRXuRrNeXmpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "# Open the h5 file\n",
        "file = h5py.File('/content/drive/MyDrive/slimmed/saved_models_PIT/test_reg15.h5', 'r')\n",
        "\n",
        "# ['beta:0', 'gamma:0', 'moving_mean:0', 'moving_variance:0']>\n",
        "file['model_weights'].keys()\n",
        "\n",
        "#file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H45AHEfZXpym",
        "outputId": "c8bdeb1c-44ae-4a8f-f24f-9652190829c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KeysViewHDF5 ['activation_165', 'activation_166', 'activation_167', 'activation_168', 'activation_169', 'activation_170', 'activation_171', 'activation_172', 'activation_173', 'activation_174', 'activation_175', 'average_pooling2d_45', 'average_pooling2d_46', 'average_pooling2d_47', 'batch_normalization_165', 'batch_normalization_166', 'batch_normalization_167', 'batch_normalization_168', 'batch_normalization_169', 'batch_normalization_170', 'batch_normalization_171', 'batch_normalization_172', 'batch_normalization_173', 'batch_normalization_174', 'batch_normalization_175', 'conv2d_107', 'conv2d_108', 'conv2d_109', 'conv2d_110', 'conv2d_111', 'conv2d_112', 'conv2d_113', 'conv2d_114', 'conv2d_115', 'dense_45', 'dense_46', 'dense_47', 'flatten_15', 'top_level_model_weights', 'zero_padding2d_33', 'zero_padding2d_34', 'zero_padding2d_35']>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "rfo-BYwmmbPy",
        "outputId": "bdfa5c27-6473-4028-f8af-406058608f3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-c5d84736ba45>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the original group\n",
        "original_group = file['original_group_name']\n",
        "\n",
        "# Create a new group with the desired name\n",
        "new_group = file.create_group('new_group_name')\n",
        "\n",
        "# Copy the contents of the original group to the new group\n",
        "original_group.copy(new_group)\n",
        "\n",
        "# Delete the original group\n",
        "del file['original_group_name']\n",
        "\n",
        "# Close the HDF5 file\n",
        "h5_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "rOL4vmANzfmf",
        "outputId": "65b9908c-90fc-4ee2-80fe-5663d5f59174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2e27b12a9dfb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Access the original group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moriginal_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original_group_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a new group with the desired name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnew_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'new_group_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             raise TypeError(\"Accessing a group is done with bytes or str, \"\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'original_group_name' doesn't exist)\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Sequential, layers\n",
        "from tensorflow.keras import backend\n",
        "import math"
      ],
      "metadata": {
        "id": "q8NLQscmcRsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "width_mult = 1\n",
        "in_shape = 256\n",
        "\n",
        "dil_ht = False\n",
        "dilLIST = [1, 2, 4, 4, 4, 4, 2]\n",
        "#dilLIST = [1, 1, 1, 1, 1, 1, 1]\n",
        "ofMAP = [32, 32, 63, 64, 62, 120, 59, 27, 28, 38, 51, 1]\n",
        "n_ch=4"
      ],
      "metadata": {
        "id": "-HY5aOac2Z1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dil_list = dilLIST\n",
        "ofmap = ofMAP\n",
        "\n",
        "rf = [5, 9, 17]\n",
        "\n",
        "if not dil_list and dil_ht:\n",
        "    dil_list = [\n",
        "                2, 2, 1,\n",
        "                4, 4,\n",
        "                8, 8\n",
        "                ]\n",
        "elif not dil_list:\n",
        "    dil_list = [\n",
        "                1, 1, 1,\n",
        "                1, 1,\n",
        "                1, 1\n",
        "                ]\n",
        "\n",
        "\n",
        "if not ofmap:\n",
        "    ofmap = [\n",
        "            32, 32, 64,\n",
        "            64, 64, 128,\n",
        "            128, 128, 128,\n",
        "            256, 128, 1\n",
        "            ]\n",
        "else:\n",
        "    for idx, i in enumerate(ofmap):\n",
        "        if i == 0:\n",
        "            ofmap[idx] = 1\n",
        "\n",
        "\n",
        "input_channel = width_mult * 32\n",
        "output_channel = input_channel * 2\n",
        "\n",
        "backend.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[0],\n",
        "    kernel_size=(1,math.ceil(rf[0]/dil_list[0])),\n",
        "    padding='same', dilation_rate=(1,dil_list[0]),\n",
        "    input_shape = (1, in_shape, n_ch)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[1],\n",
        "    kernel_size=(1,math.ceil(rf[0]/dil_list[1])),\n",
        "    padding='same', dilation_rate=(1,dil_list[1]),\n",
        "    input_shape = (1, in_shape, 32)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.ZeroPadding2D(padding=((0, 0), (4, 0))))\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[2],\n",
        "    kernel_size=(1,math.ceil(rf[0]/dil_list[2])),\n",
        "    padding='valid', dilation_rate=(1,dil_list[2]),\n",
        "    input_shape = (1, in_shape+4, 32)))\n",
        "model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "input_channel = width_mult * 64\n",
        "output_channel = input_channel*2\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[3],\n",
        "    kernel_size=(1,math.ceil(rf[1]/dil_list[3])),\n",
        "    padding='same', dilation_rate=(1,dil_list[3]),\n",
        "    input_shape = (1, in_shape/2 + 8, 64)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[4],\n",
        "    kernel_size=(1,math.ceil(rf[1]/dil_list[4])),\n",
        "    padding='same', dilation_rate=(1,dil_list[4]),\n",
        "    input_shape = (1, in_shape/2 + 8, 64)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.ZeroPadding2D(padding=((0, 0), (4, 0))))\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[5],\n",
        "    kernel_size=(1,5), padding='valid',\n",
        "    strides=2, input_shape = (1, in_shape/2 + 4, 64)))\n",
        "model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "input_channel = width_mult * 128\n",
        "output_channel = input_channel*2\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[6],\n",
        "    kernel_size=(1,math.ceil(rf[2]/dil_list[5])),\n",
        "    padding='same', dilation_rate=(1,dil_list[5]),\n",
        "    input_shape = (1, in_shape/8 + 16, 128)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[7],\n",
        "    kernel_size=(1,math.ceil(rf[2]/dil_list[6])),\n",
        "    padding='same', dilation_rate=(1,dil_list[6]),\n",
        "    input_shape = (1, in_shape/8 + 16, 128)))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.ZeroPadding2D(padding=((0, 0), (5, 0))))\n",
        "model.add(layers.Conv2D(\n",
        "    filters=ofmap[8],\n",
        "    kernel_size=(1,5), padding='valid',\n",
        "    strides=4, input_shape = (1, in_shape/8 + 5, 128)))\n",
        "model.add(layers.AveragePooling2D(pool_size=(1,2), strides=2, padding='valid'))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(ofmap[9]))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(ofmap[10]))\n",
        "model.add(layers.Activation('relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "\n",
        "model.add(layers.Dense(ofmap[11]))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KygLicIyX68W",
        "outputId": "0eaead92-3222-45ba-a9de-c50c89b9b218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 1, 256, 32)        672       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 1, 256, 32)       128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 1, 256, 32)        3104      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 1, 256, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 1, 256, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " zero_padding2d (ZeroPadding  (None, 1, 260, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 1, 256, 63)        4095      \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 1, 128, 63)       0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 1, 128, 63)        0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 1, 128, 63)       252       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 1, 128, 64)        12160     \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 1, 128, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 1, 128, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 1, 128, 62)        11966     \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 1, 128, 62)        0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 1, 128, 62)       248       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " zero_padding2d_1 (ZeroPaddi  (None, 1, 132, 62)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 1, 64, 120)        37320     \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 1, 32, 120)       0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 1, 32, 120)        0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 1, 32, 120)       480       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 1, 32, 59)         35459     \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 1, 32, 59)         0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 1, 32, 59)        236       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 1, 32, 27)         14364     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 1, 32, 27)         0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 1, 32, 27)        108       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " zero_padding2d_2 (ZeroPaddi  (None, 1, 37, 27)        0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 1, 9, 28)          3808      \n",
            "                                                                 \n",
            " average_pooling2d_2 (Averag  (None, 1, 4, 28)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 1, 4, 28)          0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 1, 4, 28)         112       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 112)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 38)                4294      \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 38)                0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 38)               152       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 51)                1989      \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 51)                0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 51)               204       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 52        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 131,587\n",
            "Trainable params: 130,435\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/content/drive/MyDrive/slimmed/saved_models_PIT/test_reg9.h5')"
      ],
      "metadata": {
        "id": "46iROETIYPSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from preprocessing import preprocessing_Dalia as pp\n",
        "from config import Config\n"
      ],
      "metadata": {
        "id": "9MCycg7gFK_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/slimmed/slimmed_dalia.pkl', 'rb') as f:\n",
        "            data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "            ppg_acc = data['X']\n",
        "            GT = data['y']\n",
        "            activity = data['act']\n",
        "\n",
        "print(\"dimension ppg_acc\",ppg_acc.shape, \"dimesion GT\", GT.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd-uDqeXDS7-",
        "outputId": "c869d839-0282-464c-86b5-d26d63e20b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimension ppg_acc (64697, 4, 256) dimesion GT (64697, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "input1 = np.moveaxis(ppg_acc, 1, 2)\n",
        "input11=input1[0:1, :, :]\n",
        "input_data = np.reshape(input1, (-1, 64697, 256, 4))\n",
        "input_data = np.moveaxis(input_data, 0, 1)\n",
        "input_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZzxX0UJNRp1",
        "outputId": "361268e3-c23f-48e8-dc8b-bdadcbe5499e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64697, 1, 256, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPQAunctMsbq",
        "outputId": "b414769b-ca0f-41a4-ce2d-529e0864f89d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022/2022 [==============================] - 72s 36ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3q-KBQgnobL",
        "outputId": "6769669d-35c5-40d8-e925-a294964952a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[84.05625 ],\n",
              "       [90.39345 ],\n",
              "       [85.88631 ],\n",
              "       ...,\n",
              "       [85.97716 ],\n",
              "       [87.631294],\n",
              "       [90.33165 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vimXIT8Inui1",
        "outputId": "fd1f4a46-ce86-45be-aef7-1bd103613110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[51.78977214],\n",
              "       [53.95587357],\n",
              "       [59.37907894],\n",
              "       ...,\n",
              "       [84.90326124],\n",
              "       [85.25177586],\n",
              "       [85.70933822]])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "S1= 3.18\n",
        "S2= 1.78\n",
        "S3= 1.91\n",
        "S4= 1.61\n",
        "S5= 2.20\n",
        "S6=\n",
        "S7=\n",
        "S8=\n",
        "S9=\n",
        "S10=\n",
        "S11=\n",
        "S12=\n",
        "S13=\n",
        "S14=\n",
        "S15="
      ],
      "metadata": {
        "id": "hBsnYz_SvRJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mean_absolute_error(predictions[4603+4099+4367+4572+4649+2622+4668+4037:4099+4603+4367+4572+4649+2622+4668+4037+4277], GT[4603+4099+4367+4572+4649+2622+4668+4037:4603+4099+4367+4572+4649+2622+4668+4037+4277])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6zVA2f9rBMz",
        "outputId": "69d7e556-c207-4fa5-dedd-a96ac2ab64ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2765108995577843"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "r = sns.lineplot(x=np.arange(0, np.size(S7pred)), y=S7GT[:, 0], linewidth=1, legend=False)"
      ],
      "metadata": {
        "id": "m9I5WrRApy90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot lambdas"
      ],
      "metadata": {
        "id": "xBo9yO85c-Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "MN_params = np.sort([138938, 285387, 15243, 80509])\n",
        "MN_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldlnPe0pgEtH",
        "outputId": "3cfbafd1-f1df-4c4c-aed6-65612e90ea98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 15243,  80509, 138938, 285387])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data for the line plots\n",
        "MN_params = [285387, 138938, 80509, 15243]\n",
        "MN_MAE = [5.088, 5.257, 5.147, 5.757]\n",
        "\n",
        "PIT_params_mn6 = [285387, 95797, 89788]\n",
        "PIT_params_mn5 = [138938, 63170, 54498]\n",
        "PIT_params_mn4 = [80509, 37679, 31717]\n",
        "PIT_params_mn3 = [15243, 13263, 6375]\n",
        "\n",
        "PIT_MAE_mn6 = [5.074, 5.601, 6.248]\n",
        "PIT_MAE_mn5 = [5.228, 5.497, 6.359]\n",
        "PIT_MAE_mn4 = [5.354, 5.568, 6.438]\n",
        "PIT_MAE_mn3 = [5.856, 6.156, 6.453]\n",
        "\n",
        "df_MN = pd.DataFrame({'MN_params': MN_params, 'MN_MAE': MN_MAE})\n",
        "df_MN = df_MN.sort_values('MN_params')\n",
        "\n",
        "df_PIT_mn6 = pd.DataFrame({'PIT_params_mn6': PIT_params_mn6, 'PIT_MAE_mn6': PIT_MAE_mn6})\n",
        "df_PIT_mn6 = df_PIT_mn6.sort_values('PIT_params_mn6')\n",
        "\n",
        "df_PIT_mn5 = pd.DataFrame({'PIT_params_mn5': PIT_params_mn5, 'PIT_MAE_mn5': PIT_MAE_mn5})\n",
        "df_PIT_mn5 = df_PIT_mn5.sort_values('PIT_params_mn5')\n",
        "\n",
        "df_PIT_mn4 = pd.DataFrame({'PIT_params_mn4': PIT_params_mn4, 'PIT_MAE_mn4': PIT_MAE_mn4})\n",
        "df_PIT_mn4 = df_PIT_mn4.sort_values('PIT_params_mn4')\n",
        "\n",
        "df_PIT_mn3 = pd.DataFrame({'PIT_params_mn3': PIT_params_mn3, 'PIT_MAE_mn3': PIT_MAE_mn3})\n",
        "df_PIT_mn3 = df_PIT_mn3.sort_values('PIT_params_mn3')\n",
        "\n",
        "\n",
        "\n",
        "scat_x_1 = [285387, 138938,80509,15243]\n",
        "scat_x_2 = [95797, 63170,37679,13263]\n",
        "scat_x_3 = [89788, 54498,31717,6375]\n",
        "\n",
        "scat_y_1 = [5.074, 5.228,5.354,5.856]\n",
        "scat_y_2 = [5.601,  5.497, 5.568, 6.156]\n",
        "scat_y_3 = [6.248,6.359,6.438,6.453]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set the style\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "# Create the plot\n",
        "\n",
        "markerz = ['square', 'triangle-up', 'triangle-down']\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
        "sns.set(font_scale = 1)\n",
        "with sns.axes_style(\"ticks\"):\n",
        "\n",
        "  #r = sns.scatterplot(x=scat_x_1, y=scat_y_1, color='blue')\n",
        "\n",
        "\n",
        "\n",
        "  # Get the current axes object\n",
        "  ax = plt.gca()\n",
        "\n",
        "  # Iterate over each data point and plot non-filled markers with contour\n",
        "  for xi, yi in zip(scat_x_1[0:3], scat_y_1[0:3]):\n",
        "      ax.plot(xi, yi, marker='D', markersize=6, color='none', markeredgecolor='black')\n",
        "\n",
        "  ax.plot(scat_x_1[3],scat_y_1[3], marker='D', markersize=6, color='none', markeredgecolor='black', label = 'λ(PIT) = 1e-7')\n",
        "\n",
        "  for xi, yi in zip(scat_x_2[0:3], scat_y_2[0:3]):\n",
        "      ax.plot(xi, yi, marker='s', markersize=6, color='none', markeredgecolor='black')\n",
        "\n",
        "  ax.plot(scat_x_2[3],scat_y_2[3], marker='s', markersize=6, color='none', markeredgecolor='black', label = 'λ(PIT) = 1e-5')\n",
        "\n",
        "  for xi, yi in zip(scat_x_3[0:3], scat_y_3[0:3]):\n",
        "      ax.plot(xi, yi, marker='o', markersize=6, color='none', markeredgecolor='black')\n",
        "\n",
        "  ax.plot(scat_x_3[3],scat_y_3[3], marker='o', markersize=6, color='none', markeredgecolor='black', label = 'λ(PIT) = 5e-3')\n",
        "\n",
        "\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  r =sns.lineplot(x='MN_params', y='MN_MAE', data=df_MN, color='blue', marker='o', markersize=6, label='MN')\n",
        "  r =sns.lineplot(x='PIT_params_mn6', y='PIT_MAE_mn6', data=df_PIT_mn6, color='green', markers=markerz, markersize=6, label='MN-PIT pour λ(MN) = 1e-6')\n",
        "  r =sns.lineplot(x='PIT_params_mn5', y='PIT_MAE_mn5', data=df_PIT_mn5, color='red', markers=markerz, markersize=6, label='MN-PIT pour λ(MN) = 1e-5')\n",
        "  r =sns.lineplot(x='PIT_params_mn4', y='PIT_MAE_mn4', data=df_PIT_mn4, color='yellow', markers=markerz, markersize=6, label='MN-PIT pour λ(MN) = 1e-4')\n",
        "  r =sns.lineplot(x='PIT_params_mn3', y='PIT_MAE_mn3', data=df_PIT_mn3, color='orange', markers=markerz, markersize=6, label='MN-PIT pour λ(MN) = 1e-3')\n",
        "\n",
        "\n",
        "# Set the plot title and labels\n",
        "plt.xlabel('Nombre de paramètres')\n",
        "plt.ylabel('MAE (BPM)')\n",
        "\n",
        "# Show the legend\n",
        "legend = plt.legend()\n",
        "legend.get_frame().set_facecolor('w')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "0-5C0Xa1dBqT",
        "outputId": "caaa89aa-4439-432f-96cf-93402bb6f62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = r.get_figure()\n",
        "fig.savefig(\"lmdaz.png\", dpi=150)"
      ],
      "metadata": {
        "id": "qrCmTp0psyVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "SNMoss0qA512",
        "0Ktu_0lV0x0v",
        "NzyXY0Mj1WnR",
        "3nKWGtet1zHV",
        "tiEdfQTo2ZVg",
        "nVHUchkiLp4s",
        "Sr0109rzz8wB",
        "rKaQC9lr03jZ",
        "9IVjVw0GdeEZ",
        "oV6QuooAFqYq",
        "Azt8w1YO9Xe6"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}